#!/usr/bin/env python3
"""
gemma3:4bã®ä¸­æ–­æ©Ÿèƒ½ã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

é•·ã„å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹éš›ã«ã€ç•°ãªã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ä¸­æ–­å‘½ä»¤ã‚’å‡ºã—ã¦ã€
å®Ÿéš›ã«ä¸­æ–­ãŒæ©Ÿèƒ½ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¾ã™
"""

import time
import subprocess
import requests
import json
from typing import Generator, Optional, Tuple
import sys
import threading

class Gemma3InterruptValidator:
    def __init__(self, model_name: str = "gemma3:4b", ollama_host: str = "http://localhost:11434"):
        """
        Gemma3ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–

        Args:
            model_name: Ollamaã®ãƒ¢ãƒ‡ãƒ«å
            ollama_host: Ollamaã®ãƒ›ã‚¹ãƒˆURL
        """
        print(f"ğŸ“¦ Ollama ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ä¸­: {model_name}")

        self.model_name = model_name
        self.ollama_host = ollama_host
        self.timeout_seconds = 60
        self.current_response = None
        self.response_closed = False

        # Ollamaã‚µãƒ¼ãƒãƒ¼ã®çŠ¶æ…‹ç¢ºèª
        try:
            response = requests.get(f"{ollama_host}/api/tags", timeout=5)
            if response.status_code == 200:
                print(f"âœ… Ollamaã‚µãƒ¼ãƒãƒ¼ãŒç¨¼åƒä¸­: {ollama_host}")
            else:
                print(f"âš ï¸  Ollamaã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“")
                self._start_ollama()
        except requests.exceptions.RequestException:
            print(f"âš ï¸  Ollamaã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“ã€‚èµ·å‹•ä¸­...")
            self._start_ollama()

        # ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª
        self._check_model_available()
        print("âœ… åˆæœŸåŒ–å®Œäº†\n")

    def _start_ollama(self):
        """Ollamaã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•"""
        print("â–¶ Ollamaã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ä¸­...")
        try:
            subprocess.Popen(
                ["ollama", "serve"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
            time.sleep(5)
            print("âœ… Ollamaã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"âŒ Ollamaèµ·å‹•å¤±æ•—: {e}")
            sys.exit(1)

    def _check_model_available(self):
        """ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            response = requests.get(f"{self.ollama_host}/api/tags", timeout=10)
            if response.status_code == 200:
                models = response.json().get("models", [])
                model_names = [m["name"] for m in models]

                if self.model_name not in model_names:
                    print(f"âš ï¸  ãƒ¢ãƒ‡ãƒ« {self.model_name} ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                    print(f"   åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {model_names}")
                else:
                    print(f"âœ… ãƒ¢ãƒ‡ãƒ« {self.model_name} ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
        except requests.exceptions.RequestException as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ç¢ºèªå¤±æ•—: {e}")
            sys.exit(1)

    def generate_with_interrupt_capability(
        self,
        prompt: str,
        max_tokens: int = 50,
        temperature: float = 0.7
    ) -> Tuple[Generator[str, None, None], float]:
        """
        ç”Ÿæˆå¯èƒ½ã§ã‚ã‚ŠãªãŒã‚‰ã€å¤–éƒ¨ã‹ã‚‰ä¸­æ–­ã§ãã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿

        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            temperature: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦

        Yields:
            ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³
        """
        url = f"{self.ollama_host}/api/generate"

        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "temperature": temperature,
            "num_predict": max_tokens,
            "stream": True,
            "raw": False
        }

        def token_generator():
            try:
                self.response_closed = False
                self.current_response = requests.post(url, json=payload, stream=True, timeout=None)

                if self.current_response.status_code != 200:
                    print(f"\nâŒ APIã‚¨ãƒ©ãƒ¼: {self.current_response.status_code}")
                    yield "APIã‚¨ãƒ©ãƒ¼"
                    return

                token_count = 0
                for line in self.current_response.iter_lines():
                    # å¤–éƒ¨ã‹ã‚‰ã®ä¸­æ–­ãƒã‚§ãƒƒã‚¯
                    if self.response_closed:
                        print(f"\nâ±ï¸  å¤–éƒ¨ã‹ã‚‰ã®ä¸­æ–­ä¿¡å·ã‚’å—ä¿¡: ç”Ÿæˆã‚’ä¸­æ–­")
                        self.current_response.close()
                        yield "[ä¸­æ–­]"
                        return

                    if line:
                        try:
                            data = json.loads(line)
                            token_text = data.get("response", "")

                            if token_text:
                                yield token_text
                                token_count += 1

                            # ç”Ÿæˆå®Œäº†ãƒ•ãƒ©ã‚°
                            if data.get("done", False):
                                break
                        except json.JSONDecodeError:
                            continue

            except requests.exceptions.Timeout:
                print(f"\nâ±ï¸  ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                yield "[ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ]"
                return
            except Exception as e:
                print(f"\nâŒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
                yield f"[ã‚¨ãƒ©ãƒ¼: {e}]"
                return

        return token_generator(), time.time()

    def interrupt_generation(self):
        """ç”Ÿæˆã‚’ä¸­æ–­"""
        self.response_closed = True
        if self.current_response:
            try:
                self.current_response.close()
            except:
                pass

    def test_long_generation_without_interrupt(self):
        """
        é•·ã„å¿œç­”ã‚’ä¸­æ–­ãªã—ã§ç”Ÿæˆï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
        """
        print("\n" + "="*70)
        print("ğŸ”„ ãƒ†ã‚¹ãƒˆ1: é•·ã„å¿œç­”ã‚’ä¸­æ–­ãªã—ã§ç”Ÿæˆï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰")
        print("="*70 + "\n")

        prompt = """ã‚ãªãŸã¯æœ‰èƒ½ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã—ã¦ã€è©³ã—ãã€ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚

è³ªå•: äººå·¥çŸ¥èƒ½ã¨ã¯ä½•ã‹ã€ãã®å¿œç”¨ä¾‹ã‚’å«ã‚ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚"""

        print(f"â³ ç”Ÿæˆé–‹å§‹ï¼ˆmax_tokens=200ã€ä¸­æ–­ãªã—ï¼‰...")
        print("-" * 70)

        start_time = time.time()
        result = ""
        token_count = 0

        generator, _ = self.generate_with_interrupt_capability(prompt, max_tokens=200, temperature=0.7)

        for token in generator:
            if token not in ["[ä¸­æ–­]", "[ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ]", "[APIã‚¨ãƒ©ãƒ¼]"]:
                print(token, end="", flush=True)
                result += token
                token_count += 1

        elapsed = time.time() - start_time
        print(f"\n" + "-" * 70)
        print(f"âœ… ç”Ÿæˆå®Œäº†")
        print(f"â±ï¸  åˆè¨ˆæ™‚é–“: {elapsed*1000:.1f}ms")
        print(f"ğŸ“Š ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {token_count}")
        print(f"ğŸ“ çµæœ: {result[:100]}..." if len(result) > 100 else f"ğŸ“ çµæœ: {result}")

        return elapsed, token_count, result

    def test_long_generation_with_interrupt_at_timing(self, interrupt_delay_ms: int):
        """
        é•·ã„å¿œç­”ã‚’æŒ‡å®šã—ãŸã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ä¸­æ–­

        Args:
            interrupt_delay_ms: ä¸­æ–­ã¾ã§å¾…æ©Ÿã™ã‚‹æ™‚é–“ï¼ˆãƒŸãƒªç§’ï¼‰
        """
        print("\n" + "="*70)
        print(f"ğŸ”„ ãƒ†ã‚¹ãƒˆ2: é•·ã„å¿œç­”ã‚’ {interrupt_delay_ms}ms ã§ä¸­æ–­")
        print("="*70 + "\n")

        prompt = """ã‚ãªãŸã¯æœ‰èƒ½ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã—ã¦ã€è©³ã—ãã€ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚

è³ªå•: äººå·¥çŸ¥èƒ½ã¨ã¯ä½•ã‹ã€ãã®å¿œç”¨ä¾‹ã‚’å«ã‚ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚"""

        print(f"â³ ç”Ÿæˆé–‹å§‹ï¼ˆmax_tokens=200ã€{interrupt_delay_ms}mså¾Œã«ä¸­æ–­ï¼‰...")
        print("-" * 70)

        start_time = time.time()
        result = ""
        token_count = 0

        # ä¸­æ–­ã‚¿ã‚¹ã‚¯ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
        def interrupt_after_delay():
            time.sleep(interrupt_delay_ms / 1000.0)
            now_interrupt = time.time()
            elapsed_ms = (now_interrupt - start_time) * 1000
            print(f"\nâ¸ï¸  [ä¸­æ–­ä¿¡å·] {interrupt_delay_ms}msæ™‚ç‚¹ã§ç”Ÿæˆã‚’ä¸­æ–­ã—ã¾ã™... (å®Ÿéš›: {elapsed_ms:.1f}ms)")
            self.interrupt_generation()

        interrupt_thread = threading.Thread(target=interrupt_after_delay, daemon=True)
        interrupt_thread.start()

        generator, _ = self.generate_with_interrupt_capability(prompt, max_tokens=200, temperature=0.7)

        for token in generator:
            if token not in ["[ä¸­æ–­]", "[ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ]", "[APIã‚¨ãƒ©ãƒ¼]"]:
                print(token, end="", flush=True)
                result += token
                token_count += 1
            elif token == "[ä¸­æ–­]":
                print(f"[ä¸­æ–­]", end="", flush=True)

        elapsed = time.time() - start_time
        print(f"\n" + "-" * 70)
        print(f"âœ… ä¸­æ–­æˆåŠŸ")
        print(f"â±ï¸  å®Ÿéš›ã®çµŒéæ™‚é–“: {elapsed*1000:.1f}ms")
        print(f"ğŸ“Š ä¸­æ–­ã¾ã§ã«ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³æ•°: {token_count}")
        print(f"ğŸ“ çµæœ: {result[:100]}..." if len(result) > 100 else f"ğŸ“ çµæœ: {result}")

        return elapsed, token_count, result


def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    print("\n" + "="*70)
    print("ğŸš€ Gemma3:4b ä¸­æ–­æ©Ÿèƒ½ã®æœ‰åŠ¹æ€§æ¤œè¨¼")
    print("="*70)

    try:
        # ãƒãƒªãƒ‡ãƒ¼ã‚¿åˆæœŸåŒ–
        validator = Gemma3InterruptValidator()

        # ãƒ†ã‚¹ãƒˆ1: ä¸­æ–­ãªã—ã§é•·ã„å¿œç­”ã‚’ç”Ÿæˆ
        baseline_time, baseline_tokens, baseline_result = validator.test_long_generation_without_interrupt()

        # ãƒ†ã‚¹ãƒˆ2-4: ç•°ãªã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ä¸­æ–­
        interrupt_results = []
        for interrupt_delay in [500, 1000, 1500]:
            time.sleep(1)  # ãƒ†ã‚¹ãƒˆé–“éš”
            elapsed, tokens, result = validator.test_long_generation_with_interrupt_at_timing(interrupt_delay)
            interrupt_results.append({
                "delay_ms": interrupt_delay,
                "elapsed_ms": elapsed * 1000,
                "tokens": tokens,
                "result_length": len(result)
            })

        # ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        print("\n" + "="*70)
        print("ğŸ“Š æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼")
        print("="*70)

        print(f"\nğŸ”µ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆä¸­æ–­ãªã—ï¼‰:")
        print(f"  â€¢ ç”Ÿæˆæ™‚é–“: {baseline_time*1000:.1f}ms")
        print(f"  â€¢ ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {baseline_tokens}")
        print(f"  â€¢ çµæœé•·: {len(baseline_result)}æ–‡å­—")

        print(f"\nğŸŸ  ä¸­æ–­ã‚ã‚Šã®ãƒ†ã‚¹ãƒˆçµæœ:")
        for result in interrupt_results:
            time_ratio = result["elapsed_ms"] / (baseline_time * 1000)
            token_ratio = result["tokens"] / baseline_tokens if baseline_tokens > 0 else 0
            print(f"\n  ã€{result['delay_ms']}msã§ä¸­æ–­ã€‘")
            print(f"    â€¢ å®Ÿéš›ã®çµŒéæ™‚é–“: {result['elapsed_ms']:.1f}ms ({time_ratio:.1%} of baseline)")
            print(f"    â€¢ ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {result['tokens']} ({token_ratio:.1%} of baseline)")
            print(f"    â€¢ çµæœé•·: {result['result_length']}æ–‡å­—")

        print(f"\n{'='*70}")
        print("ğŸ’¡ æ¤œè¨¼çµæœã®è§£é‡ˆ:")
        print(f"{'='*70}")
        print("â€¢ ä¸­æ–­ãŒæœ‰åŠ¹ãªå ´åˆ:")
        print("  - çµŒéæ™‚é–“ãŒæŒ‡å®šã—ãŸä¸­æ–­ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚ˆã‚Šå¤§å¹…ã«çŸ­ã„")
        print("  - ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Šå¤§å¹…ã«å°‘ãªã„")
        print("  - çµæœé•·ãŒãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Šå¤§å¹…ã«çŸ­ã„")
        print("\nâ€¢ ä¸­æ–­ãŒç„¡åŠ¹ãªå ´åˆ:")
        print("  - çµŒéæ™‚é–“ãŒãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨åŒç¨‹åº¦")
        print("  - ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨åŒç¨‹åº¦")
        print("  - çµæœé•·ãŒãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨åŒç¨‹åº¦")

        print(f"\n{'='*70}")
        print("âœ… æ¤œè¨¼å®Œäº†")
        print(f"{'='*70}")

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
