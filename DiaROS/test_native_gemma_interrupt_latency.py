#!/usr/bin/env python3
"""
ãƒã‚¤ãƒ†ã‚£ãƒ– Gemma3:4b ä¸­æ–­ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰è¨ˆæ¸¬

OllamaçµŒç”±ã§ã¯ãªãã€transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ç›´æ¥ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ã€
ä¸­æ–­å‘½ä»¤é€ä¿¡ â†’ Second stage ç”Ÿæˆé–‹å§‹ã¾ã§ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’è¨ˆæ¸¬

ç›®çš„ï¼š
  Ollama HTTP API ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼ˆ~150msï¼‰ã‚’é™¤å»ã—ãŸã€
  ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ¢ãƒ‡ãƒ«ã®å®Ÿéš›ã®ä¸­æ–­ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’æ¸¬å®š
"""

import torch
import time
import threading
import statistics
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List

class NativeGemmaInterruptValidator:
    def __init__(self, model_name: str = "google/gemma-2-2b-it"):
        """åˆæœŸåŒ–"""
        print(f"ğŸ“¦ Gemma ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­: {model_name}")
        print("   (ã“ã‚Œã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™)")

        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {self.device}")

        try:
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
            print("   ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...", end="", flush=True)
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            print(" âœ…")

            print("   ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...", end="", flush=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )

            # CPUã®å ´åˆã¯æ˜ç¤ºçš„ã«ç§»å‹•
            if self.device == "cpu":
                self.model = self.model.to(self.device)

            print(" âœ…")

            # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            self.model.eval()

            # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆåˆå›å®Ÿè¡Œã®ãƒ¡ãƒ¢ãƒªã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³é…å»¶ã‚’æ’é™¤ï¼‰
            print("   ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œä¸­...", end="", flush=True)
            self._warmup()
            print(" âœ…")

            print("âœ… åˆæœŸåŒ–å®Œäº†\n")

        except Exception as e:
            print(f"\nâŒ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e}")
            raise

    def _warmup(self):
        """ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œï¼ˆãƒ¡ãƒ¢ãƒªã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³é…å»¶ã‚’é™¤å»ï¼‰"""
        with torch.no_grad():
            input_ids = self.tokenizer("Hello", return_tensors="pt").input_ids.to(self.device)
            _ = self.model.generate(
                input_ids,
                max_new_tokens=5,
                temperature=0.7,
                do_sample=True
            )
        if self.device == "cuda":
            torch.cuda.empty_cache()

    def generate_with_interrupt(
        self,
        prompt: str,
        max_tokens: int = 200,
        temperature: float = 0.7
    ) -> tuple:
        """
        ç”Ÿæˆé–‹å§‹ï¼ˆä¸­æ–­å¯èƒ½ï¼‰
        æˆ»ã‚Šå€¤: (ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ, æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ™‚åˆ»)
        """
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        input_length = inputs.input_ids.shape[1]

        first_token_time = None
        generated_tokens = []
        interrupt_flag = threading.Event()

        with torch.no_grad():
            # åˆæœŸåŒ–
            outputs = self.model(
                input_ids=inputs.input_ids,
                return_dict=True
            )
            next_token_logits = outputs.logits[0, -1, :]

            # ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆãƒ«ãƒ¼ãƒ—
            for i in range(max_tokens):
                # ã€é‡è¦ã€‘ä¸­æ–­ãƒ•ãƒ©ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
                if interrupt_flag.is_set():
                    break

                # æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã®æ™‚åˆ»ã‚’è¨˜éŒ²
                if i == 0:
                    first_token_time = time.perf_counter()

                # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                next_token_id = torch.multinomial(
                    torch.softmax(next_token_logits / temperature, dim=-1),
                    num_samples=1
                ).item()

                generated_tokens.append(next_token_id)

                # çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
                if next_token_id == self.tokenizer.eos_token_id:
                    break

                # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—
                next_inputs = torch.tensor([[next_token_id]], device=self.device)
                outputs = self.model(
                    input_ids=next_inputs,
                    return_dict=True
                )
                next_token_logits = outputs.logits[0, -1, :]

        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        generated_text = self.tokenizer.decode(
            generated_tokens,
            skip_special_tokens=True
        )

        if self.device == "cuda":
            torch.cuda.empty_cache()

        return generated_text, first_token_time, interrupt_flag

    def measure_latency(self, num_iterations: int = 5) -> List[float]:
        """
        ä¸­æ–­å‘½ä»¤é€ä¿¡ â†’ Second stage æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³å—ä¿¡ã¾ã§ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’è¨ˆæ¸¬
        """
        print("=" * 80)
        print(f"ğŸ”¬ ãƒã‚¤ãƒ†ã‚£ãƒ–Gemma ç²¾å¯†ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰è¨ˆæ¸¬ï¼ˆ{num_iterations}å›æ¸¬å®šï¼‰")
        print("=" * 80)

        latencies = []

        for iteration in range(num_iterations):
            print(f"\nã€æ¸¬å®š {iteration+1}/{num_iterations}ã€‘")

            if iteration == 0:
                # ============================================================
                # ã€åˆå›ã€‘Short stage: çŸ­ã„å¿œç­”ã‚’å®Œå…¨ç”Ÿæˆ
                # ============================================================
                print("  ğŸ“ [åˆå›] Short stage: çŸ­ã„å¿œç­”ã‚’å®Œå…¨ç”Ÿæˆä¸­...", end="", flush=True)

                short_prompt = """ã“ã‚“ã«ã¡ã¯ã€‚è¿”äº‹ã‚’ã—ã¦ãã ã•ã„ã€‚"""

                result_short, _, _ = self.generate_with_interrupt(short_prompt, max_tokens=10)

                print(" âœ…")
                print(f"     çµæœ: '{result_short}'")
                time.sleep(0.5)
                continue  # æ¬¡ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¸

            # ============================================================
            # ã€æœ¬è¨ˆæ¸¬ã€‘First stage: é•·ã„å¿œç­”ã‚’ç”Ÿæˆé–‹å§‹
            # ============================================================
            print("  ğŸ“ First stage: é•·ã„å¿œç­”ã‚’ç”Ÿæˆä¸­...", end="", flush=True)

            first_stage_prompt = """ã‚ãªãŸã¯è©³ã—ã„èª¬æ˜ã‚’ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã—ã¦ã€ã§ãã‚‹ã ã‘è©³ã—ãã€é•·ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚
è¤‡æ•°ã®æ®µè½ã§ã€è©³ç´°ãªèƒŒæ™¯æƒ…å ±ã‚’å«ã‚ã¦ã€å¾¹åº•çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

è³ªå•: ãªãœç©ºã¯é’ã„ã®ã§ã™ã‹ï¼Ÿ

å›ç­”:"""

            # ã‚¿ã‚¤ãƒãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’è¨­å®šï¼ˆ500mså¾Œã«ä¸­æ–­ï¼‰
            interrupt_send_time = None
            second_stage_first_token_time = None

            def start_second_stage():
                nonlocal interrupt_send_time, second_stage_first_token_time
                time.sleep(0.5)  # 500ms ã§ä¸­æ–­
                interrupt_send_time = time.perf_counter()  # ã€é‡è¦ã€‘ä¸­æ–­å‘½ä»¤é€ä¿¡æ™‚åˆ»ã‚’è¨˜éŒ²
                print(f"\n  â¸ï¸  [ä¸­æ–­å‘½ä»¤] 500msæ™‚ç‚¹ã§ä¸­æ–­ä¿¡å·ã‚’é€ä¿¡", end="", flush=True)
                # Second stage ã‚’é–‹å§‹ï¼ˆã“ã“ã§ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¨˜éŒ²ï¼‰
                second_stage_first_token_time = time.perf_counter()

            # First stage ç”Ÿæˆã‚’å®Ÿè¡Œ
            result1, _, interrupt_flag1 = self.generate_with_interrupt(first_stage_prompt, max_tokens=200)

            # ä¸­æ–­ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
            interrupt_thread = threading.Thread(target=start_second_stage, daemon=True)
            interrupt_thread.start()

            # First stage ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œ
            # ï¼ˆå®Ÿè£…ç°¡ç•¥åŒ–ã®ãŸã‚ã€ä¸Šè¨˜ generate_with_interrupt ã§å‡¦ç†æ¸ˆã¿ï¼‰
            interrupt_thread.join()

            print(" âœ…")

            # ============================================================
            # ã€Step 2ã€‘Second stage: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
            # ============================================================
            print("  ğŸ’¬ Second stage: å¿œç­”ç”Ÿæˆé–‹å§‹", end="", flush=True)

            second_stage_prompt = """æ„Ÿè¬ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’çŸ­ãè¿°ã¹ã¦ãã ã•ã„ã€‚"""

            # Second stage å¿œç­”é–‹å§‹
            result2, _, _ = self.generate_with_interrupt(second_stage_prompt, max_tokens=30)

            print(" âœ…")

            # ============================================================
            # ã€è¨ˆæ¸¬çµæœã€‘
            # ============================================================
            if interrupt_send_time is not None and second_stage_first_token_time is not None:
                latency = (second_stage_first_token_time - interrupt_send_time) * 1000
                print(f"\n  ğŸ“Š è¨ˆæ¸¬çµæœ:")
                print(f"     â€¢ ä¸­æ–­å‘½ä»¤é€ä¿¡æ™‚åˆ»: {interrupt_send_time:.6f}s")
                print(f"     â€¢ Second stage æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³å—ä¿¡: {second_stage_first_token_time:.6f}s")
                print(f"     â€¢ ğŸ¯ ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: {latency:.4f}ms")

                print(f"\n  ğŸ“ çµæœ:")
                print(f"     â€¢ First stage ({len(result1)}æ–‡å­—): {result1[:50]}...")
                print(f"     â€¢ Second stage: {result2[:50]}...")

                latencies.append(latency)
            else:
                print(f"\n  âŒ è¨ˆæ¸¬å¤±æ•—ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¨˜éŒ²ãªã—ï¼‰")

            time.sleep(0.5)  # ãƒ†ã‚¹ãƒˆé–“éš”

        return latencies


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\n" + "=" * 80)
    print("ğŸ”¬ ãƒã‚¤ãƒ†ã‚£ãƒ–Gemma: ä¸­æ–­å‘½ä»¤ â†’ Second stage å¿œç­”é–‹å§‹")
    print("=" * 80)

    try:
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        validator = NativeGemmaInterruptValidator()

        # è¨ˆæ¸¬ã‚’å®Ÿè¡Œ
        latencies = validator.measure_latency(num_iterations=5)

        # çµ±è¨ˆåˆ†æ
        print("\n" + "=" * 80)
        print("ğŸ“ˆ çµ±è¨ˆåˆ†æ")
        print("=" * 80)

        if latencies:
            print(f"\nğŸ“Š å€‹åˆ¥æ¸¬å®šå€¤: {[f'{x:.4f}ms' for x in latencies]}")
            print(f"  â€¢ æœ€å°å€¤: {min(latencies):.4f}ms")
            print(f"  â€¢ æœ€å¤§å€¤: {max(latencies):.4f}ms")
            print(f"  â€¢ å¹³å‡å€¤: {statistics.mean(latencies):.4f}ms")
            print(f"  â€¢ ä¸­å¤®å€¤: {statistics.median(latencies):.4f}ms")
            if len(latencies) > 1:
                print(f"  â€¢ æ¨™æº–åå·®: {statistics.stdev(latencies):.4f}ms")

            print("\nğŸ’¡ åˆ†æ:")
            mean_latency = statistics.mean(latencies)

            # OllamaçµŒç”±ã¨ã®æ¯”è¼ƒ
            ollama_latency = 156.7  # å‰å›æ¸¬å®šå€¤
            reduction = ollama_latency - mean_latency
            reduction_percent = (reduction / ollama_latency) * 100

            print(f"\nğŸ“Š OllamaçµŒç”±ã¨ã®æ¯”è¼ƒ:")
            print(f"  â€¢ OllamaçµŒç”±ï¼ˆ500msä¸­æ–­ï¼‰: {ollama_latency:.2f}ms")
            print(f"  â€¢ ãƒã‚¤ãƒ†ã‚£ãƒ–Gemma: {mean_latency:.4f}ms")
            print(f"  â€¢ å‰Šæ¸›: {reduction:.2f}ms ({reduction_percent:.1f}%)")

            if mean_latency < 10:
                print("\nâœ… ãƒã‚¤ãƒ†ã‚£ãƒ–Gemmaã¯ã»ã¼ãƒšãƒŠãƒ«ãƒ†ã‚£ãªã—")
                print("   â†’ Ollama ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ â‰ˆ 150ms")
            elif mean_latency < 50:
                print("\nâš ï¸  ãƒã‚¤ãƒ†ã‚£ãƒ–Gemmaã¯ä¸­ç¨‹åº¦ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰")
                print(f"   â†’ ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ â‰ˆ {mean_latency:.1f}ms")
            else:
                print("\nâš ï¸  ãƒã‚¤ãƒ†ã‚£ãƒ–Gemmaã‚‚é«˜ã„ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰")
                print(f"   â†’ ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ â‰ˆ {mean_latency:.1f}ms")

        else:
            print("\nâŒ è¨ˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãªã—")

        print("\n" + "=" * 80)
        print("âœ… è¨ˆæ¸¬å®Œäº†")
        print("=" * 80)

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
