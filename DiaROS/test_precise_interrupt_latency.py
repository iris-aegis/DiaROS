#!/usr/bin/env python3
"""
æ­£ç¢ºãªä¸­æ–­ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¨ˆæ¸¬ãƒ†ã‚¹ãƒˆ

å®Ÿæ¸¬ã‚·ãƒŠãƒªã‚ªï¼š
1. First stage ã§é•·ã„å¿œç­”ã‚’ç”Ÿæˆé–‹å§‹
2. 100ms ã§å¿œç­”ç”Ÿæˆã‚’ä¸­æ–­
3. Second stage ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
4. ä¸­æ–­å‘½ä»¤é€ä¿¡æ™‚åˆ» â†’ Second stage å¿œç­”ãŒå®Ÿéš›ã«é–‹å§‹ã•ã‚Œã‚‹æ™‚åˆ»ã¾ã§ã®æ™‚é–“ã‚’è¨ˆæ¸¬

ã€è¨ˆæ¸¬å¯¾è±¡ã€‘
- interrupt_send_time: ä¸­æ–­å‘½ä»¤ã‚’é€ä¿¡ã—ãŸæ™‚åˆ»
- second_stage_first_token_time: Second stage ãŒæœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å—ã‘å–ã£ãŸæ™‚åˆ»
- å·®åˆ†: å®Ÿéš›ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰æ™‚é–“
"""

import time
import subprocess
import requests
import json
from typing import Generator, Tuple
import sys
import threading
import statistics

class PreciseInterruptLatencyValidator:
    def __init__(self, model_name: str = "gemma3:4b", ollama_host: str = "http://localhost:11434"):
        """åˆæœŸåŒ–"""
        print(f"ğŸ“¦ Ollama ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ä¸­: {model_name}")

        self.model_name = model_name
        self.ollama_host = ollama_host
        self.current_response = None
        self.response_closed = False

        # è¨ˆæ¸¬ç”¨ã®å¤‰æ•°
        self.interrupt_send_time = None
        self.second_stage_first_token_time = None
        self.first_token_received = False

        try:
            response = requests.get(f"{ollama_host}/api/tags", timeout=5)
            if response.status_code == 200:
                print(f"âœ… Ollamaã‚µãƒ¼ãƒãƒ¼ãŒç¨¼åƒä¸­: {ollama_host}")
        except requests.exceptions.RequestException:
            print(f"âš ï¸  Ollamaã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“ã€‚èµ·å‹•ä¸­...")
            self._start_ollama()

        self._check_model_available()
        print("âœ… åˆæœŸåŒ–å®Œäº†\n")

    def _start_ollama(self):
        """Ollamaã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•"""
        print("â–¶ Ollamaã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ä¸­...")
        try:
            subprocess.Popen(
                ["ollama", "serve"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
            time.sleep(5)
            print("âœ… Ollamaã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"âŒ Ollamaèµ·å‹•å¤±æ•—: {e}")
            sys.exit(1)

    def _check_model_available(self):
        """ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            response = requests.get(f"{self.ollama_host}/api/tags", timeout=10)
            if response.status_code == 200:
                models = response.json().get("models", [])
                model_names = [m["name"] for m in models]
                if self.model_name in model_names:
                    print(f"âœ… ãƒ¢ãƒ‡ãƒ« {self.model_name} ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
        except requests.exceptions.RequestException as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ç¢ºèªå¤±æ•—: {e}")
            sys.exit(1)

    def generate_streaming_with_timing(
        self,
        prompt: str,
        max_tokens: int = 50,
        temperature: float = 0.7
    ) -> Tuple[Generator[str, None, None], float]:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆï¼ˆæœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ™‚åˆ»ã‚’è¨˜éŒ²ï¼‰"""
        url = f"{self.ollama_host}/api/generate"

        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "temperature": temperature,
            "num_predict": max_tokens,
            "stream": True,
            "raw": False
        }

        api_request_time = time.perf_counter()  # API ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡æ™‚åˆ»

        def token_generator():
            try:
                self.response_closed = False
                self.current_response = requests.post(url, json=payload, stream=True, timeout=None)

                if self.current_response.status_code != 200:
                    yield "APIã‚¨ãƒ©ãƒ¼"
                    return

                first_token = True
                for line in self.current_response.iter_lines():
                    if self.response_closed:
                        self.current_response.close()
                        yield "[ä¸­æ–­]"
                        return

                    if line:
                        try:
                            data = json.loads(line)
                            token_text = data.get("response", "")

                            # ã€é‡è¦ã€‘æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã®æ™‚åˆ»ã‚’è¨˜éŒ²
                            if first_token and token_text:
                                self.second_stage_first_token_time = time.perf_counter()
                                first_token = False

                            if token_text:
                                yield token_text
                            if data.get("done", False):
                                break
                        except json.JSONDecodeError:
                            continue

            except Exception as e:
                yield f"[ã‚¨ãƒ©ãƒ¼: {e}]"

        return token_generator(), api_request_time

    def interrupt_generation(self):
        """ç”Ÿæˆã‚’ä¸­æ–­"""
        self.response_closed = True
        if self.current_response:
            try:
                self.current_response.close()
            except:
                pass

    def measure_latency(self, num_iterations: int = 5) -> list:
        """
        ä¸­æ–­å‘½ä»¤é€ä¿¡ â†’ Second stage æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³å—ä¿¡ã¾ã§ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’è¨ˆæ¸¬

        ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼š
        1. First iteration: çŸ­ã„å¿œç­”ã‚’ç”Ÿæˆï¼ˆå®Œå…¨ç”Ÿæˆï¼‰
        2. Second iterationï½: é•·ã„å¿œç­”ã‚’ç”Ÿæˆé–‹å§‹ â†’ 500msã§ä¸­æ–­ â†’ Second stageé–‹å§‹
        """
        print("=" * 80)
        print(f"ğŸ”¬ ç²¾å¯†ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰è¨ˆæ¸¬ï¼ˆ{num_iterations}å›æ¸¬å®šï¼‰")
        print("=" * 80)

        latencies = []

        for iteration in range(num_iterations):
            print(f"\nã€æ¸¬å®š {iteration+1}/{num_iterations}ã€‘")

            if iteration == 0:
                # ============================================================
                # ã€åˆå›ã€‘Short stage: çŸ­ã„å¿œç­”ã‚’å®Œå…¨ç”Ÿæˆ
                # ============================================================
                print("  ğŸ“ [åˆå›] Short stage: çŸ­ã„å¿œç­”ã‚’å®Œå…¨ç”Ÿæˆä¸­...", end="", flush=True)

                short_prompt = """ã“ã‚“ã«ã¡ã¯ã€‚è¿”äº‹ã‚’ã—ã¦ãã ã•ã„ã€‚"""

                result_short = ""
                gen_short, _ = self.generate_streaming_with_timing(short_prompt, max_tokens=10)
                for token in gen_short:
                    if token not in ["[ä¸­æ–­]", "[APIã‚¨ãƒ©ãƒ¼]"]:
                        result_short += token

                print(" âœ…")
                print(f"     çµæœ: '{result_short}'")
                time.sleep(0.5)
                continue  # æ¬¡ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¸

            # ============================================================
            # ã€æœ¬è¨ˆæ¸¬ã€‘First stage: é•·ã„å¿œç­”ã‚’ç”Ÿæˆé–‹å§‹
            # ============================================================
            print("  ğŸ“ First stage: é•·ã„å¿œç­”ã‚’ç”Ÿæˆä¸­...", end="", flush=True)

            first_stage_prompt = """ã‚ãªãŸã¯è©³ã—ã„èª¬æ˜ã‚’ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã—ã¦ã€ã§ãã‚‹ã ã‘è©³ã—ãã€é•·ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚
è¤‡æ•°ã®æ®µè½ã§ã€è©³ç´°ãªèƒŒæ™¯æƒ…å ±ã‚’å«ã‚ã¦ã€å¾¹åº•çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

è³ªå•: ãªãœç©ºã¯é’ã„ã®ã§ã™ã‹ï¼Ÿ

å›ç­”:"""

            # ã‚¿ã‚¤ãƒãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’è¨­å®šï¼ˆ500mså¾Œã«ä¸­æ–­ï¼‰
            self.interrupt_send_time = None
            self.second_stage_first_token_time = None

            def interrupt_after_500ms():
                time.sleep(0.5)  # 500ms ã§ä¸­æ–­
                self.interrupt_send_time = time.perf_counter()  # ã€é‡è¦ã€‘ä¸­æ–­å‘½ä»¤é€ä¿¡æ™‚åˆ»ã‚’è¨˜éŒ²
                print(f"\n  â¸ï¸  [ä¸­æ–­å‘½ä»¤] 500msæ™‚ç‚¹ã§ä¸­æ–­ä¿¡å·ã‚’é€ä¿¡", end="", flush=True)
                self.interrupt_generation()

            interrupt_thread = threading.Thread(target=interrupt_after_500ms, daemon=True)
            interrupt_thread.start()

            # First stage ç”Ÿæˆã‚’å®Ÿè¡Œ
            result1 = ""
            gen1, api_request_time1 = self.generate_streaming_with_timing(first_stage_prompt, max_tokens=200)
            for token in gen1:
                if token not in ["[ä¸­æ–­]", "[APIã‚¨ãƒ©ãƒ¼]"]:
                    result1 += token

            interrupt_thread.join()  # ä¸­æ–­ã‚¹ãƒ¬ãƒƒãƒ‰å®Œäº†å¾…æ©Ÿ
            print(" âœ…")

            # ============================================================
            # ã€Step 2ã€‘Second stage: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
            # ============================================================
            print("  ğŸ’¬ Second stage: å¿œç­”ç”Ÿæˆé–‹å§‹", end="", flush=True)

            second_stage_prompt = """æ„Ÿè¬ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’çŸ­ãè¿°ã¹ã¦ãã ã•ã„ã€‚"""

            # Second stage å¿œç­”é–‹å§‹
            result2 = ""
            gen2, api_request_time2 = self.generate_streaming_with_timing(second_stage_prompt, max_tokens=30)

            for token in gen2:
                if token not in ["[ä¸­æ–­]", "[APIã‚¨ãƒ©ãƒ¼]"]:
                    result2 += token

            print(" âœ…")

            # ============================================================
            # ã€è¨ˆæ¸¬çµæœã€‘
            # ============================================================
            if self.interrupt_send_time is not None and self.second_stage_first_token_time is not None:
                latency = (self.second_stage_first_token_time - self.interrupt_send_time) * 1000
                print(f"\n  ğŸ“Š è¨ˆæ¸¬çµæœ:")
                print(f"     â€¢ ä¸­æ–­å‘½ä»¤é€ä¿¡æ™‚åˆ»: {self.interrupt_send_time:.6f}s")
                print(f"     â€¢ Second stage æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³å—ä¿¡: {self.second_stage_first_token_time:.6f}s")
                print(f"     â€¢ ğŸ¯ ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: {latency:.4f}ms")

                print(f"\n  ğŸ“ çµæœ:")
                print(f"     â€¢ First stage ({len(result1)}æ–‡å­—): {result1[:50]}...")
                print(f"     â€¢ Second stage: {result2[:50]}...")

                latencies.append(latency)
            else:
                print(f"\n  âŒ è¨ˆæ¸¬å¤±æ•—ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¨˜éŒ²ãªã—ï¼‰")

            time.sleep(0.5)  # ãƒ†ã‚¹ãƒˆé–“éš”

        return latencies


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\n" + "=" * 80)
    print("ğŸ”¬ ç²¾å¯†ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰è¨ˆæ¸¬: ä¸­æ–­å‘½ä»¤ â†’ Second stage å¿œç­”é–‹å§‹")
    print("=" * 80)

    try:
        validator = PreciseInterruptLatencyValidator()

        # è¨ˆæ¸¬ã‚’å®Ÿè¡Œ
        latencies = validator.measure_latency(num_iterations=5)

        # çµ±è¨ˆåˆ†æ
        print("\n" + "=" * 80)
        print("ğŸ“ˆ çµ±è¨ˆåˆ†æ")
        print("=" * 80)

        if latencies:
            print(f"\nğŸ“Š å€‹åˆ¥æ¸¬å®šå€¤: {[f'{x:.4f}ms' for x in latencies]}")
            print(f"  â€¢ æœ€å°å€¤: {min(latencies):.4f}ms")
            print(f"  â€¢ æœ€å¤§å€¤: {max(latencies):.4f}ms")
            print(f"  â€¢ å¹³å‡å€¤: {statistics.mean(latencies):.4f}ms")
            print(f"  â€¢ ä¸­å¤®å€¤: {statistics.median(latencies):.4f}ms")
            if len(latencies) > 1:
                print(f"  â€¢ æ¨™æº–åå·®: {statistics.stdev(latencies):.4f}ms")

            print("\nğŸ’¡ åˆ†æ:")
            mean_latency = statistics.mean(latencies)

            if mean_latency < 50:
                print("  âœ… ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å°‘ãªã„: < 50ms")
                print("     â†’ ä¸­æ–­æˆ¦ç•¥ã¯åŠ¹ç‡çš„")
            elif mean_latency < 150:
                print("  âš ï¸  ä¸­ç¨‹åº¦ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: 50-150ms")
                print("     â†’ APIåˆæœŸåŒ–ã®ã‚³ã‚¹ãƒˆã‚’å«ã‚€æ­£å¸¸ãªå€¤")
            else:
                print("  âš ï¸  é«˜ã„ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: > 150ms")
                print("     â†’ Second stage APIåˆæœŸåŒ–ã«æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å¯èƒ½æ€§")
        else:
            print("\nâŒ è¨ˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãªã—")

        print("\n" + "=" * 80)
        print("âœ… è¨ˆæ¸¬å®Œäº†")
        print("=" * 80)

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
