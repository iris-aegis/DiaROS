#!/usr/bin/env python3
"""
çœŸã®ãƒã‚¤ãƒ†ã‚£ãƒ– Gemma-3-4B ç›´æ¥æ¨è«–ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰è¨ˆæ¸¬

Ollama ã‚’ä¸€åˆ‡ä½¿ã‚ãšã€ctransformers ã‚’ä½¿ã£ã¦ãƒ­ãƒ¼ã‚«ãƒ« GGUF ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰
ç›´æ¥æ¨è«–ã—ã€ä¸­æ–­ï½Second stage é–‹å§‹ã¾ã§ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’è¨ˆæ¸¬

ä½¿ç”¨æ–¹æ³•ï¼š
    python3 test_true_native_gemma3_latency.py
"""

import time
import statistics
import threading
from typing import List
from ctransformers import AutoModelForCausalLM

class TrueNativeGemma3Validator:
    def __init__(self, model_path: str = "/root/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25"):
        """åˆæœŸåŒ–"""
        print(f"ğŸ“¦ ãƒã‚¤ãƒ†ã‚£ãƒ– Gemma-3-4B GGUF ãƒ­ãƒ¼ãƒ‰ä¸­: {model_path}")
        print("   (æœ€åˆã®ãƒ­ãƒ¼ãƒ‰ã«ã¯æ•°ç§’ã‹ã‹ã‚Šã¾ã™)")

        self.model_path = model_path
        self.interrupt_flag = False
        self.interrupt_send_time = None
        self.second_stage_first_token_time = None

        try:
            # GGUF ãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥ãƒ­ãƒ¼ãƒ‰ï¼ˆOllama ã‚’ä½¿ã‚ãšï¼‰
            print("   ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...", end="", flush=True)
            try:
                # ã¾ãšè‡ªå‹•æ¤œå‡ºã‚’è©¦ã™
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    gpu_layers=50,  # GPU ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
                )
            except Exception as e1:
                print(f"\n   è‡ªå‹•æ¤œå‡ºå¤±æ•—: {e1}, gemma3 ã¨ã—ã¦å†è©¦è¡Œ...", end="", flush=True)
                try:
                    # gemma3 ã¨ã—ã¦æ˜ç¤ºçš„ã«æŒ‡å®š
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        model_type="gemma3",
                        gpu_layers=50,
                    )
                except Exception as e2:
                    print(f"\n   gemma3 å¤±æ•—: {e2}, llama ã¨ã—ã¦å†è©¦è¡Œ...", end="", flush=True)
                    # llama ã¨ã—ã¦æŒ‡å®šï¼ˆæ±ç”¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        model_type="llama",
                        gpu_layers=50,
                    )
            print(" âœ…")

            # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆåˆå›å®Ÿè¡Œã®ãƒ¡ãƒ¢ãƒªã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³é…å»¶ã‚’æ’é™¤ï¼‰
            print("   ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œä¸­...", end="", flush=True)
            self._warmup()
            print(" âœ…")

            print("âœ… åˆæœŸåŒ–å®Œäº†\n")

        except Exception as e:
            print(f"\nâŒ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e}")
            raise

    def _warmup(self):
        """ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œ"""
        try:
            _ = self.model("test", max_new_tokens=10)
        except Exception as e:
            print(f"\nâš ï¸  ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")

    def generate_with_interrupt(
        self,
        prompt: str,
        max_tokens: int = 200,
    ) -> tuple:
        """
        æ¨è«–å®Ÿè¡Œï¼ˆä¸­æ–­å¯¾å¿œï¼‰

        æˆ»ã‚Šå€¤: (ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ, æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ™‚åˆ»)
        """
        first_token_time = None
        generated_text = ""
        self.interrupt_flag = False

        try:
            # æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ™‚åˆ»ã‚’è¨˜éŒ²ï¼ˆç”Ÿæˆé–‹å§‹æ™‚ï¼‰
            first_token_time = time.perf_counter()

            # ç”Ÿæˆã‚’å®Ÿè¡Œ
            generated_text = self.model(
                prompt,
                max_new_tokens=max_tokens,
                stop=["<end_of_turn>"],
            )

            # ã€é‡è¦ã€‘ä¸­æ–­ãƒã‚§ãƒƒã‚¯ï¼ˆç”Ÿæˆå®Œäº†æ™‚ï¼‰
            if self.interrupt_flag:
                generated_text = "ã†ã‚“"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

        except Exception as e:
            print(f"âŒ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            first_token_time = time.perf_counter()

        return generated_text, first_token_time

    def interrupt_generation(self):
        """ç”Ÿæˆã‚’ä¸­æ–­"""
        self.interrupt_flag = True

    def measure_latency(self, num_iterations: int = 5) -> List[float]:
        """
        ä¸­æ–­å‘½ä»¤é€ä¿¡ â†’ Second stage æ¨è«–é–‹å§‹ã¾ã§ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’è¨ˆæ¸¬

        ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼š
        1. First iteration: çŸ­ã„å¿œç­”ã‚’ç”Ÿæˆï¼ˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼‰
        2. Second iterationï½: é•·ã„å¿œç­”ã‚’ç”Ÿæˆé–‹å§‹ â†’ ä¸­æ–­ â†’ Second stageé–‹å§‹
        """
        print("=" * 80)
        print(f"ğŸ”¬ ãƒã‚¤ãƒ†ã‚£ãƒ– Gemma-3-4B ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰è¨ˆæ¸¬ï¼ˆ{num_iterations}å›æ¸¬å®šï¼‰")
        print("=" * 80)

        latencies = []

        for iteration in range(num_iterations):
            print(f"\nã€æ¸¬å®š {iteration+1}/{num_iterations}ã€‘")

            if iteration == 0:
                # ============================================================
                # ã€åˆå›ã€‘Short stage: çŸ­ã„å¿œç­”ã‚’å®Œå…¨ç”Ÿæˆï¼ˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼‰
                # ============================================================
                print("  ğŸ”¥ ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—: çŸ­ã„å¿œç­”ã‚’ç”Ÿæˆä¸­...", end="", flush=True)

                short_prompt = "ã“ã‚“ã«ã¡ã¯ã€‚è¿”äº‹ã‚’ã—ã¦ãã ã•ã„ã€‚"

                start_time = time.perf_counter()
                result_short, _ = self.generate_with_interrupt(short_prompt, max_tokens=10)
                end_time = time.perf_counter()

                warmup_time = (end_time - start_time) * 1000
                print(f" âœ… ({warmup_time:.2f}ms)")
                print(f"   çµæœ: '{result_short[:50]}...'")
                time.sleep(0.5)
                continue

            # ============================================================
            # ã€æœ¬è¨ˆæ¸¬ã€‘First stage: é•·ã„å¿œç­”ã‚’ç”Ÿæˆé–‹å§‹
            # ============================================================
            print("  ğŸ“ First stage: é•·ã„å¿œç­”ã‚’ç”Ÿæˆä¸­...", end="", flush=True)

            first_stage_prompt = """ã‚ãªãŸã¯è©³ã—ã„èª¬æ˜ã‚’ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã—ã¦ã€ã§ãã‚‹ã ã‘è©³ã—ãã€é•·ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚
è¤‡æ•°ã®æ®µè½ã§ã€è©³ç´°ãªèƒŒæ™¯æƒ…å ±ã‚’å«ã‚ã¦ã€å¾¹åº•çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

è³ªå•: ãªãœç©ºã¯é’ã„ã®ã§ã™ã‹ï¼Ÿ

å›ç­”:"""

            # ã‚¿ã‚¤ãƒãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’è¨­å®šï¼ˆ500mså¾Œã«ä¸­æ–­ï¼‰
            self.interrupt_send_time = None
            self.second_stage_first_token_time = None

            def interrupt_after_500ms():
                time.sleep(0.5)  # 500ms ã§ä¸­æ–­
                self.interrupt_send_time = time.perf_counter()
                print(f"\n  â¸ï¸  [ä¸­æ–­å‘½ä»¤] 500msæ™‚ç‚¹ã§ä¸­æ–­ä¿¡å·ã‚’é€ä¿¡", end="", flush=True)
                self.interrupt_generation()

            interrupt_thread = threading.Thread(target=interrupt_after_500ms, daemon=True)
            interrupt_thread.start()

            # First stage ç”Ÿæˆã‚’å®Ÿè¡Œ
            start_time = time.perf_counter()
            result1, first_token_time1 = self.generate_with_interrupt(first_stage_prompt, max_tokens=200)
            end_time = time.perf_counter()
            first_stage_time = (end_time - start_time) * 1000

            interrupt_thread.join()
            print(" âœ…")

            # ============================================================
            # ã€Step 2ã€‘Second stage: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
            # ============================================================
            print("  ğŸ’¬ Second stage: å¿œç­”ç”Ÿæˆé–‹å§‹...", end="", flush=True)

            second_stage_prompt = "æ„Ÿè¬ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’çŸ­ãè¿°ã¹ã¦ãã ã•ã„ã€‚"

            # Second stage å¿œç­”é–‹å§‹
            start_time2 = time.perf_counter()
            result2, first_token_time2 = self.generate_with_interrupt(second_stage_prompt, max_tokens=30)
            self.second_stage_first_token_time = first_token_time2
            end_time2 = time.perf_counter()

            second_stage_time = (end_time2 - start_time2) * 1000

            print(" âœ…")

            # ============================================================
            # ã€è¨ˆæ¸¬çµæœã€‘
            # ============================================================
            if self.interrupt_send_time is not None and self.second_stage_first_token_time is not None:
                latency = (self.second_stage_first_token_time - self.interrupt_send_time) * 1000
                print(f"\n  ğŸ“Š è¨ˆæ¸¬çµæœ:")
                print(f"     â€¢ First stage ç·æ™‚é–“: {first_stage_time:.2f}ms")
                print(f"     â€¢ Second stage ç·æ™‚é–“: {second_stage_time:.2f}ms")
                print(f"     â€¢ ä¸­æ–­å‘½ä»¤é€ä¿¡ï½SSæ¨è«–é–‹å§‹: {latency:.2f}ms")
                print(f"     â€¢ First stage ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆé•·: {len(result1)}æ–‡å­—")
                print(f"     â€¢ Second stage ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆé•·: {len(result2)}æ–‡å­—")

                latencies.append(latency)
            else:
                print(f"\n  âš ï¸  è¨ˆæ¸¬å¤±æ•—ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¨˜éŒ²ãªã—ï¼‰")

            time.sleep(0.5)

        return latencies


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\n" + "=" * 80)
    print("ğŸ”¬ ãƒã‚¤ãƒ†ã‚£ãƒ–ï¼ˆOllama ãªã—ï¼‰Gemma-3-4B ç›´æ¥æ¨è«–ãƒ†ã‚¹ãƒˆ")
    print("=" * 80)

    try:
        # åˆæœŸåŒ–
        validator = TrueNativeGemma3Validator()

        # è¨ˆæ¸¬ã‚’å®Ÿè¡Œ
        latencies = validator.measure_latency(num_iterations=5)

        # çµ±è¨ˆåˆ†æ
        print("\n" + "=" * 80)
        print("ğŸ“ˆ çµ±è¨ˆåˆ†æ")
        print("=" * 80)

        if latencies:
            print(f"\nğŸ“Š å€‹åˆ¥æ¸¬å®šå€¤: {[f'{x:.2f}ms' for x in latencies]}")
            print(f"  â€¢ æœ€å°å€¤: {min(latencies):.2f}ms")
            print(f"  â€¢ æœ€å¤§å€¤: {max(latencies):.2f}ms")
            print(f"  â€¢ å¹³å‡å€¤: {statistics.mean(latencies):.2f}ms")
            print(f"  â€¢ ä¸­å¤®å€¤: {statistics.median(latencies):.2f}ms")
            if len(latencies) > 1:
                print(f"  â€¢ æ¨™æº–åå·®: {statistics.stdev(latencies):.2f}ms")

            print("\nğŸ’¡ åˆ†æ:")
            mean_latency = statistics.mean(latencies)

            # Ollama REST API ã¨ã®æ¯”è¼ƒ
            rest_api_latency = 156.7  # å‰å›ã® REST API è¨ˆæ¸¬å€¤
            native_reduction = rest_api_latency - mean_latency
            reduction_percent = (native_reduction / rest_api_latency) * 100

            print(f"\nğŸ“Š æ¨è«–æ–¹å¼ã®æ¯”è¼ƒ:")
            print(f"  â€¢ REST API çµŒç”±ï¼ˆollamaï¼‰: {rest_api_latency:.2f}ms")
            print(f"  â€¢ ãƒã‚¤ãƒ†ã‚£ãƒ–ç›´æ¥å®Ÿè¡Œ: {mean_latency:.2f}ms")
            print(f"  â€¢ å‰Šæ¸›: {native_reduction:.2f}ms ({reduction_percent:.1f}%)")

            if native_reduction > 20:
                print(f"\nâœ… ãƒã‚¤ãƒ†ã‚£ãƒ–å®Ÿè¡Œã§å¤§å¹…ãªå‰Šæ¸›ã‚’ç¢ºèª")
                print(f"   æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: {mean_latency:.0f}ms")
                print(f"   API/IPC ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: {native_reduction:.0f}ms")
            elif native_reduction > 0:
                print(f"\nâš ï¸  è‹¥å¹²ã®å‰Šæ¸›ã‚’ç¢ºèª")
                print(f"   â†’ ãƒã‚¤ãƒ†ã‚£ãƒ–ç›´æ¥å®Ÿè¡Œã§ã‚‚å¿…é ˆã®åˆæœŸåŒ–æ™‚é–“: {mean_latency:.0f}ms")
            else:
                print(f"\nâš ï¸  å‰Šæ¸›ãªã—")
                print(f"   â†’ ãƒã‚¤ãƒ†ã‚£ãƒ–å®Ÿè¡Œã‚‚åŒç­‰ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰")

        else:
            print("\nâŒ è¨ˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãªã—")

        print("\n" + "=" * 80)
        print("âœ… è¨ˆæ¸¬å®Œäº†")
        print("=" * 80)

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
