### turnTaking.py ###
""""
æ–°ä»•æ§˜ - å¸¸æ™‚æ¨è«–ç‰ˆ
- 100msã”ã¨ã«å¸¸ã«ãƒ¢ãƒ‡ãƒ«æ¨è«–ã‚’å®Ÿè¡Œ
- ç„¡å£°åŒºé–“ã®å¾…æ©Ÿãªã—
- 5ç§’é–“ã®ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§å¸¸æ™‚ã‚¿ãƒ¼ãƒ³ãƒ†ã‚¤ã‚­ãƒ³ã‚°åˆ¤å®š
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ã®å‘ä¸Š
"""
import numpy as np
import webrtcvad
import time
import queue
import sys
from scipy.io.wavfile import write
import torch
import torch.nn as nn
from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor
import transformers
from datetime import datetime
transformers.logging.set_verbosity_error()

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ¥ãƒ¼ï¼ˆros2_turn_taking.py ã‹ã‚‰å…±æœ‰ï¼‰
stream_queue = queue.Queue()
turn_taking_result_queue = queue.Queue()

THRESHOLD = 0.75

def push_audio_data(data):
    stream_queue.put(data)
    sys.stdout.flush()

def get_audio_data():
    if not stream_queue.empty():
        data = stream_queue.get()
        sys.stdout.flush()
        return data
    sys.stdout.flush()
    return None

class TurnTakingModel:
    def __init__(self, model_id="SiRoZaRuPa/japanese-wav2vec2-base-turntaking-CSJ"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"[TurnTaking] ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        self.model = Wav2Vec2ForSequenceClassification.from_pretrained(
            model_id, token=True
        ).to(self.device)
        self.model.eval()
        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
            model_id, token=True
        )
        print("[TurnTaking] ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

    def predict(self, audio, threshold=0.75):
        inputs = self.feature_extractor(
            audio, sampling_rate=16000, return_tensors="pt"
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        with torch.no_grad():
            output = self.model(**inputs).logits
        sigmoid = nn.Sigmoid()
        probability = float(sigmoid(output)[0])
        pred = 0 if probability < threshold else 1
        return pred, probability

def TurnTaking():
    model = TurnTakingModel()
    
    sample_rate = 16000
    frame_duration = 10  # ms
    CHUNK = int(sample_rate * frame_duration / 1000)  # 160ã‚µãƒ³ãƒ—ãƒ«
    
    # å¸¸æ™‚æ¨è«–è¨­å®š
    inference_interval_ms = 100  # 100msã”ã¨ã«æ¨è«–
    inference_interval_samples = int(sample_rate * inference_interval_ms / 1000)  # 1600ã‚µãƒ³ãƒ—ãƒ«
    
    sound_buffer = np.empty(0, dtype='float32')
    last_inference_time = time.time()
    samples_since_last_inference = 0
    inference_count = 0

    sys.stdout.write("[INFO] TurnTaking started - å¸¸æ™‚æ¨è«–ãƒ¢ãƒ¼ãƒ‰\n")
    sys.stdout.write(f"[INFO] æ¨è«–é–“éš”: {inference_interval_ms}ms\n")
    sys.stdout.flush()

    while True:
        try:
            audiodata = get_audio_data()
            if audiodata is None:
                time.sleep(0.001)  # 1mså¾…æ©Ÿ
                continue

            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
            sound_buffer = np.concatenate([sound_buffer, audiodata])
            samples_since_last_inference += len(audiodata)
            
            # 5.1ç§’ã‚’è¶…ãˆãŸåˆ†ã¯å‰Šé™¤ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼‰
            max_samples = int(5.1 * sample_rate)
            if len(sound_buffer) > max_samples:
                sound_buffer = sound_buffer[-max_samples:]

            # æ¨è«–æ¡ä»¶: 100msé–“éš” AND 5ç§’åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒè“„ç©
            current_time = time.time()
            should_inference = (
                len(sound_buffer) >= 5 * sample_rate and  # 5ç§’åˆ†ã®ãƒ‡ãƒ¼ã‚¿
                samples_since_last_inference >= inference_interval_samples  # 100msé–“éš”
            )

            if should_inference:
                # æ¨è«–å®Ÿè¡Œ
                process_start_time = time.perf_counter()
                
                # æœ€æ–°5ç§’åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã§æ¨è«–
                inference_data = sound_buffer[-int(5 * sample_rate):]
                
                # æ­£è¦åŒ–
                if np.abs(inference_data).max() > 0:
                    sound_comp = inference_data / np.abs(inference_data).max()
                else:
                    sound_comp = inference_data
                
                # æ¨è«–å®Ÿè¡Œ
                pred, probability = model.predict(sound_comp, threshold=THRESHOLD)
                turn_taking_result_queue.put((pred, probability))
                
                processing_time = (time.perf_counter() - process_start_time) * 1000
                inference_count += 1
                
                # é…å»¶æ¸¬å®šãƒ­ã‚°å‡ºåŠ›
                timestamp_str = datetime.now().strftime("%H:%M:%S.%f")[:-3]
                sys.stdout.write(f"[ğŸ”„ TT_INFERENCE] {timestamp_str} | æ¨è«–#{inference_count} | å‡¦ç†æ™‚é–“: {processing_time:.1f}ms | ç¢ºä¿¡åº¦: {probability:.3f} | åˆ¤å®š: {'ã‚¿ãƒ¼ãƒ³äº¤ä»£' if pred else 'ç¶™ç¶š'}\n")
                sys.stdout.flush()
                
                # æ¨è«–ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ãƒªã‚»ãƒƒãƒˆ
                samples_since_last_inference = 0
                last_inference_time = current_time
                
                # ãƒ‡ãƒãƒƒã‚°ç”¨: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼ˆæœ€åˆã®10å›ã®ã¿ï¼‰
                if inference_count <= 10:
                    try:
                        write(f'model_input_sound_{inference_count}.wav', sample_rate, 
                              (sound_comp * 32767).astype(np.int16))
                    except Exception as e:
                        pass  # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–

        except KeyboardInterrupt:
            sys.stdout.write(f"\n[INFO] TurnTaking terminated - ç·æ¨è«–å›æ•°: {inference_count}\n")
            sys.stdout.flush()
            break
        except Exception as e:
            sys.stdout.write(f"[ERROR] TurnTaking error: {e}\n")
            sys.stdout.flush()
            time.sleep(0.1)