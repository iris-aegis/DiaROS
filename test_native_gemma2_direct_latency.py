#!/usr/bin/env python3
"""
Native Gemma-2-2B-IT ç›´æ¥æ¨è«–ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰è¨ˆæ¸¬ï¼ˆã‚ˆã‚Šå®‰å®šã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰

Ollama ã‚’ä¸€åˆ‡ä½¿ã‚ãšã€transformers ã‚’ä½¿ã£ã¦ãƒ­ãƒ¼ã‚«ãƒ«ã§ç›´æ¥æ¨è«–ã—ã€
ä¸­æ–­ï½Second stage é–‹å§‹ã¾ã§ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’è¨ˆæ¸¬

ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: google/gemma-2-2b-it (gemma-3 ã‚ˆã‚Šå®‰å®š)
"""

import torch
import time
import statistics
import threading
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List

class NativeGemma2DirectValidator:
    def __init__(self, model_name: str = "google/gemma-2-2b-it"):
        """åˆæœŸåŒ–"""
        print(f"ğŸ“¦ Native Gemma-2-2B-IT åˆæœŸåŒ–ä¸­: {model_name}")
        print("   (ã“ã‚Œã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™)")

        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {self.device}")

        try:
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿
            print("   ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...", end="", flush=True)
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.tokenizer.pad_token = self.tokenizer.eos_token
            print(" âœ…")

            # ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
            print("   ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...", end="", flush=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
            )

            # CPU ã®å ´åˆã¯æ˜ç¤ºçš„ã«ç§»å‹•
            if self.device == "cpu":
                self.model = self.model.to(self.device)

            # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            self.model.eval()
            print(" âœ…")

            # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆåˆå›å®Ÿè¡Œã®ãƒ¡ãƒ¢ãƒªã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³é…å»¶ã‚’æ’é™¤ï¼‰
            print("   ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œä¸­...", end="", flush=True)
            self._warmup()
            print(" âœ…")

            print("âœ… åˆæœŸåŒ–å®Œäº†\n")

        except Exception as e:
            print(f"\nâŒ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e}")
            raise

    def _warmup(self):
        """ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œ"""
        with torch.no_grad():
            inputs = self.tokenizer(
                "test",
                return_tensors="pt",
                max_length=512,
                truncation=True
            ).to(self.device)
            _ = self.model.generate(
                inputs.input_ids,
                max_new_tokens=10,
                temperature=0.7,
                do_sample=True
            )
        if self.device == "cuda":
            torch.cuda.empty_cache()

    def generate_with_timing(
        self,
        prompt: str,
        max_tokens: int = 200,
        temperature: float = 0.7
    ) -> tuple:
        """
        æ¨è«–å®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒŸãƒ³ã‚°è¨ˆæ¸¬ä»˜ãï¼‰

        æˆ»ã‚Šå€¤: (ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ, ç”Ÿæˆé–‹å§‹æ™‚åˆ», ç”Ÿæˆçµ‚äº†æ™‚åˆ»)
        """
        start_time = time.perf_counter()
        generated_text = ""

        try:
            with torch.no_grad():
                # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
                inputs = self.tokenizer(
                    prompt,
                    return_tensors="pt",
                    max_length=512,
                    truncation=True
                ).to(self.device)

                input_ids = inputs.input_ids

                # generate ã‚’å®Ÿè¡Œ
                outputs = self.model.generate(
                    input_ids,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    do_sample=True,
                    top_p=0.9,
                    top_k=50,
                    pad_token_id=self.tokenizer.eos_token_id,
                )

                # ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
                generated_ids = outputs[0, input_ids.shape[1]:]
                generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)

                if self.device == "cuda":
                    torch.cuda.empty_cache()

        except Exception as e:
            print(f"âŒ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

        end_time = time.perf_counter()
        return generated_text, start_time, end_time

    def measure_latency(self, num_iterations: int = 5) -> List[float]:
        """
        æ¨è«–é–‹å§‹ï½çµ‚äº†ã¾ã§ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’è¨ˆæ¸¬

        ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼š
        1. First iteration: çŸ­ã„å¿œç­”ã‚’ç”Ÿæˆï¼ˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼‰
        2. Second iterationï½: é•·ã„ã¨çŸ­ã„å¿œç­”ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¯”è¼ƒ
        """
        print("=" * 80)
        print(f"ğŸ”¬ Native Gemma-2-2B-IT ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰è¨ˆæ¸¬ï¼ˆ{num_iterations}å›æ¸¬å®šï¼‰")
        print("=" * 80)

        latencies = []

        for iteration in range(num_iterations):
            print(f"\nã€æ¸¬å®š {iteration+1}/{num_iterations}ã€‘")

            if iteration == 0:
                # ============================================================
                # ã€åˆå›ã€‘Short response: çŸ­ã„å¿œç­”ã‚’ç”Ÿæˆï¼ˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼‰
                # ============================================================
                print("  ğŸ”¥ ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—: çŸ­ã„å¿œç­”ã‚’ç”Ÿæˆä¸­...", end="", flush=True)

                short_prompt = "ã“ã‚“ã«ã¡ã¯ã€‚è¿”äº‹ã‚’ã—ã¦ãã ã•ã„ã€‚"

                result_short, start_time, end_time = self.generate_with_timing(
                    short_prompt, max_tokens=10
                )

                warmup_time = (end_time - start_time) * 1000
                print(f" âœ… ({warmup_time:.2f}ms)")
                print(f"   çµæœ: '{result_short[:50]}...'")
                time.sleep(0.5)
                continue

            # ============================================================
            # ã€æœ¬è¨ˆæ¸¬ã€‘Short vs Long æ¯”è¼ƒ
            # ============================================================
            print("  ğŸ“ Short response: çŸ­ã„å¿œç­”ã‚’ç”Ÿæˆä¸­...", end="", flush=True)

            short_prompt = "ã“ã‚“ã«ã¡ã¯ã€‚ç°¡æ½”ã«è¿”äº‹ã‚’ã—ã¦ãã ã•ã„ã€‚"

            result_short, start_short, end_short = self.generate_with_timing(
                short_prompt, max_tokens=20
            )

            short_time = (end_short - start_short) * 1000
            print(f" âœ… ({short_time:.2f}ms)")

            time.sleep(0.2)

            # ============================================================
            # Long response
            # ============================================================
            print("  ğŸ’¬ Long response: é•·ã„å¿œç­”ã‚’ç”Ÿæˆä¸­...", end="", flush=True)

            long_prompt = """ã‚ãªãŸã¯è©³ã—ã„èª¬æ˜ã‚’ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã—ã¦ã€ã§ãã‚‹ã ã‘è©³ã—ãã€é•·ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚
è¤‡æ•°ã®æ®µè½ã§ã€è©³ç´°ãªèƒŒæ™¯æƒ…å ±ã‚’å«ã‚ã¦ã€å¾¹åº•çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

è³ªå•: ãªãœç©ºã¯é’ã„ã®ã§ã™ã‹ï¼Ÿ

å›ç­”:"""

            result_long, start_long, end_long = self.generate_with_timing(
                long_prompt, max_tokens=100
            )

            long_time = (end_long - start_long) * 1000
            print(f" âœ… ({long_time:.2f}ms)")

            # ============================================================
            # è¨ˆæ¸¬çµæœ
            # ============================================================
            latency_diff = long_time - short_time
            print(f"\n  ğŸ“Š è¨ˆæ¸¬çµæœ:")
            print(f"     â€¢ Short response: {short_time:.2f}ms")
            print(f"     â€¢ Long response: {long_time:.2f}ms")
            print(f"     â€¢ ç”Ÿæˆæ™‚é–“å¢—åŠ : {latency_diff:.2f}ms")
            print(f"     â€¢ Short ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(result_short)}æ–‡å­—")
            print(f"     â€¢ Long ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(result_long)}æ–‡å­—")

            latencies.append(long_time)
            time.sleep(0.2)

        return latencies


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\n" + "=" * 80)
    print("ğŸ”¬ Native Gemma-2-2B-IT ç›´æ¥æ¨è«–ãƒ†ã‚¹ãƒˆ")
    print("=" * 80)

    try:
        # åˆæœŸåŒ–
        validator = NativeGemma2DirectValidator()

        # è¨ˆæ¸¬ã‚’å®Ÿè¡Œ
        latencies = validator.measure_latency(num_iterations=5)

        # çµ±è¨ˆåˆ†æ
        print("\n" + "=" * 80)
        print("ğŸ“ˆ çµ±è¨ˆåˆ†æ")
        print("=" * 80)

        if latencies:
            print(f"\nğŸ“Š å€‹åˆ¥æ¸¬å®šå€¤: {[f'{x:.2f}ms' for x in latencies]}")
            print(f"  â€¢ æœ€å°å€¤: {min(latencies):.2f}ms")
            print(f"  â€¢ æœ€å¤§å€¤: {max(latencies):.2f}ms")
            print(f"  â€¢ å¹³å‡å€¤: {statistics.mean(latencies):.2f}ms")
            print(f"  â€¢ ä¸­å¤®å€¤: {statistics.median(latencies):.2f}ms")
            if len(latencies) > 1:
                print(f"  â€¢ æ¨™æº–åå·®: {statistics.stdev(latencies):.2f}ms")

            print("\nğŸ’¡ åˆ†æ:")
            mean_latency = statistics.mean(latencies)

            # Ollama ã¨ã®æ¯”è¼ƒ
            ollama_latency_total = 500.7  # å‰å›æ¸¬å®šã® First stage ç·æ™‚é–“ï¼ˆ500msç”Ÿæˆï¼‰
            native_reduction = ollama_latency_total - mean_latency
            reduction_percent = (native_reduction / ollama_latency_total) * 100

            print(f"\nğŸ“Š æ¨è«–æ–¹å¼ã®æ¯”è¼ƒ:")
            print(f"  â€¢ Ollamaï¼ˆé•·å¿œç­”ï¼‰: ~500.7ms")
            print(f"  â€¢ ãƒã‚¤ãƒ†ã‚£ãƒ–ç›´æ¥å®Ÿè¡Œ: {mean_latency:.2f}ms")
            print(f"  â€¢ å‰Šæ¸›: {native_reduction:.2f}ms ({reduction_percent:.1f}%)")

            if mean_latency < 200:
                print(f"\nâœ… ãƒã‚¤ãƒ†ã‚£ãƒ–å®Ÿè¡Œã§å¤§å¹…ãªå‰Šæ¸›ã‚’ç¢ºèª")
                print(f"   â†’ Ollama ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: ç´„ {native_reduction:.0f}ms")
            elif mean_latency < 350:
                print(f"\nâ–³ ä¸­ç¨‹åº¦ã®å‰Šæ¸›")
                print(f"   â†’ Ollama ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: ç´„ {native_reduction:.0f}ms")
            else:
                print(f"\nâš ï¸  å‰Šæ¸›ãŒå°ã•ã„")
                print(f"   â†’ ãƒã‚¤ãƒ†ã‚£ãƒ–å®Ÿè¡Œã‚‚OllamaåŒç­‰ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰")

        else:
            print("\nâŒ è¨ˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãªã—")

        print("\n" + "=" * 80)
        print("âœ… è¨ˆæ¸¬å®Œäº†")
        print("=" * 80)

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
