#!/usr/bin/env python3
"""
script1.wavã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã—ã¦Silero VADã®å‡¦ç†æ™‚é–“ã‚’è¨ˆæ¸¬
- ãƒ—ãƒ­ã‚°ãƒ©ãƒ é–‹å§‹ï½çµ‚äº†ã¾ã§ã®å…¨ä½“æ™‚é–“ã‚’è¨ˆæ¸¬
- 1ãƒ•ãƒ¬ãƒ¼ãƒ (32ms)ã”ã¨ã®å¹³å‡å‡¦ç†æ™‚é–“ã‚’è¨ˆç®—
"""

import numpy as np
import soundfile as sf
import time
import torch
import sys
import warnings
import datetime

# è­¦å‘Šã‚’æŠ‘åˆ¶
warnings.filterwarnings("ignore")

def load_silero_vad():
    """SileroVADãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
    try:
        # PyTorchã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’åˆ¶é™ï¼ˆCPUãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ï¼‰
        torch.set_num_threads(1)

        print("[INFO] SileroVADãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­... (V5 ONNX)")
        # ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ã†ã‚ˆã†ã«è¨­å®šï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®å ´åˆï¼‰
        model, utils = torch.hub.load(
            repo_or_dir='snakers4/silero-vad',
            model='silero_vad',
            force_reload=False,
            onnx=True,  # â˜…V5 ONNX ã«å¤‰æ›´
            trust_repo=True
        )

        get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks = utils
        print("[INFO] SileroVADãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸ (V5 ONNX)")
        
        return model, VADIterator
    except Exception as e:
        print(f"[ERROR] SileroVADãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {e}")
        sys.exit(1)

def benchmark_silero_realtime(audio_file):
    start_time = datetime.datetime.now() # é–‹å§‹æ™‚åˆ»ã‚’å–å¾—
    """script1.wavã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã—ã¦æ™‚é–“ã‚’è¨ˆæ¸¬"""
    print("=" * 80)
    print("Silero VAD ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    print("=" * 80)

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    print(f"\n[INFO] éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­: {audio_file}")
    try:
        audio_data, sr = sf.read(audio_file)
    except Exception as e:
        print(f"[ERROR] éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

    print(f"  ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ: {sr}Hz")
    print(f"  éŸ³å£°é•·: {len(audio_data)}ã‚µãƒ³ãƒ—ãƒ« ({len(audio_data)/sr:.2f}ç§’)")

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆç¢ºèª
    if sr != 16000:
        print("[WARNING] ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãŒ16000Hzã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚Silero VADã¯16000Hzã¾ãŸã¯8000Hzã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        # ç°¡æ˜“çš„ãªãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆé–“å¼•ãï¼‰
        if sr == 48000:
            audio_data = audio_data[::3]
            sr = 16000
            print("  -> 16000Hzã«ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã—ãŸ")
        elif sr == 32000:
            audio_data = audio_data[::2]
            sr = 16000
            print("  -> 16000Hzã«ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã—ãŸ")

    # ãƒ¢ãƒãƒ©ãƒ«å¤‰æ›
    if audio_data.ndim > 1:
        audio_data = np.mean(audio_data, axis=1)

    # float32ã«å¤‰æ›
    audio_data = audio_data.astype(np.float32)

    # Silero VAD ãƒ­ãƒ¼ãƒ‰
    model, VADIterator = load_silero_vad()

    # VADè¨­å®š
    # Silero VADã¯ 512, 1024, 1536 samples ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã‚’ã‚µãƒãƒ¼ãƒˆ
    # 16kHzã®å ´åˆ: 512=32ms, 1024=64ms, 1536=96ms
    frame_duration = 32  # ms
    CHUNK = 512 # 32ms at 16kHz

    # VADIteratoråˆæœŸåŒ–
    vad_iterator = VADIterator(model, threshold=0.5, sampling_rate=sr)

    print(f"\n[INFO] VADè¨­å®š:")
    print(f"  - ãƒ•ãƒ¬ãƒ¼ãƒ é•·: {frame_duration}ms ({CHUNK}ã‚µãƒ³ãƒ—ãƒ«)")
    print(f"  - ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ: {sr}Hz")

    # ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã¨VADåˆ¤å®šçµæœã‚’è¨˜éŒ²
    total_frames = 0
    speech_frames = 0
    silence_frames = 0

    # çŠ¶æ…‹ç®¡ç†
    previous_has_speech = None  # å‰ãƒ•ãƒ¬ãƒ¼ãƒ ãŒéŸ³å£°ã‚’å«ã‚“ã§ã„ãŸã‹
    silent_start_time_seconds = None  # ç„¡éŸ³é–‹å§‹æ™‚åˆ»
    speech_start_time_seconds = None  # éŸ³å£°é–‹å§‹æ™‚åˆ»
    sound_count = 0  # é€£ç¶šéŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚«ã‚¦ãƒ³ãƒˆ
    silent_count = 0  # é€£ç¶šç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã‚«ã‚¦ãƒ³ãƒˆ

    # å‡¦ç†æ™‚é–“è¨ˆæ¸¬ç”¨ãƒªã‚¹ãƒˆ
    frame_processing_times = []

    print(f"\n[INFO] ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†é–‹å§‹...")
    print("-" * 80)

    # ãƒ—ãƒ­ã‚°ãƒ©ãƒ å…¨ä½“ã®é–‹å§‹æ™‚åˆ»
    program_start_time = time.time()

    # ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†
    # ç«¯æ•°ã¯åˆ‡ã‚Šæ¨ã¦ã¾ãŸã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆSileroã¯å›ºå®šé•·ã‚’å¥½ã‚€ãŸã‚ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°æ¨å¥¨ï¼‰
    total_chunks = (len(audio_data) + CHUNK - 1) // CHUNK

    for chunk_idx in range(total_chunks):
        # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†é–‹å§‹æ™‚åˆ»
        frame_start_time = time.time()

        # ç›¸å¯¾æ™‚åˆ»ã‚’è¨ˆç®—ï¼ˆç§’ï¼‰
        current_time_sec = (chunk_idx * frame_duration) / 1000.0

        # ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
        start_idx = chunk_idx * CHUNK
        end_idx = min(start_idx + CHUNK, len(audio_data))
        audiodata = audio_data[start_idx:end_idx]

        # ä¸è¶³åˆ†ã‚’ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        if len(audiodata) < CHUNK:
            audiodata = np.pad(audiodata, (0, CHUNK - len(audiodata)))

        # Torch Tensorã«å¤‰æ›
        chunk_tensor = torch.from_numpy(audiodata)

        try:
            # Silero VADå‡¦ç†
            # vad_iteratorã¯éŸ³å£°åŒºé–“ã®é–‹å§‹/çµ‚äº†è¾æ›¸ã‚’è¿”ã™
            # {'start': timestamp} or {'end': timestamp}
            speech_dict = vad_iterator(chunk_tensor, return_seconds=True)

            # éŸ³å£°ç¢ºç‡ã‚’å–å¾—ï¼ˆVADIteratorã®å†…éƒ¨ã§è¨ˆç®—ï¼‰
            # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ã€speech_dictãŒè¿”ã•ã‚ŒãŸï¼ˆéŸ³å£°æ¤œå‡ºï¼‰ã‹ã©ã†ã‹ã§åˆ¤å®š
            has_speech = speech_dict is not None and (('start' in speech_dict) or ('end' in speech_dict))

            # çŠ¶æ…‹é·ç§»ã®æ¤œå‡º
            if previous_has_speech is not None and previous_has_speech != has_speech:
                if not has_speech:
                    # éŸ³å£°ã‹ã‚‰ç„¡éŸ³ã¸
                    print(f"[{current_time_sec:.3f}s] éŸ³å£°çµ‚äº† â†’ ç„¡éŸ³é–‹å§‹")
                    silent_start_time_seconds = current_time_sec
                    silent_count = 0
                else:
                    # ç„¡éŸ³ã‹ã‚‰éŸ³å£°ã¸
                    if silent_start_time_seconds is not None:
                        silent_duration = current_time_sec - silent_start_time_seconds
                        print(f"[{current_time_sec:.3f}s] ç„¡éŸ³çµ‚äº† â†’ éŸ³å£°é–‹å§‹ (ç„¡éŸ³ç¶™ç¶š: {silent_duration*1000:.0f}ms)")
                    else:
                        print(f"[{current_time_sec:.3f}s] éŸ³å£°é–‹å§‹")
                    silent_start_time_seconds = None
                    sound_count = 0

            # VADè©³ç´°ã‚¤ãƒ™ãƒ³ãƒˆã®å‡ºåŠ›
            if speech_dict is not None:
                if 'start' in speech_dict:
                    speech_start_time_seconds = current_time_sec
                if 'end' in speech_dict:
                    if speech_start_time_seconds is not None:
                        speech_duration = current_time_sec - speech_start_time_seconds
                        print(f"[{current_time_sec:.3f}s] âœ“ éŸ³å£°åŒºé–“ç¢ºå®šï¼ˆç¶™ç¶š: {speech_duration*1000:.0f}msï¼‰")

            if has_speech:
                speech_frames += 1
                sound_count += 1
            else:
                silence_frames += 1
                silent_count += 1

                # 100msç„¡éŸ³æ¤œå‡ºï¼ˆ100msã«é”ã—ãŸæ™‚ã®ã¿å‡ºåŠ›ï¼‰
                if silent_count == int(100 / frame_duration):
                    if silent_start_time_seconds is not None:
                        silent_duration = current_time_sec - silent_start_time_seconds
                        print(f"[{current_time_sec:.3f}s] âœ“ 100msç„¡éŸ³æ¤œå‡ºå®Œäº†ï¼ˆç¶™ç¶š: {silent_duration*1000:.0f}msï¼‰")

            total_frames += 1
            previous_has_speech = has_speech

        except Exception as e:
            print(f"[ERROR] VADå‡¦ç†ã‚¨ãƒ©ãƒ¼ at {current_time_sec:.3f}s: {e}")
            continue

        # é€²æ—è¡¨ç¤º
        if (chunk_idx + 1) % (max(1, total_chunks // 10)) == 0:
            progress = ((chunk_idx + 1) / total_chunks) * 100
            sys.stdout.write(f"\r[INFO] å‡¦ç†ä¸­... {progress:.0f}%")
            sys.stdout.flush()

        # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†çµ‚äº†æ™‚åˆ»
        frame_end_time = time.time()
        frame_processing_time_ms = (frame_end_time - frame_start_time) * 1000
        frame_processing_times.append(frame_processing_time_ms)

        # ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†æ™‚é–“ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡ºåŠ›
        print(f"[{current_time_sec:.3f}s] å‡¦ç†æ™‚é–“: {frame_processing_time_ms:.4f}ms")

    # ãƒ—ãƒ­ã‚°ãƒ©ãƒ å…¨ä½“ã®çµ‚äº†æ™‚åˆ»
    program_end_time = time.time()

    print("\r" + " " * 40 + "\r", end="")  # é€²æ—è¡¨ç¤ºã‚’ã‚¯ãƒªã‚¢
    print("-" * 80)

    # è¨ˆæ¸¬çµæœ
    # frame_processing_timesã¯å„ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‡¦ç†æ™‚é–“ã®ãƒªã‚¹ãƒˆ
    if len(frame_processing_times) > 0:
        total_processing_time = sum(frame_processing_times)  # ms
    else:
        total_processing_time = 0

    program_total_time = (program_end_time - program_start_time) * 1000  # ms

    print(f"\n[çµæœ] è¨ˆæ¸¬æƒ…å ±:")
    print(f"  ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {total_frames}")
    print(f"  éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ : {speech_frames} ({speech_frames/total_frames*100:.1f}%)" if total_frames > 0 else "  éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ : 0 (0.0%)")
    print(f"  ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ : {silence_frames} ({silence_frames/total_frames*100:.1f}%)" if total_frames > 0 else "  ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ : 0 (0.0%)")

    print(f"\n[çµæœ] å‡¦ç†æ™‚é–“:")
    print(f"  âœ… å…¨ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†æ™‚é–“ã®åˆè¨ˆ: {total_processing_time:.2f}ms")
    print(f"  ğŸ“Š ãƒ—ãƒ­ã‚°ãƒ©ãƒ å…¨ä½“æ™‚é–“: {program_total_time:.2f}ms")
    print(f"  â±ï¸ å®ŸéŸ³å£°æ™‚é–“: {len(audio_data)/sr*1000:.2f}ms")

    # 1ãƒ•ãƒ¬ãƒ¼ãƒ ã‚ãŸã‚Šã®å¹³å‡å‡¦ç†æ™‚é–“
    avg_time_per_frame = total_processing_time / total_frames if total_frames > 0 else 0

    print(f"\n[çµæœ] ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ã®å‡¦ç†æ™‚é–“çµ±è¨ˆ:")
    print(f"  1ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆ{frame_duration}msï¼‰ã‚ãŸã‚Šã®å¹³å‡å‡¦ç†æ™‚é–“: {avg_time_per_frame:.4f}ms")
    print(f"  ç›¸å¯¾å‡¦ç†è² è·: {avg_time_per_frame/frame_duration*100:.2f}% (ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¦ä»¶: < 1%)")

    # è©³ç´°ãªçµ±è¨ˆæƒ…å ±
    if len(frame_processing_times) > 0:
        min_frame_time = min(frame_processing_times)
        max_frame_time = max(frame_processing_times)
        median_frame_time = np.median(frame_processing_times)
        p95_frame_time = np.percentile(frame_processing_times, 95)
        p99_frame_time = np.percentile(frame_processing_times, 99)

        print(f"\n[è©³ç´°] ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†æ™‚é–“ã®åˆ†å¸ƒ:")
        print(f"  æœ€å°: {min_frame_time:.4f}ms")
        print(f"  æœ€å¤§: {max_frame_time:.4f}ms")
        print(f"  ä¸­å¤®å€¤: {median_frame_time:.4f}ms")
        print(f"  P95: {p95_frame_time:.4f}ms")
        print(f"  P99: {p99_frame_time:.4f}ms")

    # 1ç§’ã‚ãŸã‚Šã®æ¨å®šå‡¦ç†æ™‚é–“
    # 1ç§’é–“ã«å«ã¾ã‚Œã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ æ•° = 1000ms / frame_duration_ms
    frames_per_sec = 1000 / frame_duration
    estimated_1sec = avg_time_per_frame * frames_per_sec

    print(f"\n[æ¨å®š] 1ç§’é–“ã®å‡¦ç†æ™‚é–“:")
    print(f"  æ¨å®šå‡¦ç†æ™‚é–“: {estimated_1sec:.2f}ms")
    print(f"  ç›¸å¯¾å‡¦ç†è² è·: {estimated_1sec/1000*100:.2f}% (ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¦ä»¶: < 1%)")

    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ¤å®š
    print(f"\n[åˆ¤å®š]")
    # å‡¦ç†æ™‚é–“ãŒãƒ•ãƒ¬ãƒ¼ãƒ é•·ã‚ˆã‚ŠçŸ­ã‘ã‚Œã°ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯èƒ½
    # ã•ã‚‰ã«ä½™è£•ã‚’æŒã£ã¦ 1/10 (10%) ä»¥ä¸‹ãªã‚‰å®‰å…¨åœ
    if avg_time_per_frame < frame_duration * 0.1:
        print(f"  âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†å¯èƒ½ï¼ˆä½™è£•åº¦: {(frame_duration - avg_time_per_frame):.3f}ms/ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰")
    elif avg_time_per_frame < frame_duration:
        print(f"  âš ï¸ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†å¯èƒ½ã ãŒè² è·ãŒé«˜ã„ï¼ˆä½™è£•åº¦: {(frame_duration - avg_time_per_frame):.3f}ms/ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰")
    else:
        print(f"  âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ä¸å¯ï¼ˆè¶…é: {(avg_time_per_frame - frame_duration):.3f}ms/ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰")

    end_time = datetime.datetime.now() # çµ‚äº†æ™‚åˆ»ã‚’å–å¾—
    total_time = end_time - start_time # å·®åˆ†ã‚’è¨ˆç®—
    total_ms = total_time.total_seconds() * 1000

    return {
        'audio_file': audio_file,
        'total_frames': total_frames,
        'speech_frames': speech_frames,
        'silence_frames': silence_frames,
        'total_processing_time_ms': total_processing_time,
        'program_total_time_ms': program_total_time,
        'avg_time_per_frame_ms': avg_time_per_frame,
        'estimated_1sec_ms': estimated_1sec,
        'audio_duration_sec': len(audio_data) / sr,
        'realtime_capable': avg_time_per_frame < frame_duration,
        'frame_processing_times': frame_processing_times
    }



def main():
    start_time = datetime.datetime.now()  # é–‹å§‹æ™‚åˆ»ã‚’å–å¾—

    audio_file = '/workspace/script1.wav'

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    import os
    if not os.path.exists(audio_file):
        print(f"[ERROR] éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {audio_file}")
        return

    result = benchmark_silero_realtime(audio_file)

    end_time = datetime.datetime.now()  # çµ‚äº†æ™‚åˆ»ã‚’å–å¾—
    total_time = end_time - start_time  # å·®åˆ†ã‚’è¨ˆç®—
    total_ms = total_time.total_seconds() * 1000
    total_frames = result['total_frames']

    print("\n" + "=" * 80)
    print("çµ‚äº†")
    print("=" * 80)
    print(f"ãƒ—ãƒ­ã‚°ãƒ©ãƒ å…¨ä½“ã®å®Ÿè¡Œæ™‚é–“: {total_ms:.2f} milliseconds")  # å·®åˆ†ã‚’ãƒŸãƒªç§’ã§å‡ºåŠ›

    if total_frames > 0:
        avg_time_overall = total_ms / total_frames
        print(f"1ãƒ•ãƒ¬ãƒ¼ãƒ (32ms)ã‚ãŸã‚Šã®å¹³å‡å‡¦ç†æ™‚é–“ (å…¨ä½“): {avg_time_overall:.4f} milliseconds")

if __name__ == "__main__":
    main()
