# ä¸€æ—¦å±¥æ­´è«¦ã‚

# ============================================================
# ãƒ¢ãƒ‡ãƒ«è¨­å®š - ã“ã“ã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆ
# ============================================================
# ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ: "gpt-3.5-turbo" ã¾ãŸã¯ "gemma3:12b"
MODEL_NAME = "gemma3:12b"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gemma3:12b
# ============================================================

import requests
import json
import sys
import os
import time
import threading
from datetime import datetime, timedelta
from queue import Queue, Empty
from concurrent.futures import ThreadPoolExecutor
import openai
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import ChatOllama
from .timeTracker import get_time_tracker

class NaturalLanguageGeneration:
    def __init__(self):
        self.rc = { "word": "" }
        
        self.query = ""
        self.update_flag = False
        self.dialogue_history = []
        self.user_speak_is_final = False
        self.last_reply = ""  # ç”Ÿæˆã—ãŸå¯¾è©±æ–‡ã‚’ã“ã“ã«æ ¼ç´
        self.last_source_words = []  # å¯¾è©±ç”Ÿæˆã®å…ƒã«ã—ãŸéŸ³å£°èªè­˜çµæœã‚’æ ¼ç´
        
        # ROS2 bagè¨˜éŒ²ç”¨ã®è¿½åŠ æƒ…å ±
        self.last_request_id = 0
        self.last_worker_name = ""
        self.last_start_timestamp_ns = 0
        self.last_completion_timestamp_ns = 0
        self.last_inference_duration_ms = 0.0
        
        # æ–°ã—ã„æ™‚åˆ»æƒ…å ±ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åˆæœŸåŒ–
        self.request_id = 0
        self.worker_name = ""
        self.start_timestamp_ns = 0
        self.completion_timestamp_ns = 0
        self.inference_duration_ms = 0.0
        
        # æ¥ç¶šã‚¨ãƒ©ãƒ¼åˆ¶å¾¡ç”¨
        self.connection_error_count = 0
        self.last_connection_error_time = None
        self.connection_error_suppress_until = None
        
        # ã‚¿ã‚¤ãƒ ãƒˆãƒ©ãƒƒã‚«ãƒ¼åˆæœŸåŒ–
        self.time_tracker = get_time_tracker("nlg_pc")
        self.current_session_id = None
        
        # ä¸¦åˆ—å‡¦ç†è¨­å®šã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼ˆå˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹ç‰ˆï¼‰
        # self.inference_queue = Queue()  # æ¨è«–ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚­ãƒ¥ãƒ¼
        # self.result_queue = Queue()     # æ¨è«–çµæœã®ã‚­ãƒ¥ãƒ¼
        # self.request_counter = 0        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        self.last_request_time = None   # æœ€å¾Œã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚åˆ»
        self.last_inference_time = None # æœ€å¾Œã®æ¨è«–å®Ÿè¡Œæ™‚åˆ»
        self.inference_interval = 2.5   # æ¨è«–é–“éš”ï¼ˆç§’ï¼‰
        # self.executor = ThreadPoolExecutor(max_workers=3, thread_name_prefix="NLG-Worker")
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ä¸Šéƒ¨ã®MODEL_NAMEã‚’ä½¿ç”¨ï¼‰
        self.model_name = MODEL_NAME

        if self.model_name == "gemma3:12b":
            # Ollama gemma3:12bã®åˆæœŸåŒ–
            sys.stdout.write('[NLG] Ollama gemma3:12bãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...\n')
            sys.stdout.flush()
            self.ollama_model = ChatOllama(
                model=self.model_name,
                verbose=True,  # è©³ç´°ãƒ­ã‚°æœ‰åŠ¹åŒ–
                temperature=0.7,  # å¿œç­”ã®å¤šæ§˜æ€§
                top_p=0.9,  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¨­å®š
                num_predict=50,  # æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆçŸ­ç¸®ï¼‰
                keep_alive="10m",  # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒã™ã‚‹æ™‚é–“ï¼ˆå»¶é•·ï¼‰
                # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
                additional_kwargs={
                    "verbose": True,
                    "num_ctx": 4096,
                    "num_batch": 3072
                }
            )
            sys.stdout.write('[NLG] âœ… gemma3:12bãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†\n')
            sys.stdout.flush()

        elif self.model_name == "gpt-3.5-turbo":
            # GPT-3.5-turboè¨­å®š
            sys.stdout.write('[NLG] GPT-3.5-turboãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...\n')
            sys.stdout.flush()

            # OpenAI APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®š
            openai.api_key = os.environ.get("OPENAI_API_KEY")
            if not openai.api_key:
                sys.stdout.write('[NLG ERROR] OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“\n')
                sys.stdout.flush()
                raise ValueError("OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            else:
                sys.stdout.write('[NLG] âœ… GPT-3.5-turboãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†\n')
                sys.stdout.flush()

        else:
            raise ValueError(f"æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«: {self.model_name}")

        sys.stdout.write('NaturalLanguageGeneration (å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹) start up.\n')
        sys.stdout.write(f'ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.model_name}\n')
        sys.stdout.write('=====================================================\n')

    def update(self, query):
        now = datetime.now()

        # æ¥ç¶šã‚¨ãƒ©ãƒ¼æŠ‘åˆ¶ä¸­ã¯æ–°ã—ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ã‘ä»˜ã‘ãªã„
        if self.connection_error_suppress_until and now < self.connection_error_suppress_until:
            return

        # éŸ³å£°èªè­˜çµæœãŒãƒªã‚¹ãƒˆã®å ´åˆã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸ‹ã‚è¾¼ã‚€
        self.asr_results = None
        # ç©ºãƒªã‚¹ãƒˆã¾ãŸã¯å…¨ã¦ç©ºæ–‡å­—åˆ—ãªã‚‰ä½•ã‚‚ã—ãªã„
        if isinstance(query, list):
            if not query or all((not x or x.strip() == "") for x in query):
                self.update_flag = False
                return
            self.asr_results = query
            self.query = query
        else:
            if not query or (isinstance(query, str) and query.strip() == ""):
                self.update_flag = False
                return
            self.query = query
            self.asr_results = None

        # 2.5ç§’é–“éš”åˆ¶å¾¡ã‚’å‰Šé™¤: éŸ³å£°èªè­˜çµæœãŒæ¥ã‚‹ãŸã³ã«ã™ãå¿œç­”ç”Ÿæˆ
        # (ä»¥å‰ã®é–“éš”åˆ¶å¾¡ã‚³ãƒ¼ãƒ‰ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ)

        # å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹æ¨è«–ã«å¤‰æ›´ï¼ˆä¸¦åˆ—å‡¦ç†ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
        # self.request_counter += 1
        # request_id = self.request_counter
        request_id = 1  # å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹ã§ã¯å›ºå®šID

        sys.stdout.write(f"[{now.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸš€ æ¨è«–é–‹å§‹ (ID: {request_id}, ãƒ¢ãƒ‡ãƒ«: {self.model_name})\n")
        sys.stdout.flush()

        # å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹æ¨è«–ã‚’å®Ÿè¡Œï¼ˆä¸¦åˆ—å‡¦ç†ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
        # future = self.executor.submit(self._perform_parallel_inference, request_id, query, now)

        # ã‚·ãƒ³ãƒ—ãƒ«ãªæ¨è«–å®Ÿè¡Œã«æˆ»ã™
        self._perform_simple_inference(query)

        # æœ€å¾Œã®æ¨è«–æ™‚åˆ»ã‚’æ›´æ–°
        self.last_inference_time = now
        self.last_request_time = now
        self.update_flag = True
        
    def set_session_id(self, session_id: str):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’è¨­å®š"""
        self.current_session_id = session_id


    # def generate_dialogue(self, query):
    #     sys.stdout.write('å¯¾è©±å±¥æ­´ä½œæˆ\n')
    #     sys.stdout.flush()
    #     response_res = self.response(query)
    #     dialogue_res = response_res
    #     if ":" in dialogue_res:
    #         dialogue_res = dialogue_res.split(":")[1]
    #     self.dialogue_history.append("usr:" + query)
    #     self.dialogue_history.append("sys:" + dialogue_res)
    #     # self.dialogue_historyã®æœ€å¾Œã‹ã‚‰ï¼”ã¤ã®è¦ç´ ã‚’ä¿å­˜
    #     if len(self.dialogue_history) > 5:
    #         self.dialogue_history = self.dialogue_history[-4:]
    #     sys.stdout.write('å¯¾è©±å±¥æ­´ä½œæˆ\n')
    #     sys.stdout.flush()
    #     return response_res
    
    def _perform_simple_inference(self, query):
        """ã‚·ãƒ³ãƒ—ãƒ«ãªå˜ä¸€ã‚¹ãƒ¬ãƒƒãƒ‰æ¨è«– (GPT-3.5-turboä½¿ç”¨)"""
        start_time = datetime.now()

        # æ¨è«–é–‹å§‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        if self.current_session_id:
            self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "inference_start", {
                "query_type": "list" if isinstance(query, list) else "string",
                "query_length": len(query) if isinstance(query, list) else len(str(query))
            })

        try:
            # GPT-3.5-turboã‚’ä½¿ç”¨ï¼ˆOllamaã‚³ãƒ¼ãƒ‰ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
            # ollama_model = self.ollama_model

            res = ""  # resã‚’å¿…ãšåˆæœŸåŒ–
            asr_results = self.asr_results

            if asr_results and isinstance(asr_results, list) and len(asr_results) >= 1:
                if all((not x or x.strip() == "") for x in asr_results):
                    self.last_reply = ""
                    self.last_source_words = []
                    return
                
                # éŸ³å£°èªè­˜çµæœã‚’ã™ã¹ã¦åˆ—æŒ™
                asr_lines = []
                for idx, asr in enumerate(asr_results):
                    asr_lines.append(f"èªè­˜çµæœ{idx+1}: {asr}")
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
                current_dir = os.path.dirname(os.path.abspath(__file__))
                prompt_file_path = os.path.join(current_dir, "prompts", "remdis_test.txt")

                if not os.path.exists(prompt_file_path):
                    workspace_path = "/workspace/DiaROS/DiaROS_py/diaros/prompts/remdis_test.txt"
                    if os.path.exists(workspace_path):
                        prompt_file_path = workspace_path
                
                try:
                    with open(prompt_file_path, 'r', encoding='utf-8') as f:
                        prompt = f.read()
                    if not prompt.strip():
                        sys.stdout.write(f"[NLG ERROR] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™: {prompt_file_path}\n")
                        sys.stdout.flush()
                        return
                except FileNotFoundError:
                    sys.stdout.write(f"[NLG ERROR] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {prompt_file_path}\n")
                    sys.stdout.flush()
                    return
                except Exception as e:
                    sys.stdout.write(f"[NLG ERROR] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {prompt_file_path} - {e}\n")
                    sys.stdout.flush()
                    return
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆï¼ˆéŸ³å£°èªè­˜çµæœã‚’çµ„ã¿è¾¼ã¿ï¼‰
                full_prompt = f"{prompt}\n\néŸ³å£°èªè­˜çµæœ: {', '.join(asr_results)}"
                
                # LLMå‘¼ã³å‡ºã—
                llm_start_time = datetime.now()
                sys.stdout.write(f"[{llm_start_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸ¤– GPT-3.5-turboæ¨è«–é–‹å§‹\n")
                # LLMæ¨è«–é–‹å§‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                if self.current_session_id:
                    self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "llm_start", {
                        "model": self.model_name,
                        "prompt_type": "asr_dialogue",
                        "prompt_length": len(prompt),
                        "asr_count": len(asr_results)
                    })

                # GPT-3.5-turbo APIã‚’ç›´æ¥å‘¼ã³å‡ºã—ã¦å¯¾è©±ç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ï¼‰
                try:
                    # OpenAI Chat Completion API v1.0+ (ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°)
                    from openai import OpenAI
                    client = OpenAI(api_key=openai.api_key)

                    response_stream = client.chat.completions.create(
                        model=self.model_name,
                        messages=[
                            {"role": "system", "content": prompt},
                            {"role": "user", "content": f"éŸ³å£°èªè­˜çµæœ: {', '.join(asr_results)}"}
                        ],
                        temperature=0.7,
                        max_tokens=100,  # çŸ­ã„å¿œç­”ç”¨
                        stream=True  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–
                    )

                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å‡¦ç†
                    res = ""
                    token_count = 0
                    first_token_time = None
                    last_token_time = llm_start_time
                    token_times = []  # å„ãƒˆãƒ¼ã‚¯ãƒ³ã®æ™‚é–“å·®ã‚’è¨˜éŒ²

                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’é€æ¬¡å‡¦ç†
                    for chunk in response_stream:
                        current_time = datetime.now()

                        # ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾— (OpenAI v1.0+ API)
                        if chunk.choices and len(chunk.choices) > 0:
                            delta = chunk.choices[0].delta
                            token_fragment = delta.content if delta.content else ''
                        else:
                            token_fragment = ''

                        if token_fragment:
                            token_count += 1

                            # Time to First Token (TTFT) ã‚’è¨ˆæ¸¬
                            if first_token_time is None:
                                first_token_time = current_time
                                ttft_ms = (first_token_time - llm_start_time).total_seconds() * 1000
                                sys.stdout.write(f"[{current_time.strftime('%H:%M:%S.%f')[:-3]}][NLG TOKEN] ğŸ¯ æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³å—ä¿¡ (TTFT: {ttft_ms:.1f}ms)\n")
                                sys.stdout.flush()

                            # Inter-Token Latency (ITL) ã‚’è¨ˆæ¸¬
                            itl_ms = (current_time - last_token_time).total_seconds() * 1000
                            token_times.append(itl_ms)

                            # ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®è©³ç´°ãƒ­ã‚°å‡ºåŠ›
                            sys.stdout.write(f"[{current_time.strftime('%H:%M:%S.%f')[:-3]}][NLG TOKEN] #{token_count}: '{token_fragment}' (ITL: {itl_ms:.1f}ms)\n")
                            sys.stdout.flush()

                            res += token_fragment
                            last_token_time = current_time

                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†
                    llm_end_time = datetime.now()
                    llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000

                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âœ… GPT-3.5-turboæ¨è«–å®Œäº† (LLMæ™‚é–“: {llm_duration:.1f}ms)\n")

                    # ãƒˆãƒ¼ã‚¯ãƒ³çµ±è¨ˆæƒ…å ±
                    if token_count > 0 and first_token_time:
                        avg_itl = sum(token_times) / len(token_times) if token_times else 0
                        ttft_ms = (first_token_time - llm_start_time).total_seconds() * 1000
                        sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG STATS] ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {token_count}\n")
                        sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG STATS] ğŸ“Š TTFT (æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§): {ttft_ms:.1f}ms\n")
                        sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG STATS] ğŸ“Š å¹³å‡ITL (ãƒˆãƒ¼ã‚¯ãƒ³é–“éš”): {avg_itl:.1f}ms\n")

                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] ç”Ÿæˆå¿œç­”: '{res}'\n")
                    sys.stdout.flush()

                except Exception as api_error:
                    # APIå‘¼ã³å‡ºã—å¤±æ•—æ™‚ã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®š
                    res = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
                    llm_end_time = datetime.now()
                    llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âŒ GPT-3.5-turbo APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {api_error}\n")
                    sys.stdout.flush()
                
                # LLMæ¨è«–å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                if self.current_session_id:
                    self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "llm_complete", {
                        "model": self.model_name,
                        "llm_duration_ms": llm_duration,
                        "response_length": len(res)
                    })
                
                source_words = asr_results
                
            else:
                if not query or (isinstance(query, list) and all((not x or x.strip() == "") for x in query)):
                    self.last_reply = ""
                    self.last_source_words = []
                    return
                elif query == "dummy":
                    res = "ã¯ã„"
                    source_words = [str(query)]
                else:
                    text_input = query
                    role = "å„ªã—ã„æ€§æ ¼ã®ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰ã¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè©±ã«å¯¾ã—ã¦ç›¸æ‰‹ã‚’åŠ´ã‚‹ã‚ˆã†ãªè¿”ç­”ã®ã¿ã‚’ï¼’ï¼æ–‡å­—ä»¥å†…ã§ã—ã¦ãã ã•ã„ã€‚"

                    messages = [
                        ("system", role),
                        ("human", text_input)
                    ]
                    query_prompt = ChatPromptTemplate.from_messages(messages)
                    chain = query_prompt | ollama_model | StrOutputParser()
                    
                    llm_start_time = datetime.now()                    
                    res = chain.invoke({})
                    
                    llm_end_time = datetime.now()
                    llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âœ… Ollamaæ¨è«–å®Œäº† (LLMæ™‚é–“: {llm_duration:.1f}ms)\n")
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] ç”Ÿæˆå¿œç­”: '{res}'\n")
                    sys.stdout.flush()
                    
                    if ":" in res:
                        res = res.split(":", 1)[1]
                    
                    source_words = [str(query)]
            
            # æ”¹è¡Œã‚’é™¤å»ã—ã¦1è¡Œã«ã™ã‚‹
            res = res.replace('\n', '').replace('\r', '')
            
            end_time = datetime.now()
            total_duration = (end_time - start_time).total_seconds() * 1000
            
            # çµæœã‚’è¨­å®š
            self.last_reply = res
            self.last_source_words = source_words
            
            # ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±ã‚’è¨­å®š
            self.request_id = 1
            self.worker_name = "nlg-single"
            self.start_timestamp_ns = int(start_time.timestamp() * 1_000_000_000)
            self.completion_timestamp_ns = int(end_time.timestamp() * 1_000_000_000)
            self.inference_duration_ms = total_duration
            
            # æˆåŠŸæ™‚ã¯æ¥ç¶šã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
            self.connection_error_count = 0
            self.connection_error_suppress_until = None
            
            # æ¨è«–å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
            if self.current_session_id:
                self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "inference_complete", {
                    "total_duration_ms": total_duration,
                    "response": res,
                    "source_words": source_words
                })
            
            sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸ å‡¦ç†å®Œäº† (ç·æ™‚é–“: {total_duration:.1f}ms): {res}\n")
            sys.stdout.flush()
            
        except Exception as e:
            end_time = datetime.now()
            
            # æ¥ç¶šã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†
            error_str = str(e)
            is_connection_error = (
                "[Errno 111] Connection refused" in error_str or
                "llama runner process has terminated" in error_str or
                "broken pipe" in error_str or
                "status code: 500" in error_str
            )
            
            if is_connection_error:
                self.connection_error_count += 1
                # é€£ç¶šæ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒ5å›ä»¥ä¸Šãªã‚‰30ç§’é–“ãƒªã‚¯ã‚¨ã‚¹ãƒˆæŠ‘åˆ¶
                if self.connection_error_count >= 5:
                    self.connection_error_suppress_until = end_time + timedelta(seconds=30)
                    if self.connection_error_count == 5:  # åˆå›æŠ‘åˆ¶æ™‚ã®ã¿ãƒ­ã‚°å‡ºåŠ›
                        sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG WARNING] ğŸš« é€£ç¶šæ¥ç¶šã‚¨ãƒ©ãƒ¼æ¤œå‡ºã€‚30ç§’é–“ãƒªã‚¯ã‚¨ã‚¹ãƒˆæŠ‘åˆ¶ã—ã¾ã™\n")
                        sys.stdout.flush()
                # æ¥ç¶šã‚¨ãƒ©ãƒ¼ã¯è©³ç´°ãƒ­ã‚°ã‚’æŠ‘åˆ¶
                if self.connection_error_count <= 3:  # æœ€åˆã®3å›ã®ã¿ãƒ­ã‚°å‡ºåŠ›
                    sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG ERROR] âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: Ollamaæ¥ç¶šå¤±æ•—\n")
                    sys.stdout.flush()
            else:
                # æ¥ç¶šã‚¨ãƒ©ãƒ¼ä»¥å¤–ã®å ´åˆã¯ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆ
                self.connection_error_count = 0
                self.connection_error_suppress_until = None
                sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG ERROR] âŒ æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}\n")
                sys.stdout.flush()
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ï¼ˆå›ºå®šå¿œç­”ã®ã¿ï¼‰
            if is_connection_error:
                # Ollamaã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç°¡å˜ãªå›ºå®šå¿œç­”ã®ã¿
                fallback_responses = [
                    "ãã†ã§ã™ã­ã€‚",
                    "ãªã‚‹ã»ã©ã€‚", 
                    "ã‚ã‹ã‚Šã¾ã—ãŸã€‚",
                    "ã¯ã„ã€‚",
                    "ãã†ãªã‚“ã§ã™ã­ã€‚"
                ]
                import random
                fallback_response = random.choice(fallback_responses)
                
                self.last_reply = fallback_response
                self.last_source_words = query if isinstance(query, list) else [str(query)]
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±è¨­å®š
                self.request_id = 999  # å›ºå®šå¿œç­”ID
                self.worker_name = "static-fallback"
                self.start_timestamp_ns = int(start_time.timestamp() * 1_000_000_000)
                self.completion_timestamp_ns = int(end_time.timestamp() * 1_000_000_000)
                self.inference_duration_ms = (end_time - start_time).total_seconds() * 1000
                
                if self.connection_error_count <= 3:
                    sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG FALLBACK] ğŸ”„ å›ºå®šå¿œç­”ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {fallback_response}\n")
                    sys.stdout.flush()
            else:
                # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®çµæœã‚’è¨­å®š
                self.last_reply = ""
                self.last_source_words = []

    # _perform_parallel_inference() ãƒ¡ã‚½ãƒƒãƒ‰ã¯ç¾åœ¨ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
    # GPT-3.5-turboç‰ˆã¯å¿…è¦ã«ãªã£ãŸã‚‰å®Ÿè£…
    """
    def _perform_parallel_inference(self, request_id, query, start_time):
        \"""ä¸¦åˆ—æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°\"""
        worker_name = f"nlg-worker-{request_id % 3 + 1}"  # 3ã¤ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã§äº¤äº’å®Ÿè¡Œ
        
        sys.stdout.write(f"[{start_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸ”„ {worker_name} æ¨è«–é–‹å§‹\n")
        sys.stdout.flush()
        
        # æ¨è«–é–‹å§‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        if self.current_session_id:
            self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "parallel_inference_start", {
                "worker_name": worker_name,
                "request_id": request_id,
                "query_type": "list" if isinstance(query, list) else "string",
                "query_length": len(query) if isinstance(query, list) else len(str(query))
            })
        
        try:
            # åˆæœŸåŒ–æ¸ˆã¿ã®Ollamaãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            ollama_model = self.ollama_model
            
            res = ""  # resã‚’å¿…ãšåˆæœŸåŒ–
            asr_results = self.asr_results if hasattr(self, 'asr_results') else None
            
            if asr_results and isinstance(asr_results, list) and len(asr_results) >= 1:
                if all((not x or x.strip() == "") for x in asr_results):
                    self.last_reply = ""
                    self.last_source_words = []
                    return
                
                # éŸ³å£°èªè­˜çµæœã‚’ã™ã¹ã¦åˆ—æŒ™
                asr_lines = []
                for idx, asr in enumerate(asr_results):
                    asr_lines.append(f"èªè­˜çµæœ{idx+1}: {asr}")
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
                current_dir = os.path.dirname(os.path.abspath(__file__))
                prompt_file_path = os.path.join(current_dir, "prompts", "example_dialog.txt")

                if not os.path.exists(prompt_file_path):
                    workspace_path = "/workspace/DiaROS/DiaROS_py/diaros/prompts/example_dialog.txt"
                    if os.path.exists(workspace_path):
                        prompt_file_path = workspace_path
                
                try:
                    with open(prompt_file_path, 'r', encoding='utf-8') as f:
                        prompt = f.read()
                    if not prompt.strip():
                        sys.stdout.write(f"[NLG ERROR] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™: {prompt_file_path}\n")
                        sys.stdout.flush()
                        return
                except FileNotFoundError:
                    sys.stdout.write(f"[NLG ERROR] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {prompt_file_path}\n")
                    sys.stdout.flush()
                    return
                except Exception as e:
                    sys.stdout.write(f"[NLG ERROR] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {prompt_file_path} - {e}\n")
                    sys.stdout.flush()
                    return
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆï¼ˆéŸ³å£°èªè­˜çµæœã‚’çµ„ã¿è¾¼ã¿ï¼‰
                full_prompt = f"{prompt}\n\néŸ³å£°èªè­˜çµæœ: {', '.join(asr_results)}"
                
                # LLMå‘¼ã³å‡ºã—
                llm_start_time = datetime.now()
                sys.stdout.write(f"[{llm_start_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸ¤– {worker_name} Ollamaæ¨è«–é–‹å§‹\n")
                
                # LLMæ¨è«–é–‹å§‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                if self.current_session_id:
                    self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "llm_start", {
                        "model": self.model_name,
                        "worker_name": worker_name,
                        "prompt_type": "asr_dialogue",
                        "prompt_length": len(prompt),
                        "asr_count": len(asr_results)
                    })
                
                # Ollama APIã‚’ç›´æ¥å‘¼ã³å‡ºã—ã¦å¯¾è©±ç”Ÿæˆã¨çµ±è¨ˆæƒ…å ±ã‚’å–å¾—ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ï¼‰
                try:
                    import requests
                    api_response = requests.post('http://localhost:11434/api/generate',
                        json={
                            'model': self.model_name,
                            'prompt': full_prompt,
                            'stream': True,  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–
                            'options': {
                                'temperature': 0.7,
                                'top_p': 0.9,
                                'num_predict': 50,
                                'num_ctx': 4096,
                                'num_batch': 3072
                            }
                        },
                        timeout=30,
                        stream=True  # requestsã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚‚æœ‰åŠ¹åŒ–
                    )

                    if api_response.status_code == 200:
                        res = ""
                        token_count = 0
                        first_token_time = None
                        last_token_time = llm_start_time
                        token_times = []  # å„ãƒˆãƒ¼ã‚¯ãƒ³ã®æ™‚é–“å·®ã‚’è¨˜éŒ²

                        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’é€æ¬¡å‡¦ç†
                        for line in api_response.iter_lines():
                            if line:
                                current_time = datetime.now()
                                try:
                                    chunk_data = json.loads(line)
                                    token_fragment = chunk_data.get('response', '')

                                    if token_fragment:
                                        token_count += 1

                                        # Time to First Token (TTFT) ã‚’è¨ˆæ¸¬
                                        if first_token_time is None:
                                            first_token_time = current_time
                                            ttft_ms = (first_token_time - llm_start_time).total_seconds() * 1000
                                            sys.stdout.write(f"[{current_time.strftime('%H:%M:%S.%f')[:-3]}][NLG TOKEN] ğŸ¯ {worker_name} æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³å—ä¿¡ (TTFT: {ttft_ms:.1f}ms)\n")
                                            sys.stdout.flush()

                                        # Inter-Token Latency (ITL) ã‚’è¨ˆæ¸¬
                                        itl_ms = (current_time - last_token_time).total_seconds() * 1000
                                        token_times.append(itl_ms)

                                        # ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®è©³ç´°ãƒ­ã‚°å‡ºåŠ›
                                        sys.stdout.write(f"[{current_time.strftime('%H:%M:%S.%f')[:-3]}][NLG TOKEN] {worker_name} #{token_count}: '{token_fragment}' (ITL: {itl_ms:.1f}ms)\n")
                                        sys.stdout.flush()

                                        res += token_fragment
                                        last_token_time = current_time

                                    # æœ€çµ‚ãƒãƒ£ãƒ³ã‚¯ã®å‡¦ç†
                                    if chunk_data.get('done', False):
                                        llm_end_time = current_time
                                        llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000

                                        sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âœ… {worker_name} Ollamaæ¨è«–å®Œäº† (LLMæ™‚é–“: {llm_duration:.1f}ms)\n")

                                        # ãƒˆãƒ¼ã‚¯ãƒ³çµ±è¨ˆæƒ…å ±
                                        if token_count > 0 and first_token_time:
                                            avg_itl = sum(token_times) / len(token_times) if token_times else 0
                                            ttft_ms = (first_token_time - llm_start_time).total_seconds() * 1000
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG STATS] ğŸ“Š {worker_name} ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {token_count}\n")
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG STATS] ğŸ“Š {worker_name} TTFT (æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§): {ttft_ms:.1f}ms\n")
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG STATS] ğŸ“Š {worker_name} å¹³å‡ITL (ãƒˆãƒ¼ã‚¯ãƒ³é–“éš”): {avg_itl:.1f}ms\n")

                                        # Verboseçµ±è¨ˆæƒ…å ±ï¼ˆOllamaæä¾›ï¼‰
                                        if 'total_duration' in chunk_data:
                                            total_duration_ms = chunk_data['total_duration'] / 1_000_000
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} ã‹ã‹ã£ãŸæ™‚é–“: {total_duration_ms/1000:.6f}s\n")

                                        if 'load_duration' in chunk_data:
                                            load_duration_ms = chunk_data['load_duration'] / 1_000_000
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ™‚é–“: {load_duration_ms:.3f}ms\n")

                                        if 'prompt_eval_count' in chunk_data and 'prompt_eval_duration' in chunk_data:
                                            prompt_tokens = chunk_data['prompt_eval_count']
                                            prompt_eval_ms = chunk_data['prompt_eval_duration'] / 1_000_000
                                            tokens_per_sec = prompt_tokens / (prompt_eval_ms / 1000) if prompt_eval_ms > 0 else 0
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {prompt_tokens} token(s)\n")
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å‡¦ç†æ™‚é–“: {prompt_eval_ms:.3f}ms\n")
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å‡¦ç†ãƒˆãƒ¼ã‚¯ãƒ³/s: {tokens_per_sec:.2f} tokens/s\n")

                                        if 'eval_count' in chunk_data and 'eval_duration' in chunk_data:
                                            output_tokens = chunk_data['eval_count']
                                            eval_ms = chunk_data['eval_duration'] / 1_000_000
                                            output_tokens_per_sec = output_tokens / (eval_ms / 1000) if eval_ms > 0 else 0
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {output_tokens} token(s)\n")
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã«ã‹ã‹ã£ãŸæ™‚é–“: {eval_ms/1000:.6f}s\n")
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã®å‡¦ç†ãƒˆãƒ¼ã‚¯ãƒ³/s: {output_tokens_per_sec:.2f} tokens/s\n")

                                        sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} ç”Ÿæˆå¿œç­”: '{res}'\n")
                                        sys.stdout.flush()
                                        break

                                except json.JSONDecodeError:
                                    continue

                        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãŒå®Œäº†ã—ã¦ã‚‚ llm_end_time ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆ
                        if 'llm_end_time' not in locals():
                            llm_end_time = datetime.now()
                            llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                    else:
                        # APIå‘¼ã³å‡ºã—å¤±æ•—æ™‚ã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®š
                        res = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
                        llm_end_time = datetime.now()
                        llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                        sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âŒ {worker_name} Ollama APIå‘¼ã³å‡ºã—å¤±æ•— (status: {api_response.status_code})\n")
                        sys.stdout.flush()

                except Exception as api_error:
                    # APIå‘¼ã³å‡ºã—å¤±æ•—æ™‚ã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®š
                    res = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
                    llm_end_time = datetime.now()
                    llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âŒ {worker_name} Ollama APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {api_error}\n")
                    sys.stdout.flush()
                
                # LLMæ¨è«–å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                if self.current_session_id:
                    self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "llm_complete", {
                        "model": self.model_name,
                        "worker_name": worker_name,
                        "llm_duration_ms": llm_duration,
                        "response_length": len(res)
                    })
                
                source_words = asr_results
                
            else:
                if not query or (isinstance(query, list) and all((not x or x.strip() == "") for x in query)):
                    self.last_reply = ""
                    self.last_source_words = []
                    return
                elif query == "dummy":
                    res = "ã¯ã„"
                    source_words = [str(query)]
                else:
                    text_input = query
                    role = "å„ªã—ã„æ€§æ ¼ã®ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰ã¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè©±ã«å¯¾ã—ã¦ç›¸æ‰‹ã‚’åŠ´ã‚‹ã‚ˆã†ãªè¿”ç­”ã®ã¿ã‚’ï¼’ï¼æ–‡å­—ä»¥å†…ã§ã—ã¦ãã ã•ã„ã€‚"

                    messages = [
                        ("system", role),
                        ("human", text_input)
                    ]
                    query_prompt = ChatPromptTemplate.from_messages(messages)
                    chain = query_prompt | ollama_model | StrOutputParser()
                    
                    llm_start_time = datetime.now()                    
                    res = chain.invoke({})
                    
                    llm_end_time = datetime.now()
                    llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âœ… {worker_name} Ollamaæ¨è«–å®Œäº† (LLMæ™‚é–“: {llm_duration:.1f}ms)\n")
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} ç”Ÿæˆå¿œç­”: '{res}'\n")
                    sys.stdout.flush()
                    
                    if ":" in res:
                        res = res.split(":", 1)[1]
                    
                    source_words = [str(query)]
            
            # æ”¹è¡Œã‚’é™¤å»ã—ã¦1è¡Œã«ã™ã‚‹
            res = res.replace('\n', '').replace('\r', '')
            
            end_time = datetime.now()
            total_duration = (end_time - start_time).total_seconds() * 1000
            
            # çµæœã‚’è¨­å®š
            self.last_reply = res
            self.last_source_words = source_words
            
            # ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±ã‚’è¨­å®š
            self.request_id = request_id
            self.worker_name = worker_name
            self.start_timestamp_ns = int(start_time.timestamp() * 1_000_000_000)
            self.completion_timestamp_ns = int(end_time.timestamp() * 1_000_000_000)
            self.inference_duration_ms = total_duration
            
            # æˆåŠŸæ™‚ã¯æ¥ç¶šã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
            self.connection_error_count = 0
            self.connection_error_suppress_until = None
            
            # æ¨è«–å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
            if self.current_session_id:
                self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "parallel_inference_complete", {
                    "worker_name": worker_name,
                    "request_id": request_id,
                    "total_duration_ms": total_duration,
                    "response": res,
                    "source_words": source_words
                })
            
            sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸ {worker_name} å‡¦ç†å®Œäº† (ç·æ™‚é–“: {total_duration:.1f}ms): {res}\n")
            sys.stdout.flush()
            
        except Exception as e:
            end_time = datetime.now()
            
            # æ¥ç¶šã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†
            error_str = str(e)
            is_connection_error = (
                "[Errno 111] Connection refused" in error_str or
                "llama runner process has terminated" in error_str or
                "broken pipe" in error_str or
                "status code: 500" in error_str
            )
            
            if is_connection_error:
                self.connection_error_count += 1
                # é€£ç¶šæ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒ5å›ä»¥ä¸Šãªã‚‰30ç§’é–“ãƒªã‚¯ã‚¨ã‚¹ãƒˆæŠ‘åˆ¶
                if self.connection_error_count >= 5:
                    self.connection_error_suppress_until = end_time + timedelta(seconds=30)
                    if self.connection_error_count == 5:  # åˆå›æŠ‘åˆ¶æ™‚ã®ã¿ãƒ­ã‚°å‡ºåŠ›
                        sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG WARNING] ğŸš« {worker_name} é€£ç¶šæ¥ç¶šã‚¨ãƒ©ãƒ¼æ¤œå‡ºã€‚30ç§’é–“ãƒªã‚¯ã‚¨ã‚¹ãƒˆæŠ‘åˆ¶ã—ã¾ã™\n")
                        sys.stdout.flush()
                # æ¥ç¶šã‚¨ãƒ©ãƒ¼ã¯è©³ç´°ãƒ­ã‚°ã‚’æŠ‘åˆ¶
                if self.connection_error_count <= 3:  # æœ€åˆã®3å›ã®ã¿ãƒ­ã‚°å‡ºåŠ›
                    sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG ERROR] âŒ {worker_name} æ¥ç¶šã‚¨ãƒ©ãƒ¼: Ollamaæ¥ç¶šå¤±æ•—\n")
                    sys.stdout.flush()
            else:
                # æ¥ç¶šã‚¨ãƒ©ãƒ¼ä»¥å¤–ã®å ´åˆã¯ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆ
                self.connection_error_count = 0
                self.connection_error_suppress_until = None
                sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG ERROR] âŒ {worker_name} æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}\n")
                sys.stdout.flush()
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ï¼ˆå›ºå®šå¿œç­”ã®ã¿ï¼‰
            if is_connection_error:
                # Ollamaã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç°¡å˜ãªå›ºå®šå¿œç­”ã®ã¿
                fallback_responses = [
                    "ãã†ã§ã™ã­ã€‚",
                    "ãªã‚‹ã»ã©ã€‚", 
                    "ã‚ã‹ã‚Šã¾ã—ãŸã€‚",
                    "ã¯ã„ã€‚",
                    "ãã†ãªã‚“ã§ã™ã­ã€‚"
                ]
                import random
                fallback_response = random.choice(fallback_responses)
                
                self.last_reply = fallback_response
                self.last_source_words = query if isinstance(query, list) else [str(query)]
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±è¨­å®š
                self.request_id = 999  # å›ºå®šå¿œç­”ID
                self.worker_name = f"static-fallback-{worker_name}"
                self.start_timestamp_ns = int(start_time.timestamp() * 1_000_000_000)
                self.completion_timestamp_ns = int(end_time.timestamp() * 1_000_000_000)
                self.inference_duration_ms = (end_time - start_time).total_seconds() * 1000
                
                if self.connection_error_count <= 3:
                    sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG FALLBACK] ğŸ”„ {worker_name} å›ºå®šå¿œç­”ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {fallback_response}\n")
                    sys.stdout.flush()
            else:
                # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®çµæœã‚’è¨­å®š
                self.last_reply = ""
                self.last_source_words = []
    """

    def run(self):
        sys.stdout.write("[NLG] å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹æ¨è«–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹ (2.5ç§’é–“éš”åˆ¶å¾¡)\n")
        sys.stdout.write(f"[NLG] ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.model_name}\n")
        sys.stdout.flush()
        
        # ä¸¦åˆ—å‡¦ç†ç‰ˆã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
        # while True:
        #     # ä¸¦åˆ—æ¨è«–ã‚·ã‚¹ãƒ†ãƒ ã§ã¯çµæœç›£è¦–ã®ã¿
        #     try:
        #         # å®Œäº†ã—ãŸæ¨è«–çµæœãŒã‚ã‚Œã°å‡¦ç†
        #         if not self.result_queue.empty():
        #             result = self.result_queue.get_nowait()
        #             # çµæœå‡¦ç†ã¯ãƒ¯ãƒ¼ã‚«ãƒ¼å†…ã§å®Œçµã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ç‰¹ã«å‡¦ç†ãªã—
        #     except:
        #         pass
        #     
        #     time.sleep(0.01)  # 10mså¾…æ©Ÿ
        
        # å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹ç‰ˆã§ã¯ç‰¹ã«ç„¡é™ãƒ«ãƒ¼ãƒ—ã¯ä¸è¦
        pass

if __name__ == "__main__":
    gen = NaturalLanguageGeneration()
    gen.run()