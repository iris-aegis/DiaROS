# ä¸€æ—¦å±¥æ­´è«¦ã‚

import requests
import json
import sys
import os
import time
import threading
from datetime import datetime, timedelta
from queue import Queue, Empty
from concurrent.futures import ThreadPoolExecutor
import openai
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import ChatOllama
from .timeTracker import get_time_tracker

class NaturalLanguageGeneration:
    def __init__(self):
        self.rc = { "word": "" }
        
        self.query = ""
        self.update_flag = False
        self.dialogue_history = []
        self.user_speak_is_final = False
        self.last_reply = ""  # ç”Ÿæˆã—ãŸå¯¾è©±æ–‡ã‚’ã“ã“ã«æ ¼ç´
        self.last_source_words = []  # å¯¾è©±ç”Ÿæˆã®å…ƒã«ã—ãŸéŸ³å£°èªè­˜çµæœã‚’æ ¼ç´
        
        # ROS2 bagè¨˜éŒ²ç”¨ã®è¿½åŠ æƒ…å ±
        self.last_request_id = 0
        self.last_worker_name = ""
        self.last_start_timestamp_ns = 0
        self.last_completion_timestamp_ns = 0
        self.last_inference_duration_ms = 0.0
        
        # æ–°ã—ã„æ™‚åˆ»æƒ…å ±ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åˆæœŸåŒ–
        self.request_id = 0
        self.worker_name = ""
        self.start_timestamp_ns = 0
        self.completion_timestamp_ns = 0
        self.inference_duration_ms = 0.0
        
        # æ¥ç¶šã‚¨ãƒ©ãƒ¼åˆ¶å¾¡ç”¨
        self.connection_error_count = 0
        self.last_connection_error_time = None
        self.connection_error_suppress_until = None
        
        # ã‚¿ã‚¤ãƒ ãƒˆãƒ©ãƒƒã‚«ãƒ¼åˆæœŸåŒ–
        self.time_tracker = get_time_tracker("nlg_pc")
        self.current_session_id = None
        
        # ä¸¦åˆ—å‡¦ç†ç”¨ã®è¨­å®šï¼ˆä¸€æ™‚çš„ã«ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
        # self.inference_queue = Queue()  # æ¨è«–ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚­ãƒ¥ãƒ¼
        # self.result_queue = Queue()     # æ¨è«–çµæœã®ã‚­ãƒ¥ãƒ¼
        # self.request_counter = 0        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        # self.last_request_time = None   # æœ€å¾Œã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚åˆ»
        # self.executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="NLG-Worker")
        
        # Ollamaãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–æ™‚ã«1å›ã ã‘ä½œæˆï¼ˆå†åˆ©ç”¨ï¼‰
        sys.stdout.write('[NLG] Ollama ChatOllamaãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...\n')
        sys.stdout.flush()
        self.model_name = "gemma3:12b"  # ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«åã‚’æŒ‡å®š
        self.ollama_model = ChatOllama(
            model=self.model_name,
            verbose=True,  # è©³ç´°ãƒ­ã‚°æœ‰åŠ¹åŒ–
            temperature=0.7,  # å¿œç­”ã®å¤šæ§˜æ€§
            top_p=0.9,  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¨­å®š
            num_predict=1024,  # æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°
            keep_alive="5m",  # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒã™ã‚‹æ™‚é–“
            # ollamaã‚³ãƒãƒ³ãƒ‰ã®--verboseã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æœ‰åŠ¹åŒ–
            additional_kwargs={"verbose": True}
        )
        sys.stdout.write('[NLG] âœ… ChatOllamaãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†\n')
        sys.stdout.flush()
        
        sys.stdout.write('NaturalLanguageGeneration (å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹ start up.\n')
        sys.stdout.write('=====================================================\n')
        # OpenAI APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®š
        openai.api_key = os.environ.get("OPENAI_API_KEY")

    def update(self, query):
        # æ¥ç¶šã‚¨ãƒ©ãƒ¼æŠ‘åˆ¶ä¸­ã¯æ–°ã—ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ã‘ä»˜ã‘ãªã„
        # now = datetime.now()
        # if self.connection_error_suppress_until and now < self.connection_error_suppress_until:
        #     return
            
        # éŸ³å£°èªè­˜çµæœãŒãƒªã‚¹ãƒˆã®å ´åˆã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸ‹ã‚è¾¼ã‚€
        self.asr_results = None
        # ç©ºãƒªã‚¹ãƒˆã¾ãŸã¯å…¨ã¦ç©ºæ–‡å­—åˆ—ãªã‚‰ä½•ã‚‚ã—ãªã„
        if isinstance(query, list):
            if not query or all((not x or x.strip() == "") for x in query):
                self.update_flag = False
                return
            self.asr_results = query
            self.query = query
        else:
            if not query or (isinstance(query, str) and query.strip() == ""):
                self.update_flag = False
                return
            self.query = query
            self.asr_results = None

        now = datetime.now()  # â† ã“ã“ã‚’è¿½åŠ 

        # å˜ä¸€ã‚¹ãƒ¬ãƒƒãƒ‰ã§å³åº§ã«æ¨è«–å®Ÿè¡Œ
        sys.stdout.write(f"[{now.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸš€ æ¨è«–é–‹å§‹\n")
        sys.stdout.flush()
        
        # ç›´æ¥æ¨è«–ã‚’å®Ÿè¡Œ
        self._perform_simple_inference(query)
        
        self.update_flag = True
        
    def set_session_id(self, session_id: str):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’è¨­å®š"""
        self.current_session_id = session_id


    # def generate_dialogue(self, query):
    #     sys.stdout.write('å¯¾è©±å±¥æ­´ä½œæˆ\n')
    #     sys.stdout.flush()
    #     response_res = self.response(query)
    #     dialogue_res = response_res
    #     if ":" in dialogue_res:
    #         dialogue_res = dialogue_res.split(":")[1]
    #     self.dialogue_history.append("usr:" + query)
    #     self.dialogue_history.append("sys:" + dialogue_res)
    #     # self.dialogue_historyã®æœ€å¾Œã‹ã‚‰ï¼”ã¤ã®è¦ç´ ã‚’ä¿å­˜
    #     if len(self.dialogue_history) > 5:
    #         self.dialogue_history = self.dialogue_history[-4:]
    #     sys.stdout.write('å¯¾è©±å±¥æ­´ä½œæˆ\n')
    #     sys.stdout.flush()
    #     return response_res
    
    def _perform_simple_inference(self, query):
        """ã‚·ãƒ³ãƒ—ãƒ«ãªå˜ä¸€ã‚¹ãƒ¬ãƒƒãƒ‰æ¨è«–"""
        start_time = datetime.now()
        
        # æ¨è«–é–‹å§‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        if self.current_session_id:
            self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "inference_start", {
                "query_type": "list" if isinstance(query, list) else "string",
                "query_length": len(query) if isinstance(query, list) else len(str(query))
            })
        
        try:
            # åˆæœŸåŒ–æ¸ˆã¿ã®Ollamaãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            ollama_model = self.ollama_model
            
            res = ""  # resã‚’å¿…ãšåˆæœŸåŒ–
            asr_results = self.asr_results
            
            if asr_results and isinstance(asr_results, list) and len(asr_results) >= 1:
                if all((not x or x.strip() == "") for x in asr_results):
                    self.last_reply = ""
                    self.last_source_words = []
                    return
                
                # éŸ³å£°èªè­˜çµæœã‚’ã™ã¹ã¦åˆ—æŒ™
                asr_lines = []
                for idx, asr in enumerate(asr_results):
                    asr_lines.append(f"èªè­˜çµæœ{idx+1}: {asr}")
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
                current_dir = os.path.dirname(os.path.abspath(__file__))
                prompt_file_path = os.path.join(current_dir, "prompts", "example_dialog_prompt.txt")
                
                if not os.path.exists(prompt_file_path):
                    workspace_path = "/workspace/DiaROS/DiaROS_py/diaros/prompts/example_dialog_prompt.txt"
                    if os.path.exists(workspace_path):
                        prompt_file_path = workspace_path
                
                try:
                    with open(prompt_file_path, 'r', encoding='utf-8') as f:
                        prompt = f.read()
                    if not prompt.strip():
                        sys.stdout.write(f"[NLG ERROR] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™: {prompt_file_path}\n")
                        sys.stdout.flush()
                        return
                except FileNotFoundError:
                    sys.stdout.write(f"[NLG ERROR] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {prompt_file_path}\n")
                    sys.stdout.flush()
                    return
                except Exception as e:
                    sys.stdout.write(f"[NLG ERROR] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {prompt_file_path} - {e}\n")
                    sys.stdout.flush()
                    return
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆï¼ˆASRçµæœã‚’çµ„ã¿è¾¼ã¿ï¼‰
                full_prompt = f"{prompt}\n\nASRçµæœ: {', '.join(asr_results)}"
                
                # LLMå‘¼ã³å‡ºã—
                llm_start_time = datetime.now()
                sys.stdout.write(f"[{llm_start_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸ¤– Ollamaæ¨è«–é–‹å§‹\n")                
                # LLMæ¨è«–é–‹å§‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                if self.current_session_id:
                    self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "llm_start", {
                        "model": self.model_name,
                        "prompt_type": "asr_dialogue",
                        "prompt_length": len(prompt),
                        "asr_count": len(asr_results)
                    })
                
                # Ollama APIã‚’ç›´æ¥å‘¼ã³å‡ºã—ã¦å¯¾è©±ç”Ÿæˆã¨çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
                try:
                    import requests
                    api_response = requests.post('http://localhost:11434/api/generate', 
                        json={
                            'model': self.model_name,
                            'prompt': full_prompt,
                            'stream': False,
                            'options': {
                                'temperature': 0.7,
                                'top_p': 0.9,
                                'num_predict': 50
                            }
                        },
                        timeout=30
                    )
                    
                    if api_response.status_code == 200:
                        api_data = api_response.json()
                        res = api_data.get('response', '')
                        
                        # è©³ç´°çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
                        llm_end_time = datetime.now()
                        llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                        
                        sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âœ… Ollamaæ¨è«–å®Œäº† (LLMæ™‚é–“: {llm_duration:.1f}ms)\n")
                        
                        # Verboseçµ±è¨ˆæƒ…å ±
                        if 'total_duration' in api_data:
                            total_duration_ms = api_data['total_duration'] / 1_000_000
                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] ã‹ã‹ã£ãŸæ™‚é–“: {total_duration_ms/1000:.6f}s\n")
                        
                        if 'load_duration' in api_data:
                            load_duration_ms = api_data['load_duration'] / 1_000_000
                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ™‚é–“: {load_duration_ms:.3f}ms\n")
                        
                        if 'prompt_eval_count' in api_data and 'prompt_eval_duration' in api_data:
                            prompt_tokens = api_data['prompt_eval_count']
                            prompt_eval_ms = api_data['prompt_eval_duration'] / 1_000_000
                            tokens_per_sec = prompt_tokens / (prompt_eval_ms / 1000) if prompt_eval_ms > 0 else 0
                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {prompt_tokens} token(s)\n")
                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å‡¦ç†æ™‚é–“: {prompt_eval_ms:.3f}ms\n")
                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å‡¦ç†ãƒˆãƒ¼ã‚¯ãƒ³/s: {tokens_per_sec:.2f} tokens/s\n")
                        
                        if 'eval_count' in api_data and 'eval_duration' in api_data:
                            output_tokens = api_data['eval_count']
                            eval_ms = api_data['eval_duration'] / 1_000_000
                            output_tokens_per_sec = output_tokens / (eval_ms / 1000) if eval_ms > 0 else 0
                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {output_tokens} token(s)\n")
                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã«ã‹ã‹ã£ãŸæ™‚é–“: {eval_ms/1000:.6f}s\n")
                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã®å‡¦ç†ãƒˆãƒ¼ã‚¯ãƒ³/s: {output_tokens_per_sec:.2f} tokens/s\n")
                        
                        sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] ç”Ÿæˆå¿œç­”: '{res}'\n")
                        sys.stdout.flush()
                    else:
                        # APIå‘¼ã³å‡ºã—å¤±æ•—æ™‚ã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®š
                        res = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
                        llm_end_time = datetime.now()
                        llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                        sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âŒ Ollama APIå‘¼ã³å‡ºã—å¤±æ•— (status: {api_response.status_code})\n")
                        sys.stdout.flush()
                        
                except Exception as api_error:
                    # APIå‘¼ã³å‡ºã—å¤±æ•—æ™‚ã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®š
                    res = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
                    llm_end_time = datetime.now()
                    llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âŒ Ollama APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {api_error}\n")
                    sys.stdout.flush()
                
                # LLMæ¨è«–å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                if self.current_session_id:
                    self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "llm_complete", {
                        "model": self.model_name,
                        "llm_duration_ms": llm_duration,
                        "response_length": len(res)
                    })
                
                source_words = asr_results
                
            else:
                if not query or (isinstance(query, list) and all((not x or x.strip() == "") for x in query)):
                    self.last_reply = ""
                    self.last_source_words = []
                    return
                elif query == "dummy":
                    res = "ã¯ã„"
                    source_words = [str(query)]
                else:
                    text_input = query
                    role = "å„ªã—ã„æ€§æ ¼ã®ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰ã¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè©±ã«å¯¾ã—ã¦ç›¸æ‰‹ã‚’åŠ´ã‚‹ã‚ˆã†ãªè¿”ç­”ã®ã¿ã‚’ï¼’ï¼æ–‡å­—ä»¥å†…ã§ã—ã¦ãã ã•ã„ã€‚"

                    messages = [
                        ("system", role),
                        ("human", text_input)
                    ]
                    query_prompt = ChatPromptTemplate.from_messages(messages)
                    chain = query_prompt | ollama_model | StrOutputParser()
                    
                    llm_start_time = datetime.now()                    
                    res = chain.invoke({})
                    
                    llm_end_time = datetime.now()
                    llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âœ… Ollamaæ¨è«–å®Œäº† (LLMæ™‚é–“: {llm_duration:.1f}ms)\n")
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] ç”Ÿæˆå¿œç­”: '{res}'\n")
                    sys.stdout.flush()
                    
                    if ":" in res:
                        res = res.split(":", 1)[1]
                    
                    source_words = [str(query)]
            
            # æ”¹è¡Œã‚’é™¤å»ã—ã¦1è¡Œã«ã™ã‚‹
            res = res.replace('\n', '').replace('\r', '')
            
            end_time = datetime.now()
            total_duration = (end_time - start_time).total_seconds() * 1000
            
            # çµæœã‚’è¨­å®š
            self.last_reply = res
            self.last_source_words = source_words
            
            # ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±ã‚’è¨­å®š
            self.request_id = 1
            self.worker_name = "nlg-single"
            self.start_timestamp_ns = int(start_time.timestamp() * 1_000_000_000)
            self.completion_timestamp_ns = int(end_time.timestamp() * 1_000_000_000)
            self.inference_duration_ms = total_duration
            
            # æˆåŠŸæ™‚ã¯æ¥ç¶šã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
            self.connection_error_count = 0
            self.connection_error_suppress_until = None
            
            # æ¨è«–å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
            if self.current_session_id:
                self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "inference_complete", {
                    "total_duration_ms": total_duration,
                    "response": res,
                    "source_words": source_words
                })
            
            sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸ å‡¦ç†å®Œäº† (ç·æ™‚é–“: {total_duration:.1f}ms): {res}\n")
            sys.stdout.flush()
            
        except Exception as e:
            end_time = datetime.now()
            
            # æ¥ç¶šã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†
            error_str = str(e)
            is_connection_error = (
                "[Errno 111] Connection refused" in error_str or
                "llama runner process has terminated" in error_str or
                "broken pipe" in error_str or
                "status code: 500" in error_str
            )
            
            if is_connection_error:
                self.connection_error_count += 1
                # é€£ç¶šæ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒ5å›ä»¥ä¸Šãªã‚‰30ç§’é–“ãƒªã‚¯ã‚¨ã‚¹ãƒˆæŠ‘åˆ¶
                if self.connection_error_count >= 5:
                    self.connection_error_suppress_until = end_time + timedelta(seconds=30)
                    if self.connection_error_count == 5:  # åˆå›æŠ‘åˆ¶æ™‚ã®ã¿ãƒ­ã‚°å‡ºåŠ›
                        sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG WARNING] ğŸš« é€£ç¶šæ¥ç¶šã‚¨ãƒ©ãƒ¼æ¤œå‡ºã€‚30ç§’é–“ãƒªã‚¯ã‚¨ã‚¹ãƒˆæŠ‘åˆ¶ã—ã¾ã™\n")
                        sys.stdout.flush()
                # æ¥ç¶šã‚¨ãƒ©ãƒ¼ã¯è©³ç´°ãƒ­ã‚°ã‚’æŠ‘åˆ¶
                if self.connection_error_count <= 3:  # æœ€åˆã®3å›ã®ã¿ãƒ­ã‚°å‡ºåŠ›
                    sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG ERROR] âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: Ollamaæ¥ç¶šå¤±æ•—\n")
                    sys.stdout.flush()
            else:
                # æ¥ç¶šã‚¨ãƒ©ãƒ¼ä»¥å¤–ã®å ´åˆã¯ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆ
                self.connection_error_count = 0
                self.connection_error_suppress_until = None
                sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG ERROR] âŒ æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}\n")
                sys.stdout.flush()
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ï¼ˆå›ºå®šå¿œç­”ã®ã¿ï¼‰
            if is_connection_error:
                # Ollamaã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç°¡å˜ãªå›ºå®šå¿œç­”ã®ã¿
                fallback_responses = [
                    "ãã†ã§ã™ã­ã€‚",
                    "ãªã‚‹ã»ã©ã€‚", 
                    "ã‚ã‹ã‚Šã¾ã—ãŸã€‚",
                    "ã¯ã„ã€‚",
                    "ãã†ãªã‚“ã§ã™ã­ã€‚"
                ]
                import random
                fallback_response = random.choice(fallback_responses)
                
                self.last_reply = fallback_response
                self.last_source_words = query if isinstance(query, list) else [str(query)]
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±è¨­å®š
                self.request_id = 999  # å›ºå®šå¿œç­”ID
                self.worker_name = "static-fallback"
                self.start_timestamp_ns = int(start_time.timestamp() * 1_000_000_000)
                self.completion_timestamp_ns = int(end_time.timestamp() * 1_000_000_000)
                self.inference_duration_ms = (end_time - start_time).total_seconds() * 1000
                
                if self.connection_error_count <= 3:
                    sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG FALLBACK] ğŸ”„ å›ºå®šå¿œç­”ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {fallback_response}\n")
                    sys.stdout.flush()
            else:
                # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®çµæœã‚’è¨­å®š
                self.last_reply = ""
                self.last_source_words = []

    # ä¸¦åˆ—å‡¦ç†ç‰ˆï¼ˆä¸€æ™‚çš„ã«ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
    # def _perform_inference_old(self, request):
    #     """æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ä¸¦åˆ—å‡¦ç†é–¢æ•°"""
    #     # ... (çœç•¥) ...

    def run(self):
        sys.stdout.write("[NLG] å˜ä¸€ã‚¹ãƒ¬ãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹\n")
        sys.stdout.flush()
        
        while True:
            # å˜ä¸€ã‚¹ãƒ¬ãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ç‰¹ã«å‡¦ç†ãªã—
            time.sleep(0.01)  # 10mså¾…æ©Ÿ

if __name__ == "__main__":
    gen = NaturalLanguageGeneration()
    gen.run()