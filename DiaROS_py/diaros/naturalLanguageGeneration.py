# ============================================================
# ãƒ¢ãƒ‡ãƒ«è¨­å®š - ã“ã“ã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆ
# ============================================================
# ã€OpenAI API ãƒ¢ãƒ‡ãƒ«ã€‘ã‚¯ãƒ©ã‚¦ãƒ‰APIã€é«˜é€Ÿãƒ»é«˜å“è³ª
# MODEL_NAME = "gpt-3.5-turbo-0125"    # 587ms - æœ€é€Ÿãƒ»æœ€å®‰ãƒ»å®‰å®šï¼ˆæ¨å¥¨ï¼‰
# MODEL_NAME = "gpt-4.1-nano"          # 604ms - æœ€æ–°æŠ€è¡“ãƒ»é«˜é€Ÿ
# MODEL_NAME = "gpt-5-chat-latest"     # 708ms - GPT-5æœ€é€Ÿç‰ˆãƒ»å®‰å®š
# MODEL_NAME = "gpt-oss:20b"
# ã€Ollama ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã€‘ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å‹•ä½œã€GPUå¿…è¦
MODEL_NAME = "gemma3:4b"             # è»½é‡ãƒ»é«˜é€Ÿ
# MODEL_NAME = "gemma3:12b"            # é«˜å“è³ª
# MODEL_NAME = "gemma3:27b"            # æœ€é«˜å“è³ª

# ============================================================
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«åã®è¨­å®š - ã“ã“ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ‡ã‚Šæ›¿ãˆ
# ============================================================
# ã€å¯¾è©±ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‘éŸ³å£°èªè­˜çµæœã‹ã‚‰å¯¾è©±å¿œç­”ã‚’ç”Ÿæˆ
# PROMPT_FILE_NAME = "dialog_simple.txt"       # ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼ˆãƒã‚¤ã‚ºã‚¿ã‚°è‡ªå‹•é™¤å»ï¼‰
# PROMPT_FILE_NAME = "dialog_predict.txt"      # ç™ºè©±äºˆæ¸¬ä»˜ãï¼ˆãƒã‚¤ã‚ºã‚¿ã‚°è‡ªå‹•é™¤å»ï¼‰
# PROMPT_FILE_NAME = "dialog_tag.txt"          # ã‚¿ã‚°å‡¦ç†ä»˜ã
# PROMPT_FILE_NAME = "dialog_tag_ver2.txt"          # ã‚¿ã‚°å‡¦ç†ä»˜ã
# PROMPT_FILE_NAME = "dialog_explain.txt"      # è©³ç´°èª¬æ˜ä»˜ãï¼ˆãƒã‚¤ã‚ºã‚¿ã‚°è‡ªå‹•é™¤å»ï¼‰
# PROMPT_FILE_NAME = "dialog_example.txt"      # ä¾‹ç¤ºä»˜ãï¼ˆãƒã‚¤ã‚ºã‚¿ã‚°è‡ªå‹•é™¤å»ï¼‰
# PROMPT_FILE_NAME = "dialog_all.txt"          # å…¨æ©Ÿèƒ½ç‰ˆ
# PROMPT_FILE_NAME = "dialog_all_1115.txt"          # å…¨æ©Ÿèƒ½ç‰ˆ

# PROMPT_FILE_NAME = "dialog_phone.txt"        # é›»è©±å¯¾è©±ç”¨

# ã€éŸ³å£°èªè­˜çµæœã®è£œæ­£ãƒ»è£œå®Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‘éŸ³å£°èªè­˜çµæœã®ä¿®æ­£ã®ã¿
# PROMPT_FILE_NAME = "fix_asr_simple.txt"      # ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼ˆãƒã‚¤ã‚ºã‚¿ã‚°è‡ªå‹•é™¤å»ï¼‰
# PROMPT_FILE_NAME = "fix_asr.txt"             # æ¨™æº–ç‰ˆ
# PROMPT_FILE_NAME = "fix_asr_example.txt"     # ä¾‹ç¤ºä»˜ã
# PROMPT_FILE_NAME = "fix_asr_all.txt"     #
# PROMPT_FILE_NAME = "fix_asr_explain_fixed.txt"     #
# PROMPT_FILE_NAME = "fix_asr_predict.txt"     #
# PROMPT_FILE_NAME = "remdis_test_prompt.txt"     #
PROMPT_FILE_NAME = "dialog_first_stage.txt"     # 200msä»¥å†…é”æˆç”¨ï¼ˆçŸ­ã„ç›¸æ§Œã®ã¿ï¼‰

# ã€ã‚¿ã‚¤ãƒŸãƒ³ã‚°èª¿æ•´ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‘
# PROMPT_FILE_NAME = "example_make_delay.txt"  # é…å»¶ç”Ÿæˆç”¨
# PROMPT_FILE_NAME = "WebRTCVAD_timing_example.txt"   # WebRTCVADã‚¿ã‚¤ãƒŸãƒ³ã‚°ä¾‹
# PROMPT_FILE_NAME = "powerbase_timing_example.txt"   # ãƒ‘ãƒ¯ãƒ¼ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¤ãƒŸãƒ³ã‚°ä¾‹

# ã€ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‘
# PROMPT_FILE_NAME = "remdis_test.txt"         # ãƒ†ã‚¹ãƒˆç”¨ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰

# ============================================================

import requests
import json
import sys
import os
import time
import threading
from datetime import datetime, timedelta
from queue import Queue, Empty
from concurrent.futures import ThreadPoolExecutor
import openai
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import ChatOllama
from .timeTracker import get_time_tracker

class NaturalLanguageGeneration:
    def __init__(self):
        self.rc = { "word": "" }

        self.query = ""
        self.update_flag = False
        self.user_speak_is_final = False
        self.last_reply = ""  # ç”Ÿæˆã—ãŸå¯¾è©±æ–‡ã‚’ã“ã“ã«æ ¼ç´
        self.last_source_words = []  # å¯¾è©±ç”Ÿæˆã®å…ƒã«ã—ãŸéŸ³å£°èªè­˜çµæœã‚’æ ¼ç´

        # äºŒæ®µéšå¿œç­”ç”Ÿæˆç”¨ã®å¤‰æ•°
        self.first_stage_response = ""  # first_stageã§ç”Ÿæˆã—ãŸç›¸æ§Œã‚’ä¿å­˜
        self.current_stage = "first"  # first ã¾ãŸã¯ second
        self.turn_taking_decision_timestamp_ns = 0  # TurnTakingåˆ¤å®šæ™‚åˆ»ï¼ˆãƒŠãƒç§’ï¼‰

        # ROS2 bagè¨˜éŒ²ç”¨ã®è¿½åŠ æƒ…å ±
        self.last_request_id = 0
        self.last_worker_name = ""
        self.last_start_timestamp_ns = 0
        self.last_completion_timestamp_ns = 0
        self.last_inference_duration_ms = 0.0
        
        # æ–°ã—ã„æ™‚åˆ»æƒ…å ±ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åˆæœŸåŒ–
        self.request_id = 0
        self.worker_name = ""
        self.start_timestamp_ns = 0
        self.completion_timestamp_ns = 0
        self.inference_duration_ms = 0.0
        
        # æ¥ç¶šã‚¨ãƒ©ãƒ¼åˆ¶å¾¡ç”¨
        self.connection_error_count = 0
        self.last_connection_error_time = None
        self.connection_error_suppress_until = None
        
        # ã‚¿ã‚¤ãƒ ãƒˆãƒ©ãƒƒã‚«ãƒ¼åˆæœŸåŒ–
        self.time_tracker = get_time_tracker("nlg_pc")
        self.current_session_id = None
        
        # ä¸¦åˆ—å‡¦ç†è¨­å®šã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼ˆå˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹ç‰ˆï¼‰
        # self.inference_queue = Queue()  # æ¨è«–ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚­ãƒ¥ãƒ¼
        # self.result_queue = Queue()     # æ¨è«–çµæœã®ã‚­ãƒ¥ãƒ¼
        # self.request_counter = 0        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        self.last_request_time = None   # æœ€å¾Œã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚åˆ»
        self.last_inference_time = None # æœ€å¾Œã®æ¨è«–å®Ÿè¡Œæ™‚åˆ»
        self.inference_interval = 2.5   # æ¨è«–é–“éš”ï¼ˆç§’ï¼‰
        # self.executor = ThreadPoolExecutor(max_workers=3, thread_name_prefix="NLG-Worker")
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ä¸Šéƒ¨ã®MODEL_NAMEã‚’ä½¿ç”¨ï¼‰
        self.model_name = MODEL_NAME

        if self.model_name.startswith("gemma3:") or self.model_name.startswith("gpt-oss:"):
            # Ollama ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆgemma3ç³»ã€gpt-ossç³»ï¼‰
            sys.stdout.write(f'[NLG] Ollama {self.model_name}ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...\n')
            sys.stdout.flush()

            # gpt-ossã¯æ¨è«–ãƒ¢ãƒ‡ãƒ«ãªã®ã§éå¸¸ã«å¤§ããªãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå¿…è¦
            if self.model_name.startswith("gpt-oss:"):
                num_predict = 2000  # æ¨è«–ãƒˆãƒ¼ã‚¯ãƒ³ + å¿œç­”ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆè¤‡é›‘ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯¾å¿œï¼‰
                sys.stdout.write(f'[NLG] âš ï¸  gpt-oss:20bã¯æ¨è«–ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã€å¿œç­”ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ (num_predict={num_predict})\n')
                sys.stdout.flush()
            else:
                num_predict = 10  # gemma3ç³»ã¯10ãƒˆãƒ¼ã‚¯ãƒ³ã§çµ±ä¸€ï¼ˆçŸ­ã„ç›¸æ§Œç”¨ï¼‰

            # gpt-oss:20bã®é«˜é€ŸåŒ–è¨­å®šï¼ˆæ¨è«–ã‚’æœ€å°é™ã«ï¼‰
            if self.model_name.startswith("gpt-oss:"):
                temperature = 0.3  # ã‚ˆã‚Šæ±ºå®šçš„ã«ï¼ˆæ¨è«–ã‚’æ¸›ã‚‰ã™ï¼‰
                additional_kwargs = {
                    "verbose": True,
                    "num_ctx": 4096,
                    "num_batch": 3072,
                    "think": "low"  # æ¨è«–ãƒ¢ãƒ¼ãƒ‰æœ€å°åŒ–ï¼ˆé«˜é€ŸåŒ–ï¼‰
                }
            else:
                temperature = 0.7  # gemma3ç³»ã¯æ¨™æº–è¨­å®š
                additional_kwargs = {
                    "verbose": True,
                    "num_ctx": 4096,
                    "num_batch": 3072
                }

            self.ollama_model = ChatOllama(
                model=self.model_name,
                verbose=True,  # è©³ç´°ãƒ­ã‚°æœ‰åŠ¹åŒ–
                temperature=temperature,  # å¿œç­”ã®å¤šæ§˜æ€§
                top_p=0.9,  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¨­å®š
                num_predict=num_predict,  # æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°
                keep_alive="10m",  # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒã™ã‚‹æ™‚é–“ï¼ˆå»¶é•·ï¼‰
                additional_kwargs=additional_kwargs
            )
            sys.stdout.write(f'[NLG] âœ… {self.model_name}ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº† (num_predict={num_predict})\n')
            sys.stdout.flush()

        elif self.model_name.startswith("gpt-") or self.model_name.startswith("o1") or self.model_name.startswith("chatgpt-"):
            # OpenAI APIè¨­å®šï¼ˆGPT-5, GPT-4, GPT-3.5, o1ãªã©å…¨ã¦ã®OpenAIãƒ¢ãƒ‡ãƒ«ï¼‰
            sys.stdout.write(f'[NLG] OpenAI {self.model_name}ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...\n')
            sys.stdout.flush()

            # OpenAI APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®š
            openai.api_key = os.environ.get("OPENAI_API_KEY")
            if not openai.api_key:
                sys.stdout.write('[NLG ERROR] OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“\n')
                sys.stdout.flush()
                raise ValueError("OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            else:
                sys.stdout.write(f'[NLG] âœ… {self.model_name}ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†\n')
                sys.stdout.flush()

        else:
            raise ValueError(f"æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«: {self.model_name}")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        self.prompt_file_name = PROMPT_FILE_NAME
        prompt_dir = os.path.join(os.path.dirname(__file__), 'prompts')
        self.prompt_file_path = os.path.join(prompt_dir, self.prompt_file_name)

        if os.path.exists(self.prompt_file_path):
            sys.stdout.write(f'[NLG] âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª: {self.prompt_file_name}\n')
            sys.stdout.flush()
        else:
            sys.stdout.write(f'[NLG WARNING] âš ï¸  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.prompt_file_path}\n')
            sys.stdout.flush()

        sys.stdout.write('NaturalLanguageGeneration (å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹) start up.\n')
        sys.stdout.write(f'ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.model_name}\n')
        sys.stdout.write('=====================================================\n')

    def update(self, query):
        now = datetime.now()

        # æ¥ç¶šã‚¨ãƒ©ãƒ¼æŠ‘åˆ¶ä¸­ã¯æ–°ã—ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ã‘ä»˜ã‘ãªã„
        if self.connection_error_suppress_until and now < self.connection_error_suppress_until:
            return

        # éŸ³å£°èªè­˜çµæœãŒãƒªã‚¹ãƒˆã®å ´åˆã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸ‹ã‚è¾¼ã‚€
        self.asr_results = None
        # ç©ºãƒªã‚¹ãƒˆã¾ãŸã¯å…¨ã¦ç©ºæ–‡å­—åˆ—ãªã‚‰ä½•ã‚‚ã—ãªã„
        if isinstance(query, list):
            if not query or all((not x or x.strip() == "") for x in query):
                self.update_flag = False
                return
            self.asr_results = query
            self.query = query
        else:
            if not query or (isinstance(query, str) and query.strip() == ""):
                self.update_flag = False
                return
            self.query = query
            self.asr_results = None

        # 2.5ç§’é–“éš”åˆ¶å¾¡ã‚’å‰Šé™¤: éŸ³å£°èªè­˜çµæœãŒæ¥ã‚‹ãŸã³ã«ã™ãå¿œç­”ç”Ÿæˆ
        # (ä»¥å‰ã®é–“éš”åˆ¶å¾¡ã‚³ãƒ¼ãƒ‰ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ)

        # å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹æ¨è«–ã«å¤‰æ›´ï¼ˆä¸¦åˆ—å‡¦ç†ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
        # self.request_counter += 1
        # request_id = self.request_counter
        request_id = 1  # å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹ã§ã¯å›ºå®šID

        sys.stdout.write(f"[{now.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸš€ æ¨è«–é–‹å§‹ (ID: {request_id}, ãƒ¢ãƒ‡ãƒ«: {self.model_name})\n")
        sys.stdout.flush()

        # å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹æ¨è«–ã‚’å®Ÿè¡Œï¼ˆä¸¦åˆ—å‡¦ç†ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
        # future = self.executor.submit(self._perform_parallel_inference, request_id, query, now)

        # ã‚·ãƒ³ãƒ—ãƒ«ãªæ¨è«–å®Ÿè¡Œã«æˆ»ã™
        self._perform_simple_inference(query)

        # æœ€å¾Œã®æ¨è«–æ™‚åˆ»ã‚’æ›´æ–°
        self.last_inference_time = now
        self.last_request_time = now
        self.update_flag = True
        
    def set_session_id(self, session_id: str):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’è¨­å®š"""
        self.current_session_id = session_id

    def generate_first_stage(self, query):
        """First stage: å¸¸ã«ç›¸æ§Œã‚’ç”Ÿæˆï¼ˆéŸ³å£°èªè­˜çµæœãŒæ¥ã‚‹ãŸã³ã«å®Ÿè¡Œï¼‰"""
        start_time = datetime.now()

        try:
            asr_results = query if isinstance(query, list) else [str(query)]

            if not asr_results or all((not x or x.strip() == "") for x in asr_results):
                self.first_stage_response = ""
                return

            # LLMå‘¼ã³å‡ºã—
            llm_start_time = datetime.now()
            sys.stdout.write(f"[{llm_start_time.strftime('%H:%M:%S.%f')[:-3]}][NLG FIRST_STAGE] ğŸ¤– ç›¸æ§Œç”Ÿæˆé–‹å§‹\n")

            try:
                if self.model_name.startswith("gemma3:") or self.model_name.startswith("gpt-oss:"):
                    # Ollama APIã‚’ç›´æ¥å‘¼ã³å‡ºã—ã¦TTFTè¨ˆæ¸¬ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
                    import requests

                    prompt_build_start = datetime.now()
                    # è¶…ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                    simple_prompt = f"çŸ­ã„ç›¸æ§Œã‚’ä¸€ã¤: {', '.join(asr_results[:2])}"
                    prompt_build_end = datetime.now()
                    sys.stdout.write(f"[{prompt_build_end.strftime('%H:%M:%S.%f')[:-3]}][NLG FIRST_STAGE DEBUG] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰: {(prompt_build_end - prompt_build_start).total_seconds() * 1000:.1f}ms\n")
                    sys.stdout.write(f"[{prompt_build_end.strftime('%H:%M:%S.%f')[:-3]}][NLG FIRST_STAGE DEBUG] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(simple_prompt)}æ–‡å­—\n")
                    sys.stdout.flush()

                    # Ollama APIç›´æ¥å‘¼ã³å‡ºã—ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
                    api_start = datetime.now()
                    response = requests.post(
                        'http://localhost:11434/api/generate',
                        json={
                            'model': self.model_name,
                            'prompt': simple_prompt,
                            'stream': True,
                            'options': {
                                'temperature': 0.3,
                                'num_predict': 10,
                                'num_ctx': 512,
                                'num_batch': 256
                            }
                        },
                        stream=True,
                        timeout=30
                    )

                    res = ""
                    first_token_time = None
                    token_count = 0

                    for line in response.iter_lines():
                        if line:
                            try:
                                chunk_data = json.loads(line)
                                token_fragment = chunk_data.get('response', '')

                                if token_fragment:
                                    token_count += 1

                                    # Time to First Token (TTFT) è¨ˆæ¸¬
                                    if first_token_time is None:
                                        first_token_time = datetime.now()
                                        ttft_ms = (first_token_time - api_start).total_seconds() * 1000
                                        sys.stdout.write(f"[{first_token_time.strftime('%H:%M:%S.%f')[:-3]}][NLG FIRST_STAGE DEBUG] ğŸ¯ TTFT (Time to First Token): {ttft_ms:.1f}ms\n")
                                        sys.stdout.flush()

                                    res += token_fragment

                                # å®Œäº†ãƒã‚§ãƒƒã‚¯
                                if chunk_data.get('done', False):
                                    api_end = datetime.now()
                                    total_time = (api_end - api_start).total_seconds() * 1000

                                    # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
                                    load_duration = chunk_data.get('load_duration', 0) / 1e6  # ns â†’ ms
                                    prompt_eval_duration = chunk_data.get('prompt_eval_duration', 0) / 1e6
                                    eval_duration = chunk_data.get('eval_duration', 0) / 1e6
                                    prompt_eval_count = chunk_data.get('prompt_eval_count', 0)
                                    eval_count = chunk_data.get('eval_count', 0)

                                    sys.stdout.write(f"[{api_end.strftime('%H:%M:%S.%f')[:-3]}][NLG FIRST_STAGE DEBUG] LLMæ¨è«–æ™‚é–“ï¼ˆç·è¨ˆï¼‰: {total_time:.1f}ms\n")
                                    sys.stdout.write(f"[{api_end.strftime('%H:%M:%S.%f')[:-3]}][NLG FIRST_STAGE DEBUG] ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {token_count}\n")
                                    sys.stdout.write(f"[{api_end.strftime('%H:%M:%S.%f')[:-3]}][NLG FIRST_STAGE DEBUG] âš™ï¸ load_duration: {load_duration:.1f}ms\n")
                                    sys.stdout.write(f"[{api_end.strftime('%H:%M:%S.%f')[:-3]}][NLG FIRST_STAGE DEBUG] âš™ï¸ prompt_eval_duration: {prompt_eval_duration:.1f}ms ({prompt_eval_count} tokens)\n")
                                    sys.stdout.write(f"[{api_end.strftime('%H:%M:%S.%f')[:-3]}][NLG FIRST_STAGE DEBUG] âš™ï¸ eval_duration: {eval_duration:.1f}ms ({eval_count} tokens)\n")
                                    sys.stdout.write(f"[{api_end.strftime('%H:%M:%S.%f')[:-3]}][NLG FIRST_STAGE DEBUG] âš ï¸ ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰åˆ†æ: TTFT({ttft_ms:.1f}ms) - prompt_eval({prompt_eval_duration:.1f}ms) - load({load_duration:.1f}ms) = {ttft_ms - prompt_eval_duration - load_duration:.1f}ms\n")
                                    sys.stdout.flush()
                                    break

                            except json.JSONDecodeError:
                                continue

                elif self.model_name.startswith("gpt-") or self.model_name.startswith("o1"):
                    # è¶…ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                    simple_prompt = f"çŸ­ã„ç›¸æ§Œã‚’ä¸€ã¤: {', '.join(asr_results[:2])}"

                    messages = [
                        {"role": "system", "content": simple_prompt},
                        {"role": "user", "content": "ä¸Šè¨˜ã®éŸ³å£°èªè­˜çµæœã‹ã‚‰ç›¸æ§Œã‚’ä¸€ã¤å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"}
                    ]
                    response = openai.chat.completions.create(
                        model=self.model_name,
                        messages=messages,
                        max_completion_tokens=20,
                        temperature=0.3
                    )
                    res = response.choices[0].message.content.strip() if response.choices[0].message.content else ""

                # ç›¸æ§Œã®å¾Œå‡¦ç†: æ”¹è¡Œãƒ»å¥èª­ç‚¹é™¤å»
                res = res.replace('\n', '').replace('\r', '').replace('ã€‚', '').replace('ã€', '').strip()

                llm_end_time = datetime.now()
                llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000

                self.first_stage_response = res
                sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG FIRST_STAGE] âœ… ç›¸æ§Œç”Ÿæˆå®Œäº† ({llm_duration:.1f}ms): '{res}'\n")
                sys.stdout.flush()

            except Exception as api_error:
                sys.stdout.write(f"[NLG ERROR] first_stageç”Ÿæˆã‚¨ãƒ©ãƒ¼: {api_error}\n")
                sys.stdout.flush()
                self.first_stage_response = "ã†ã‚“"  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

        except Exception as e:
            sys.stdout.write(f"[NLG ERROR] first_stageå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}\n")
            sys.stdout.flush()
            self.first_stage_response = "ã†ã‚“"  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

    def generate_second_stage(self, query):
        """Second stage: turnTakingãŒå¿œç­”åˆ¤å®šã‚’å‡ºã—ãŸã‚‰å®Ÿè¡Œ"""
        start_time = datetime.now()

        try:
            asr_results = query if isinstance(query, list) else [str(query)]

            if not asr_results or all((not x or x.strip() == "") for x in asr_results):
                self.last_reply = ""
                self.last_source_words = []
                return

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            prompt_dir = os.path.join(os.path.dirname(__file__), 'prompts')
            second_stage_prompt_path = os.path.join(prompt_dir, 'dialog_second_stage.txt')

            try:
                with open(second_stage_prompt_path, 'r', encoding='utf-8') as f:
                    prompt_template = f.read()

                # {first_stage_response} ã‚’å®Ÿéš›ã®ç›¸æ§Œã«ç½®æ›ï¼ˆã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ä¸è¦ï¼‰
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨éŸ³å£°èªè­˜çµæœã‚’ç›´æ¥çµåˆ
                prompt_with_backchannel = prompt_template.replace('{first_stage_response}', self.first_stage_response)
                prompt = f"{prompt_with_backchannel}\n\n# å…ˆã»ã©æ‰“ã£ãŸç›¸æ§Œ\n{self.first_stage_response}\n\n# éŸ³å£°èªè­˜çµæœ\nã¶ã¤åˆ‡ã‚Šã®éŸ³å£°èªè­˜çµæœ: {', '.join(asr_results)}"

            except FileNotFoundError:
                sys.stdout.write(f"[NLG ERROR] second_stageãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {second_stage_prompt_path}\n")
                sys.stdout.flush()
                return

            # LLMå‘¼ã³å‡ºã—
            llm_start_time = datetime.now()
            sys.stdout.write(f"[{llm_start_time.strftime('%H:%M:%S.%f')[:-3]}][NLG SECOND_STAGE] ğŸ¤– æœ¬å¿œç­”ç”Ÿæˆé–‹å§‹ï¼ˆç›¸æ§Œ: '{self.first_stage_response}'ï¼‰\n")

            try:
                if self.model_name.startswith("gemma3:") or self.model_name.startswith("gpt-oss:"):
                    messages = [
                        ("system", prompt),
                        ("human", "ä¸Šè¨˜ã®éŸ³å£°èªè­˜çµæœã‹ã‚‰æœ¬å¿œç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")
                    ]
                    query_prompt = ChatPromptTemplate.from_messages(messages)
                    chain = query_prompt | self.ollama_model | StrOutputParser()
                    res = chain.invoke({})

                elif self.model_name.startswith("gpt-") or self.model_name.startswith("o1"):
                    messages = [
                        {"role": "system", "content": prompt},
                        {"role": "user", "content": "ä¸Šè¨˜ã®éŸ³å£°èªè­˜çµæœã‹ã‚‰æœ¬å¿œç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"}
                    ]
                    response = openai.chat.completions.create(
                        model=self.model_name,
                        messages=messages,
                        max_completion_tokens=50,
                        temperature=0.3
                    )
                    res = response.choices[0].message.content.strip() if response.choices[0].message.content else ""

                # æ”¹è¡Œé™¤å»
                res = res.replace('\n', '').replace('\r', '')

                llm_end_time = datetime.now()
                llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                total_duration = (llm_end_time - start_time).total_seconds() * 1000

                # æœ€çµ‚çš„ãªå¿œç­”ã¯æœ¬å¿œç­”ã®ã¿ï¼ˆãƒ¡ã‚¤ãƒ³PCå´ã§first_stageã¯æ—¢ã«å†ç”Ÿã•ã‚Œã¦ã„ã‚‹ï¼‰
                final_response = res

                self.last_reply = final_response
                self.last_source_words = asr_results

                # ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±ã‚’è¨­å®š
                self.request_id = 1
                self.worker_name = "nlg-two-stage"
                self.start_timestamp_ns = int(start_time.timestamp() * 1_000_000_000)
                self.completion_timestamp_ns = int(llm_end_time.timestamp() * 1_000_000_000)
                self.inference_duration_ms = total_duration

                sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG SECOND_STAGE] âœ… æœ¬å¿œç­”ç”Ÿæˆå®Œäº† ({llm_duration:.1f}ms): '{res}'\n")
                sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG SECOND_STAGE] ğŸ æœ€çµ‚å¿œç­”: '{final_response}'\n")
                sys.stdout.flush()

            except Exception as api_error:
                sys.stdout.write(f"[NLG ERROR] second_stageç”Ÿæˆã‚¨ãƒ©ãƒ¼: {api_error}\n")
                sys.stdout.flush()
                self.last_reply = self.first_stage_response  # ç›¸æ§Œã®ã¿ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                self.last_source_words = asr_results

        except Exception as e:
            sys.stdout.write(f"[NLG ERROR] second_stageå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}\n")
            sys.stdout.flush()
            self.last_reply = self.first_stage_response  # ç›¸æ§Œã®ã¿ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            self.last_source_words = asr_results

    def _perform_simple_inference(self, query):
        """ã‚·ãƒ³ãƒ—ãƒ«ãªå˜ä¸€ã‚¹ãƒ¬ãƒƒãƒ‰æ¨è«– (gemma3:12bä½¿ç”¨)"""
        start_time = datetime.now()

        # æ¨è«–é–‹å§‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        if self.current_session_id:
            self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "inference_start", {
                "query_type": "list" if isinstance(query, list) else "string",
                "query_length": len(query) if isinstance(query, list) else len(str(query))
            })

        try:
            res = ""  # resã‚’å¿…ãšåˆæœŸåŒ–
            asr_results = self.asr_results

            if asr_results and isinstance(asr_results, list) and len(asr_results) >= 1:
                if all((not x or x.strip() == "") for x in asr_results):
                    self.last_reply = ""
                    self.last_source_words = []
                    return

                # ç‰¹å®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨æ™‚ã¯ [é›‘éŸ³][ç„¡éŸ³]<unk> ã‚’é™¤å»
                noise_tag_removal_prompts = [
                    "dialog_simple.txt",
                    "fix_asr_simple.txt",
                    "dialog_predict.txt",
                    "dialog_explain.txt",
                    "dialog_example.txt"
                ]

                if self.prompt_file_name in noise_tag_removal_prompts:
                    # ä¸è¦ãªã‚¿ã‚°ã‚’é™¤å»
                    cleaned_asr_results = []
                    for asr in asr_results:
                        # [é›‘éŸ³]ã€[ç„¡éŸ³]ã€<unk>ã‚’é™¤å»
                        cleaned = asr.replace("[é›‘éŸ³]", "").replace("[ç„¡éŸ³]", "").replace("<unk>", "")
                        cleaned = cleaned.strip()
                        if cleaned:  # ç©ºæ–‡å­—åˆ—ã§ãªã„å ´åˆã®ã¿è¿½åŠ 
                            cleaned_asr_results.append(cleaned)

                    # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨
                    asr_results_for_prompt = cleaned_asr_results if cleaned_asr_results else asr_results
                else:
                    # ãã®ä»–ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã¯ãã®ã¾ã¾ä½¿ç”¨
                    asr_results_for_prompt = asr_results

                # éŸ³å£°èªè­˜çµæœã‚’ã™ã¹ã¦åˆ—æŒ™
                asr_lines = []
                for idx, asr in enumerate(asr_results_for_prompt):
                    asr_lines.append(f"èªè­˜çµæœ{idx+1}: {asr}")

                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆè¨­å®šã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½¿ç”¨ï¼‰
                try:
                    with open(self.prompt_file_path, 'r', encoding='utf-8') as f:
                        prompt = f.read()
                    if not prompt.strip():
                        sys.stdout.write(f"[NLG ERROR] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™: {self.prompt_file_path}\n")
                        sys.stdout.flush()
                        return
                except FileNotFoundError:
                    sys.stdout.write(f"[NLG ERROR] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.prompt_file_path}\n")
                    sys.stdout.flush()
                    return
                except Exception as e:
                    sys.stdout.write(f"[NLG ERROR] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {self.prompt_file_path} - {e}\n")
                    sys.stdout.flush()
                    return
                
                # LLMå‘¼ã³å‡ºã—
                llm_start_time = datetime.now()
                sys.stdout.write(f"[{llm_start_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸ¤– {self.model_name}æ¨è«–é–‹å§‹\n")
                # LLMæ¨è«–é–‹å§‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                if self.current_session_id:
                    self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "llm_start", {
                        "model": self.model_name,
                        "prompt_type": "asr_dialogue",
                        "prompt_length": len(prompt),
                        "asr_count": len(asr_results)
                    })

                # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæ¨è«–å‡¦ç†
                try:
                    if self.model_name.startswith("gemma3:") or self.model_name.startswith("gpt-oss:"):
                        # Ollama ãƒ¢ãƒ‡ãƒ«ï¼ˆAPIç›´æ¥å‘¼ã³å‡ºã—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹ï¼‰
                        full_prompt = f"{prompt}\n\nã¶ã¤åˆ‡ã‚Šã®éŸ³å£°èªè­˜çµæœ: {', '.join(asr_results_for_prompt)}"

                        # Ollama APIç›´æ¥å‘¼ã³å‡ºã—ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
                        try:
                            api_start_time = datetime.now()
                            api_response = requests.post(
                                'http://localhost:11434/api/generate',
                                json={
                                    'model': self.model_name,
                                    'prompt': full_prompt,
                                    'stream': True,  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹åŒ–ï¼ˆTTFTæœ€å°åŒ–ï¼‰
                                    'options': {
                                        'temperature': 0.7,
                                        'top_p': 0.9,
                                        'num_predict': 10,  # çŸ­ã„ç›¸æ§Œã§é«˜é€ŸåŒ–
                                        'num_ctx': 4096,
                                        'num_batch': 3072
                                    }
                                },
                                timeout=30,
                                stream=True
                            )

                            res = ""
                            first_token_time = None

                            if api_response.status_code == 200:
                                for line in api_response.iter_lines():
                                    if line:
                                        try:
                                            chunk_data = json.loads(line)
                                            token_fragment = chunk_data.get('response', '')

                                            if token_fragment:
                                                # TTFTè¨ˆæ¸¬
                                                if first_token_time is None:
                                                    first_token_time = datetime.now()
                                                    ttft_ms = (first_token_time - api_start_time).total_seconds() * 1000
                                                    sys.stdout.write(f"[{first_token_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸ¯ TTFT: {ttft_ms:.1f}ms\n")
                                                    sys.stdout.flush()

                                                res += token_fragment

                                            # å®Œäº†ãƒã‚§ãƒƒã‚¯
                                            if chunk_data.get('done', False):
                                                llm_end_time = datetime.now()
                                                llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000

                                                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ï¼ˆè©³ç´°ï¼‰
                                                load_duration = chunk_data.get('load_duration', 0) / 1e6
                                                prompt_eval_duration = chunk_data.get('prompt_eval_duration', 0) / 1e6
                                                eval_duration = chunk_data.get('eval_duration', 0) / 1e6
                                                sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] â±ï¸ load: {load_duration:.1f}ms, prompt: {prompt_eval_duration:.1f}ms, eval: {eval_duration:.1f}ms â†’ total: {llm_duration:.1f}ms\n")
                                                break
                                        except json.JSONDecodeError:
                                            continue
                            else:
                                res = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
                        except Exception as api_error:
                            sys.stdout.write(f"[NLG ERROR] Ollama APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {api_error}\n")
                            sys.stdout.flush()
                            res = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"

                    elif self.model_name.startswith("gpt-") or self.model_name.startswith("o1"):
                        # OpenAI APIï¼ˆGPT-5, GPT-4, o1ãªã©ï¼‰
                        messages = [
                            {"role": "system", "content": prompt},
                            {"role": "user", "content": f"ã¶ã¤åˆ‡ã‚Šã®éŸ³å£°èªè­˜çµæœ: {', '.join(asr_results_for_prompt)}"}
                        ]

                        # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°
                        sys.stdout.write(f"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}][NLG DEBUG] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt)}æ–‡å­—\n")
                        sys.stdout.write(f"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}][NLG DEBUG] éŸ³å£°èªè­˜çµæœæ•°: {len(asr_results_for_prompt)}\n")
                        sys.stdout.flush()

                        # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ã®æœ€é©åŒ–è¨­å®š
                        if self.model_name.startswith("gpt-4.1"):
                            # GPT-4.1ç³»: æœ€é€Ÿ540ms
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_completion_tokens=50,
                                temperature=0.3
                            )
                        elif "chat-latest" in self.model_name:
                            # GPT-5-chat-latest: æ¨è«–æœ€å°åŒ–ç‰ˆ
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_completion_tokens=50
                            )
                        elif self.model_name.startswith("gpt-5") or self.model_name.startswith("o1"):
                            # GPT-5/o1: æ¨è«–ãƒ¢ãƒ‡ãƒ«
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_completion_tokens=500,
                                reasoning_effort="low"
                            )
                        else:
                            # GPT-4oç³»: æ¨™æº–
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_tokens=50,
                                temperature=0.3
                            )

                        res = response.choices[0].message.content.strip() if response.choices[0].message.content else ""

                        # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°
                        sys.stdout.write(f"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}][NLG DEBUG] APIå¿œç­”é•·: {len(res)}æ–‡å­—\n")
                        sys.stdout.flush()

                    llm_end_time = datetime.now()
                    llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âœ… {self.model_name}æ¨è«–å®Œäº† (LLMæ™‚é–“: {llm_duration:.1f}ms)\n")
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] ç”Ÿæˆå¿œç­”: '{res}'\n")
                    sys.stdout.flush()

                except Exception as api_error:
                    # APIå‘¼ã³å‡ºã—å¤±æ•—æ™‚ã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®š
                    res = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
                    llm_end_time = datetime.now()
                    llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âŒ {self.model_name} APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {api_error}\n")
                    sys.stdout.flush()

                # LLMæ¨è«–å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                if self.current_session_id:
                    self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "llm_complete", {
                        "model": self.model_name,
                        "llm_duration_ms": llm_duration,
                        "response_length": len(res)
                    })
                
                source_words = asr_results
                
            else:
                if not query or (isinstance(query, list) and all((not x or x.strip() == "") for x in query)):
                    self.last_reply = ""
                    self.last_source_words = []
                    return
                elif query == "dummy":
                    res = "ã¯ã„"
                    source_words = [str(query)]
                else:
                    text_input = query
                    role = "å„ªã—ã„æ€§æ ¼ã®ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰ã¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè©±ã«å¯¾ã—ã¦ç›¸æ‰‹ã‚’åŠ´ã‚‹ã‚ˆã†ãªè¿”ç­”ã®ã¿ã‚’ï¼’ï¼æ–‡å­—ä»¥å†…ã§ã—ã¦ãã ã•ã„ã€‚"

                    llm_start_time = datetime.now()

                    if self.model_name.startswith("gemma3:") or self.model_name.startswith("gpt-oss:"):
                        # Ollama gemma3ç³»ãƒ¢ãƒ‡ãƒ«ï¼ˆLangChainçµŒç”±ï¼‰
                        messages = [
                            ("system", role),
                            ("human", text_input)
                        ]
                        query_prompt = ChatPromptTemplate.from_messages(messages)
                        chain = query_prompt | self.ollama_model | StrOutputParser()
                        res = chain.invoke({})

                    elif self.model_name.startswith("gpt-") or self.model_name.startswith("o1"):
                        # OpenAI APIï¼ˆGPT-5, GPT-4, o1ãªã©ï¼‰
                        messages = [
                            {"role": "system", "content": role},
                            {"role": "user", "content": text_input}
                        ]

                        # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ã®æœ€é©åŒ–è¨­å®š
                        if self.model_name.startswith("gpt-4.1"):
                            # GPT-4.1ç³»: æœ€é€Ÿ540ms
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_completion_tokens=50,
                                temperature=0.3
                            )
                        elif "chat-latest" in self.model_name:
                            # GPT-5-chat-latest: æ¨è«–æœ€å°åŒ–ç‰ˆ
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_completion_tokens=50
                            )
                        elif self.model_name.startswith("gpt-5") or self.model_name.startswith("o1"):
                            # GPT-5/o1: æ¨è«–ãƒ¢ãƒ‡ãƒ«
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_completion_tokens=500,
                                reasoning_effort="low"
                            )
                        else:
                            # GPT-4oç³»: æ¨™æº–
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_tokens=50,
                                temperature=0.3
                            )

                        res = response.choices[0].message.content.strip()

                    llm_end_time = datetime.now()
                    llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âœ… {self.model_name}æ¨è«–å®Œäº† (LLMæ™‚é–“: {llm_duration:.1f}ms)\n")
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] ç”Ÿæˆå¿œç­”: '{res}'\n")
                    sys.stdout.flush()

                    if ":" in res:
                        res = res.split(":", 1)[1]

                    source_words = [str(query)]
            
            # æ”¹è¡Œã‚’é™¤å»ã—ã¦1è¡Œã«ã™ã‚‹
            res = res.replace('\n', '').replace('\r', '')
            
            end_time = datetime.now()
            total_duration = (end_time - start_time).total_seconds() * 1000
            
            # çµæœã‚’è¨­å®š
            self.last_reply = res
            self.last_source_words = source_words
            
            # ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±ã‚’è¨­å®š
            self.request_id = 1
            self.worker_name = "nlg-single"
            self.start_timestamp_ns = int(start_time.timestamp() * 1_000_000_000)
            self.completion_timestamp_ns = int(end_time.timestamp() * 1_000_000_000)
            self.inference_duration_ms = total_duration
            
            # æˆåŠŸæ™‚ã¯æ¥ç¶šã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
            self.connection_error_count = 0
            self.connection_error_suppress_until = None
            
            # æ¨è«–å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
            if self.current_session_id:
                self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "inference_complete", {
                    "total_duration_ms": total_duration,
                    "response": res,
                    "source_words": source_words
                })
            
            sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸ å‡¦ç†å®Œäº† (ç·æ™‚é–“: {total_duration:.1f}ms): {res}\n")
            sys.stdout.flush()
            
        except Exception as e:
            end_time = datetime.now()
            
            # æ¥ç¶šã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†
            error_str = str(e)
            is_connection_error = (
                "[Errno 111] Connection refused" in error_str or
                "llama runner process has terminated" in error_str or
                "broken pipe" in error_str or
                "status code: 500" in error_str
            )
            
            if is_connection_error:
                self.connection_error_count += 1
                # é€£ç¶šæ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒ5å›ä»¥ä¸Šãªã‚‰30ç§’é–“ãƒªã‚¯ã‚¨ã‚¹ãƒˆæŠ‘åˆ¶
                if self.connection_error_count >= 5:
                    self.connection_error_suppress_until = end_time + timedelta(seconds=30)
                    if self.connection_error_count == 5:  # åˆå›æŠ‘åˆ¶æ™‚ã®ã¿ãƒ­ã‚°å‡ºåŠ›
                        sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG WARNING] ğŸš« é€£ç¶šæ¥ç¶šã‚¨ãƒ©ãƒ¼æ¤œå‡ºã€‚30ç§’é–“ãƒªã‚¯ã‚¨ã‚¹ãƒˆæŠ‘åˆ¶ã—ã¾ã™\n")
                        sys.stdout.flush()
                # æ¥ç¶šã‚¨ãƒ©ãƒ¼ã¯è©³ç´°ãƒ­ã‚°ã‚’æŠ‘åˆ¶
                if self.connection_error_count <= 3:  # æœ€åˆã®3å›ã®ã¿ãƒ­ã‚°å‡ºåŠ›
                    sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG ERROR] âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: Ollamaæ¥ç¶šå¤±æ•—\n")
                    sys.stdout.flush()
            else:
                # æ¥ç¶šã‚¨ãƒ©ãƒ¼ä»¥å¤–ã®å ´åˆã¯ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆ
                self.connection_error_count = 0
                self.connection_error_suppress_until = None
                sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG ERROR] âŒ æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}\n")
                sys.stdout.flush()
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ï¼ˆå›ºå®šå¿œç­”ã®ã¿ï¼‰
            if is_connection_error:
                # Ollamaã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç°¡å˜ãªå›ºå®šå¿œç­”ã®ã¿
                fallback_responses = [
                    "ãã†ã§ã™ã­ã€‚",
                    "ãªã‚‹ã»ã©ã€‚", 
                    "ã‚ã‹ã‚Šã¾ã—ãŸã€‚",
                    "ã¯ã„ã€‚",
                    "ãã†ãªã‚“ã§ã™ã­ã€‚"
                ]
                import random
                fallback_response = random.choice(fallback_responses)
                
                self.last_reply = fallback_response
                self.last_source_words = query if isinstance(query, list) else [str(query)]
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±è¨­å®š
                self.request_id = 999  # å›ºå®šå¿œç­”ID
                self.worker_name = "static-fallback"
                self.start_timestamp_ns = int(start_time.timestamp() * 1_000_000_000)
                self.completion_timestamp_ns = int(end_time.timestamp() * 1_000_000_000)
                self.inference_duration_ms = (end_time - start_time).total_seconds() * 1000
                
                if self.connection_error_count <= 3:
                    sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG FALLBACK] ğŸ”„ å›ºå®šå¿œç­”ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {fallback_response}\n")
                    sys.stdout.flush()
            else:
                # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®çµæœã‚’è¨­å®š
                self.last_reply = ""
                self.last_source_words = []

<<<<<<< HEAD

    def run(self):
        sys.stdout.write("[NLG] å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹æ¨è«–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹ (2.5ç§’é–“éš”åˆ¶å¾¡)\n")
        sys.stdout.write(f"[NLG] ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.model_name}\n")
        sys.stdout.flush()
        
=======
    # _perform_parallel_inference() ãƒ¡ã‚½ãƒƒãƒ‰ã¯ç¾åœ¨ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
    # GPT-3.5-turboç‰ˆã¯å¿…è¦ã«ãªã£ãŸã‚‰å®Ÿè£…
    """
    def _perform_parallel_inference(self, request_id, query, start_time):
        \"""ä¸¦åˆ—æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°\"""
        worker_name = f"nlg-worker-{request_id % 3 + 1}"  # 3ã¤ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã§äº¤äº’å®Ÿè¡Œ
        
        sys.stdout.write(f"[{start_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸ”„ {worker_name} æ¨è«–é–‹å§‹\n")
        sys.stdout.flush()
        
        # æ¨è«–é–‹å§‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        if self.current_session_id:
            self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "parallel_inference_start", {
                "worker_name": worker_name,
                "request_id": request_id,
                "query_type": "list" if isinstance(query, list) else "string",
                "query_length": len(query) if isinstance(query, list) else len(str(query))
            })
        
        try:
            res = ""  # resã‚’å¿…ãšåˆæœŸåŒ–
            asr_results = self.asr_results if hasattr(self, 'asr_results') else None
            
            if asr_results and isinstance(asr_results, list) and len(asr_results) >= 1:
                if all((not x or x.strip() == "") for x in asr_results):
                    self.last_reply = ""
                    self.last_source_words = []
                    return
                
                # éŸ³å£°èªè­˜çµæœã‚’ã™ã¹ã¦åˆ—æŒ™
                asr_lines = []
                for idx, asr in enumerate(asr_results):
                    asr_lines.append(f"èªè­˜çµæœ{idx+1}: {asr}")
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
                current_dir = os.path.dirname(os.path.abspath(__file__))
                prompt_file_path = os.path.join(current_dir, "prompts", "example_dialog.txt")

                if not os.path.exists(prompt_file_path):
                    workspace_path = "/workspace/DiaROS/DiaROS_py/diaros/prompts/example_dialog.txt"
                    if os.path.exists(workspace_path):
                        prompt_file_path = workspace_path
                
                try:
                    with open(prompt_file_path, 'r', encoding='utf-8') as f:
                        prompt = f.read()
                    if not prompt.strip():
                        sys.stdout.write(f"[NLG ERROR] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™: {prompt_file_path}\n")
                        sys.stdout.flush()
                        return
                except FileNotFoundError:
                    sys.stdout.write(f"[NLG ERROR] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {prompt_file_path}\n")
                    sys.stdout.flush()
                    return
                except Exception as e:
                    sys.stdout.write(f"[NLG ERROR] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {prompt_file_path} - {e}\n")
                    sys.stdout.flush()
                    return
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆï¼ˆéŸ³å£°èªè­˜çµæœã‚’çµ„ã¿è¾¼ã¿ï¼‰
                full_prompt = f"{prompt}\n\nã¶ã¤åˆ‡ã‚Šã®éŸ³å£°èªè­˜çµæœ: {', '.join(asr_results)}"
                
                # LLMå‘¼ã³å‡ºã—
                llm_start_time = datetime.now()
                sys.stdout.write(f"[{llm_start_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸ¤– {worker_name} Ollamaæ¨è«–é–‹å§‹\n")
                
                # LLMæ¨è«–é–‹å§‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                if self.current_session_id:
                    self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "llm_start", {
                        "model": self.model_name,
                        "worker_name": worker_name,
                        "prompt_type": "asr_dialogue",
                        "prompt_length": len(prompt),
                        "asr_count": len(asr_results)
                    })
                
                # Ollama APIã‚’ç›´æ¥å‘¼ã³å‡ºã—ã¦å¯¾è©±ç”Ÿæˆã¨çµ±è¨ˆæƒ…å ±ã‚’å–å¾—ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ï¼‰
                try:
                    import requests
                    api_response = requests.post('http://localhost:11434/api/generate',
                        json={
                            'model': self.model_name,
                            'prompt': full_prompt,
                            'stream': True,  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–
                            'options': {
                                'temperature': 0.7,
                                'top_p': 0.9,
                                'num_predict': 50,
                                'num_ctx': 4096,
                                'num_batch': 3072
                            }
                        },
                        timeout=30,
                        stream=True  # requestsã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚‚æœ‰åŠ¹åŒ–
                    )

                    if api_response.status_code == 200:
                        res = ""
                        token_count = 0
                        first_token_time = None
                        last_token_time = llm_start_time
                        token_times = []  # å„ãƒˆãƒ¼ã‚¯ãƒ³ã®æ™‚é–“å·®ã‚’è¨˜éŒ²

                        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’é€æ¬¡å‡¦ç†
                        for line in api_response.iter_lines():
                            if line:
                                current_time = datetime.now()
                                try:
                                    chunk_data = json.loads(line)
                                    token_fragment = chunk_data.get('response', '')

                                    if token_fragment:
                                        token_count += 1

                                        # Time to First Token (TTFT) ã‚’è¨ˆæ¸¬
                                        if first_token_time is None:
                                            first_token_time = current_time
                                            ttft_ms = (first_token_time - llm_start_time).total_seconds() * 1000
                                            sys.stdout.write(f"[{current_time.strftime('%H:%M:%S.%f')[:-3]}][NLG TOKEN] ğŸ¯ {worker_name} æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³å—ä¿¡ (TTFT: {ttft_ms:.1f}ms)\n")
                                            sys.stdout.flush()

                                        # Inter-Token Latency (ITL) ã‚’è¨ˆæ¸¬
                                        itl_ms = (current_time - last_token_time).total_seconds() * 1000
                                        token_times.append(itl_ms)

                                        # ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®è©³ç´°ãƒ­ã‚°å‡ºåŠ›
                                        sys.stdout.write(f"[{current_time.strftime('%H:%M:%S.%f')[:-3]}][NLG TOKEN] {worker_name} #{token_count}: '{token_fragment}' (ITL: {itl_ms:.1f}ms)\n")
                                        sys.stdout.flush()

                                        res += token_fragment
                                        last_token_time = current_time

                                    # æœ€çµ‚ãƒãƒ£ãƒ³ã‚¯ã®å‡¦ç†
                                    if chunk_data.get('done', False):
                                        llm_end_time = current_time
                                        llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000

                                        sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âœ… {worker_name} Ollamaæ¨è«–å®Œäº† (LLMæ™‚é–“: {llm_duration:.1f}ms)\n")

                                        # ãƒˆãƒ¼ã‚¯ãƒ³çµ±è¨ˆæƒ…å ±
                                        if token_count > 0 and first_token_time:
                                            avg_itl = sum(token_times) / len(token_times) if token_times else 0
                                            ttft_ms = (first_token_time - llm_start_time).total_seconds() * 1000
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG STATS] ğŸ“Š {worker_name} ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {token_count}\n")
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG STATS] ğŸ“Š {worker_name} TTFT (æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§): {ttft_ms:.1f}ms\n")
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG STATS] ğŸ“Š {worker_name} å¹³å‡ITL (ãƒˆãƒ¼ã‚¯ãƒ³é–“éš”): {avg_itl:.1f}ms\n")

                                        # Verboseçµ±è¨ˆæƒ…å ±ï¼ˆOllamaæä¾›ï¼‰
                                        if 'total_duration' in chunk_data:
                                            total_duration_ms = chunk_data['total_duration'] / 1_000_000
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} ã‹ã‹ã£ãŸæ™‚é–“: {total_duration_ms/1000:.6f}s\n")

                                        if 'load_duration' in chunk_data:
                                            load_duration_ms = chunk_data['load_duration'] / 1_000_000
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ™‚é–“: {load_duration_ms:.3f}ms\n")

                                        if 'prompt_eval_count' in chunk_data and 'prompt_eval_duration' in chunk_data:
                                            prompt_tokens = chunk_data['prompt_eval_count']
                                            prompt_eval_ms = chunk_data['prompt_eval_duration'] / 1_000_000
                                            tokens_per_sec = prompt_tokens / (prompt_eval_ms / 1000) if prompt_eval_ms > 0 else 0
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {prompt_tokens} token(s)\n")
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å‡¦ç†æ™‚é–“: {prompt_eval_ms:.3f}ms\n")
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å‡¦ç†ãƒˆãƒ¼ã‚¯ãƒ³/s: {tokens_per_sec:.2f} tokens/s\n")

                                        if 'eval_count' in chunk_data and 'eval_duration' in chunk_data:
                                            output_tokens = chunk_data['eval_count']
                                            eval_ms = chunk_data['eval_duration'] / 1_000_000
                                            output_tokens_per_sec = output_tokens / (eval_ms / 1000) if eval_ms > 0 else 0
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {output_tokens} token(s)\n")
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã«ã‹ã‹ã£ãŸæ™‚é–“: {eval_ms/1000:.6f}s\n")
                                            sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã®å‡¦ç†ãƒˆãƒ¼ã‚¯ãƒ³/s: {output_tokens_per_sec:.2f} tokens/s\n")

                                        sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} ç”Ÿæˆå¿œç­”: '{res}'\n")
                                        sys.stdout.flush()
                                        break

                                except json.JSONDecodeError:
                                    continue

                        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãŒå®Œäº†ã—ã¦ã‚‚ llm_end_time ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆ
                        if 'llm_end_time' not in locals():
                            llm_end_time = datetime.now()
                            llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                    else:
                        # APIå‘¼ã³å‡ºã—å¤±æ•—æ™‚ã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®š
                        res = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
                        llm_end_time = datetime.now()
                        llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                        sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âŒ {worker_name} Ollama APIå‘¼ã³å‡ºã—å¤±æ•— (status: {api_response.status_code})\n")
                        sys.stdout.flush()

                except Exception as api_error:
                    # APIå‘¼ã³å‡ºã—å¤±æ•—æ™‚ã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®š
                    res = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
                    llm_end_time = datetime.now()
                    llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âŒ {worker_name} Ollama APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {api_error}\n")
                    sys.stdout.flush()
                
                # LLMæ¨è«–å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                if self.current_session_id:
                    self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "llm_complete", {
                        "model": self.model_name,
                        "worker_name": worker_name,
                        "llm_duration_ms": llm_duration,
                        "response_length": len(res)
                    })
                
                source_words = asr_results
                
            else:
                if not query or (isinstance(query, list) and all((not x or x.strip() == "") for x in query)):
                    self.last_reply = ""
                    self.last_source_words = []
                    return
                elif query == "dummy":
                    res = "ã¯ã„"
                    source_words = [str(query)]
                else:
                    text_input = query
                    role = "å„ªã—ã„æ€§æ ¼ã®ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰ã¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè©±ã«å¯¾ã—ã¦ç›¸æ‰‹ã‚’åŠ´ã‚‹ã‚ˆã†ãªè¿”ç­”ã®ã¿ã‚’ï¼’ï¼æ–‡å­—ä»¥å†…ã§ã—ã¦ãã ã•ã„ã€‚"

                    llm_start_time = datetime.now()

                    if self.model_name.startswith("gemma3:") or self.model_name.startswith("gpt-oss:"):
                        # Ollama gemma3ç³»ãƒ¢ãƒ‡ãƒ«ï¼ˆLangChainçµŒç”±ï¼‰
                        messages = [
                            ("system", role),
                            ("human", text_input)
                        ]
                        query_prompt = ChatPromptTemplate.from_messages(messages)
                        chain = query_prompt | self.ollama_model | StrOutputParser()
                        res = chain.invoke({})

                    elif self.model_name.startswith("gpt-") or self.model_name.startswith("o1"):
                        # OpenAI APIï¼ˆGPT-5, GPT-4, o1ãªã©ï¼‰
                        messages = [
                            {"role": "system", "content": role},
                            {"role": "user", "content": text_input}
                        ]

                        # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ã®æœ€é©åŒ–è¨­å®š
                        if self.model_name.startswith("gpt-4.1"):
                            # GPT-4.1ç³»: æœ€é€Ÿ540ms
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_completion_tokens=50,
                                temperature=0.3
                            )
                        elif "chat-latest" in self.model_name:
                            # GPT-5-chat-latest: æ¨è«–æœ€å°åŒ–ç‰ˆ
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_completion_tokens=50
                            )
                        elif self.model_name.startswith("gpt-5") or self.model_name.startswith("o1"):
                            # GPT-5/o1: æ¨è«–ãƒ¢ãƒ‡ãƒ«
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_completion_tokens=500,
                                reasoning_effort="low"
                            )
                        else:
                            # GPT-4oç³»: æ¨™æº–
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_tokens=50,
                                temperature=0.3
                            )

                        res = response.choices[0].message.content.strip()

                    llm_end_time = datetime.now()
                    llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âœ… {worker_name} {self.model_name}æ¨è«–å®Œäº† (LLMæ™‚é–“: {llm_duration:.1f}ms)\n")
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] {worker_name} ç”Ÿæˆå¿œç­”: '{res}'\n")
                    sys.stdout.flush()

                    if ":" in res:
                        res = res.split(":", 1)[1]

                    source_words = [str(query)]
            
            # æ”¹è¡Œã‚’é™¤å»ã—ã¦1è¡Œã«ã™ã‚‹
            res = res.replace('\n', '').replace('\r', '')
            
            end_time = datetime.now()
            total_duration = (end_time - start_time).total_seconds() * 1000
            
            # çµæœã‚’è¨­å®š
            self.last_reply = res
            self.last_source_words = source_words
            
            # ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±ã‚’è¨­å®š
            self.request_id = request_id
            self.worker_name = worker_name
            self.start_timestamp_ns = int(start_time.timestamp() * 1_000_000_000)
            self.completion_timestamp_ns = int(end_time.timestamp() * 1_000_000_000)
            self.inference_duration_ms = total_duration
            
            # æˆåŠŸæ™‚ã¯æ¥ç¶šã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
            self.connection_error_count = 0
            self.connection_error_suppress_until = None
            
            # æ¨è«–å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
            if self.current_session_id:
                self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "parallel_inference_complete", {
                    "worker_name": worker_name,
                    "request_id": request_id,
                    "total_duration_ms": total_duration,
                    "response": res,
                    "source_words": source_words
                })
            
            sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸ {worker_name} å‡¦ç†å®Œäº† (ç·æ™‚é–“: {total_duration:.1f}ms): {res}\n")
            sys.stdout.flush()
            
        except Exception as e:
            end_time = datetime.now()
            
            # æ¥ç¶šã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†
            error_str = str(e)
            is_connection_error = (
                "[Errno 111] Connection refused" in error_str or
                "llama runner process has terminated" in error_str or
                "broken pipe" in error_str or
                "status code: 500" in error_str
            )
            
            if is_connection_error:
                self.connection_error_count += 1
                # é€£ç¶šæ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒ5å›ä»¥ä¸Šãªã‚‰30ç§’é–“ãƒªã‚¯ã‚¨ã‚¹ãƒˆæŠ‘åˆ¶
                if self.connection_error_count >= 5:
                    self.connection_error_suppress_until = end_time + timedelta(seconds=30)
                    if self.connection_error_count == 5:  # åˆå›æŠ‘åˆ¶æ™‚ã®ã¿ãƒ­ã‚°å‡ºåŠ›
                        sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG WARNING] ğŸš« {worker_name} é€£ç¶šæ¥ç¶šã‚¨ãƒ©ãƒ¼æ¤œå‡ºã€‚30ç§’é–“ãƒªã‚¯ã‚¨ã‚¹ãƒˆæŠ‘åˆ¶ã—ã¾ã™\n")
                        sys.stdout.flush()
                # æ¥ç¶šã‚¨ãƒ©ãƒ¼ã¯è©³ç´°ãƒ­ã‚°ã‚’æŠ‘åˆ¶
                if self.connection_error_count <= 3:  # æœ€åˆã®3å›ã®ã¿ãƒ­ã‚°å‡ºåŠ›
                    sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG ERROR] âŒ {worker_name} æ¥ç¶šã‚¨ãƒ©ãƒ¼: Ollamaæ¥ç¶šå¤±æ•—\n")
                    sys.stdout.flush()
            else:
                # æ¥ç¶šã‚¨ãƒ©ãƒ¼ä»¥å¤–ã®å ´åˆã¯ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆ
                self.connection_error_count = 0
                self.connection_error_suppress_until = None
                sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG ERROR] âŒ {worker_name} æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}\n")
                sys.stdout.flush()
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ï¼ˆå›ºå®šå¿œç­”ã®ã¿ï¼‰
            if is_connection_error:
                # Ollamaã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç°¡å˜ãªå›ºå®šå¿œç­”ã®ã¿
                fallback_responses = [
                    "ãã†ã§ã™ã­ã€‚",
                    "ãªã‚‹ã»ã©ã€‚", 
                    "ã‚ã‹ã‚Šã¾ã—ãŸã€‚",
                    "ã¯ã„ã€‚",
                    "ãã†ãªã‚“ã§ã™ã­ã€‚"
                ]
                import random
                fallback_response = random.choice(fallback_responses)
                
                self.last_reply = fallback_response
                self.last_source_words = query if isinstance(query, list) else [str(query)]
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±è¨­å®š
                self.request_id = 999  # å›ºå®šå¿œç­”ID
                self.worker_name = f"static-fallback-{worker_name}"
                self.start_timestamp_ns = int(start_time.timestamp() * 1_000_000_000)
                self.completion_timestamp_ns = int(end_time.timestamp() * 1_000_000_000)
                self.inference_duration_ms = (end_time - start_time).total_seconds() * 1000
                
                if self.connection_error_count <= 3:
                    sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG FALLBACK] ğŸ”„ {worker_name} å›ºå®šå¿œç­”ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {fallback_response}\n")
                    sys.stdout.flush()
            else:
                # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®çµæœã‚’è¨­å®š
                self.last_reply = ""
                self.last_source_words = []
    """

    def run(self):
        sys.stdout.write("[NLG] å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹æ¨è«–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹ (2.5ç§’é–“éš”åˆ¶å¾¡)\n")
        sys.stdout.write(f"[NLG] ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.model_name}\n")
        sys.stdout.flush()
        
>>>>>>> 5d1bb974d10e290d00ef142d14ca452a728f451a
        # ä¸¦åˆ—å‡¦ç†ç‰ˆã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
        # while True:
        #     # ä¸¦åˆ—æ¨è«–ã‚·ã‚¹ãƒ†ãƒ ã§ã¯çµæœç›£è¦–ã®ã¿
        #     try:
        #         # å®Œäº†ã—ãŸæ¨è«–çµæœãŒã‚ã‚Œã°å‡¦ç†
        #         if not self.result_queue.empty():
        #             result = self.result_queue.get_nowait()
        #             # çµæœå‡¦ç†ã¯ãƒ¯ãƒ¼ã‚«ãƒ¼å†…ã§å®Œçµã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ç‰¹ã«å‡¦ç†ãªã—
        #     except:
        #         pass
        #     
        #     time.sleep(0.01)  # 10mså¾…æ©Ÿ
        
        # å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹ç‰ˆã§ã¯ç‰¹ã«ç„¡é™ãƒ«ãƒ¼ãƒ—ã¯ä¸è¦
        pass

if __name__ == "__main__":
    gen = NaturalLanguageGeneration()
    gen.run()