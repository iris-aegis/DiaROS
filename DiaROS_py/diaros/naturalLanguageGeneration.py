# ============================================================
# ãƒ¢ãƒ‡ãƒ«è¨­å®š - ã“ã“ã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆ
# ============================================================
# ã€OpenAI API ãƒ¢ãƒ‡ãƒ«ã€‘ã‚¯ãƒ©ã‚¦ãƒ‰APIã€é«˜é€Ÿãƒ»é«˜å“è³ª
# MODEL_NAME = "gpt-3.5-turbo-0125"    # 587ms - æœ€é€Ÿãƒ»æœ€å®‰ãƒ»å®‰å®šï¼ˆæ¨å¥¨ï¼‰
MODEL_NAME = "gpt-4.1-nano"          # 604ms - æœ€æ–°æŠ€è¡“ãƒ»é«˜é€Ÿ
# MODEL_NAME = "gpt-5-chat-latest"     # 708ms - GPT-5æœ€é€Ÿç‰ˆãƒ»å®‰å®š
# MODEL_NAME = "gpt-oss:20b"
# ã€Ollama ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã€‘ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å‹•ä½œã€GPUå¿…è¦
<<<<<<< HEAD
MODEL_NAME = "gemma3:4b"             
# MODEL_NAME = "gemma3:12b"            
# MODEL_NAME = "gemma3:27b"            
=======
# MODEL_NAME = "gemma3:4b"
# MODEL_NAME = "gemma3:12b"
# MODEL_NAME = "gemma3:27b"
>>>>>>> 55154d4 (Fix: NLGã‚’GPT-4.1ã§1-shotä¾‹ç¤ºã«ã‚ˆã‚‹1å›å¿œç­”ã«å¤‰æ›´)

# ============================================================
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«åã®è¨­å®š - ã“ã“ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ‡ã‚Šæ›¿ãˆ
# ============================================================
# ã€å¯¾è©±ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‘éŸ³å£°èªè­˜çµæœã‹ã‚‰å¯¾è©±å¿œç­”ã‚’ç”Ÿæˆ
# PROMPT_FILE_NAME = "dialog_simple.txt"       # ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼ˆãƒã‚¤ã‚ºã‚¿ã‚°è‡ªå‹•é™¤å»ï¼‰
# PROMPT_FILE_NAME = "dialog_predict.txt"      # ç™ºè©±äºˆæ¸¬ä»˜ãï¼ˆãƒã‚¤ã‚ºã‚¿ã‚°è‡ªå‹•é™¤å»ï¼‰
# PROMPT_FILE_NAME = "dialog_tag.txt"          # ã‚¿ã‚°å‡¦ç†ä»˜ã
# PROMPT_FILE_NAME = "dialog_tag_ver2.txt"          # ã‚¿ã‚°å‡¦ç†ä»˜ã
# PROMPT_FILE_NAME = "dialog_explain.txt"      # è©³ç´°èª¬æ˜ä»˜ãï¼ˆãƒã‚¤ã‚ºã‚¿ã‚°è‡ªå‹•é™¤å»ï¼‰
# PROMPT_FILE_NAME = "dialog_example.txt"      # ä¾‹ç¤ºä»˜ãï¼ˆãƒã‚¤ã‚ºã‚¿ã‚°è‡ªå‹•é™¤å»ï¼‰
PROMPT_FILE_NAME = "dialog_example_role.txt"      # ä¾‹ç¤ºä»˜ãï¼ˆãƒã‚¤ã‚ºã‚¿ã‚°è‡ªå‹•é™¤å»ï¼‰
# PROMPT_FILE_NAME = "dialog_all.txt"          # å…¨æ©Ÿèƒ½ç‰ˆ
# PROMPT_FILE_NAME = "dialog_all_1115.txt"          # å…¨æ©Ÿèƒ½ç‰ˆ

# PROMPT_FILE_NAME = "dialog_phone.txt"        # é›»è©±å¯¾è©±ç”¨

# ã€éŸ³å£°èªè­˜çµæœã®è£œæ­£ãƒ»è£œå®Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‘éŸ³å£°èªè­˜çµæœã®ä¿®æ­£ã®ã¿
# PROMPT_FILE_NAME = "fix_asr_simple.txt"      # ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼ˆãƒã‚¤ã‚ºã‚¿ã‚°è‡ªå‹•é™¤å»ï¼‰
# PROMPT_FILE_NAME = "fix_asr.txt"             # æ¨™æº–ç‰ˆ
# PROMPT_FILE_NAME = "fix_asr_example.txt"     # ä¾‹ç¤ºä»˜ã
# PROMPT_FILE_NAME = "fix_asr_all.txt"     #
# PROMPT_FILE_NAME = "fix_asr_explain_fixed.txt"     #
# PROMPT_FILE_NAME = "fix_asr_predict.txt"     #
# PROMPT_FILE_NAME = "remdis_test_prompt.txt"     #
<<<<<<< HEAD
# PROMPT_FILE_NAME = "dialog_first_stage.txt"     # 200msä»¥å†…é”æˆç”¨ï¼ˆçŸ­ã„ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ã®ã¿ï¼‰
=======
PROMPT_FILE_NAME = "dialog_example_role.txt"     # 1-shotä¾‹ç¤ºä»˜ãï¼ˆGPT-4.1æ¨å¥¨ï¼‰
>>>>>>> 55154d4 (Fix: NLGã‚’GPT-4.1ã§1-shotä¾‹ç¤ºã«ã‚ˆã‚‹1å›å¿œç­”ã«å¤‰æ›´)

# ã€ã‚¿ã‚¤ãƒŸãƒ³ã‚°èª¿æ•´ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‘
# PROMPT_FILE_NAME = "example_make_delay.txt"  # é…å»¶ç”Ÿæˆç”¨
# PROMPT_FILE_NAME = "WebRTCVAD_timing_example.txt"   # WebRTCVADã‚¿ã‚¤ãƒŸãƒ³ã‚°ä¾‹
# PROMPT_FILE_NAME = "powerbase_timing_example.txt"   # ãƒ‘ãƒ¯ãƒ¼ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¤ãƒŸãƒ³ã‚°ä¾‹

# ã€ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‘
# PROMPT_FILE_NAME = "remdis_test.txt"         # ãƒ†ã‚¹ãƒˆç”¨ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰

# ============================================================

import requests
import json
import sys
import os
import time
import threading
from datetime import datetime, timedelta
from queue import Queue, Empty
from concurrent.futures import ThreadPoolExecutor
import openai
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import ChatOllama
from .timeTracker import get_time_tracker

class NaturalLanguageGeneration:
    def __init__(self, dm_ref=None, rnlg_ref=None):
        self.rc = { "word": "" }
        # â˜…DMå‚ç…§ã‚’ä¿æŒï¼ˆ2.5ç§’é–“éš”ASRå±¥æ­´ã‚’å–å¾—ã™ã‚‹ãŸã‚ï¼‰
        self.dm_ref = dm_ref
        # â˜…ROS2NLGå‚ç…§ã‚’ä¿æŒï¼ˆROS2ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰å—ã‘å–ã£ãŸ2.5ç§’é–“éš”ASRå±¥æ­´ã‚’å–å¾—ã™ã‚‹ãŸã‚ï¼‰
        self.rnlg_ref = rnlg_ref

        self.query = ""
        self.update_flag = False
        self.user_speak_is_final = False
        self.last_reply = ""  # ç”Ÿæˆã—ãŸå¯¾è©±æ–‡ã‚’ã“ã“ã«æ ¼ç´
        self.last_source_words = []  # å¯¾è©±ç”Ÿæˆã®å…ƒã«ã—ãŸéŸ³å£°èªè­˜çµæœã‚’æ ¼ç´

        # äºŒæ®µéšå¿œç­”ç”Ÿæˆç”¨ã®å¤‰æ•°
        self.first_stage_response = ""  # first_stageã§ç”Ÿæˆã—ãŸãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ã‚’ä¿å­˜
        self.current_stage = "first"  # first ã¾ãŸã¯ secondï¼ˆDMã‹ã‚‰ã®stageæŒ‡å®šã§åˆ‡ã‚Šæ›¿ã‚ã‚‹ï¼‰
        self.turn_taking_decision_timestamp_ns = 0  # TurnTakingåˆ¤å®šæ™‚åˆ»ï¼ˆãƒŠãƒç§’ï¼‰
        self.first_stage_response_cached = ""  # first_stageãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.asr_history_2_5s = []  # 2.5ç§’é–“éš”ã®ASRçµæœãƒªã‚¹ãƒˆï¼ˆSecond stageç”Ÿæˆç”¨ï¼‰

        # Second stage å‡¦ç†ä¸­ã® first stage ãƒªã‚¯ã‚¨ã‚¹ãƒˆç®¡ç†
        self.is_generating_second_stage = False  # Second stage ç”Ÿæˆä¸­ãƒ•ãƒ©ã‚°
        self.pending_first_stage_request = None  # ä¿ç•™ä¸­ã® first_stage ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆæœ€æ–°ã®ã¿ä¿æŒï¼‰

        # ROS2 bagè¨˜éŒ²ç”¨ã®è¿½åŠ æƒ…å ±
        self.last_request_id = 0
        self.last_worker_name = ""
        self.last_start_timestamp_ns = 0
        self.last_completion_timestamp_ns = 0
        self.last_inference_duration_ms = 0.0
        
        # æ–°ã—ã„æ™‚åˆ»æƒ…å ±ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åˆæœŸåŒ–
        self.request_id = 0
        self.worker_name = ""
        self.start_timestamp_ns = 0
        self.completion_timestamp_ns = 0
        self.inference_duration_ms = 0.0
        
        # æ¥ç¶šã‚¨ãƒ©ãƒ¼åˆ¶å¾¡ç”¨
        self.connection_error_count = 0
        self.last_connection_error_time = None
        self.connection_error_suppress_until = None
        
        # ã‚¿ã‚¤ãƒ ãƒˆãƒ©ãƒƒã‚«ãƒ¼åˆæœŸåŒ–
        self.time_tracker = get_time_tracker("nlg_pc")
        self.current_session_id = None
        
        # ä¸¦åˆ—å‡¦ç†è¨­å®šã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼ˆå˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹ç‰ˆï¼‰
        # self.inference_queue = Queue()  # æ¨è«–ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚­ãƒ¥ãƒ¼
        # self.result_queue = Queue()     # æ¨è«–çµæœã®ã‚­ãƒ¥ãƒ¼
        # self.request_counter = 0        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        self.last_request_time = None   # æœ€å¾Œã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚åˆ»
        self.last_inference_time = None # æœ€å¾Œã®æ¨è«–å®Ÿè¡Œæ™‚åˆ»
        self.inference_interval = 2.5   # æ¨è«–é–“éš”ï¼ˆç§’ï¼‰
        # self.executor = ThreadPoolExecutor(max_workers=3, thread_name_prefix="NLG-Worker")
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ä¸Šéƒ¨ã®MODEL_NAMEã‚’ä½¿ç”¨ï¼‰
        self.model_name = MODEL_NAME

        if self.model_name.startswith("gemma3:") or self.model_name.startswith("gpt-oss:"):
            # Ollama ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆgemma3ç³»ã€gpt-ossç³»ï¼‰
            sys.stdout.write(f'[NLG] Ollama {self.model_name}ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...\n')
            sys.stdout.flush()

            # gpt-ossã¯æ¨è«–ãƒ¢ãƒ‡ãƒ«ãªã®ã§éå¸¸ã«å¤§ããªãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå¿…è¦
            if self.model_name.startswith("gpt-oss:"):
                num_predict = 2000  # æ¨è«–ãƒˆãƒ¼ã‚¯ãƒ³ + å¿œç­”ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆè¤‡é›‘ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯¾å¿œï¼‰
                sys.stdout.write(f'[NLG] âš ï¸  gpt-oss:20bã¯æ¨è«–ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã€å¿œç­”ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ (num_predict={num_predict})\n')
                sys.stdout.flush()
            else:
                num_predict = 10  # gemma3ç³»ã¯10ãƒˆãƒ¼ã‚¯ãƒ³ã§çµ±ä¸€ï¼ˆçŸ­ã„ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ç”¨ï¼‰

            # gpt-oss:20bã®é«˜é€ŸåŒ–è¨­å®šï¼ˆæ¨è«–ã‚’æœ€å°é™ã«ï¼‰
            if self.model_name.startswith("gpt-oss:"):
                temperature = 0.3  # ã‚ˆã‚Šæ±ºå®šçš„ã«ï¼ˆæ¨è«–ã‚’æ¸›ã‚‰ã™ï¼‰
                additional_kwargs = {
                    "verbose": True,
                    "num_ctx": 4096,
                    "num_batch": 3072,
                    "think": "low"  # æ¨è«–ãƒ¢ãƒ¼ãƒ‰æœ€å°åŒ–ï¼ˆé«˜é€ŸåŒ–ï¼‰
                }
            else:
                temperature = 0.7  # gemma3ç³»ã¯æ¨™æº–è¨­å®š
                additional_kwargs = {
                    "verbose": True,
                    "num_ctx": 4096,
                    "num_batch": 3072
                }

            self.ollama_model = ChatOllama(
                model=self.model_name,
                verbose=True,  # è©³ç´°ãƒ­ã‚°æœ‰åŠ¹åŒ–
                temperature=temperature,  # å¿œç­”ã®å¤šæ§˜æ€§
                top_p=0.9,  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¨­å®š
                num_predict=num_predict,  # æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°
                keep_alive="10m",  # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒã™ã‚‹æ™‚é–“ï¼ˆå»¶é•·ï¼‰
                additional_kwargs=additional_kwargs
            )
            sys.stdout.write(f'[NLG] âœ… {self.model_name}ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº† (num_predict={num_predict})\n')
            sys.stdout.flush()

        elif self.model_name.startswith("gpt-") or self.model_name.startswith("o1") or self.model_name.startswith("chatgpt-"):
            # OpenAI APIè¨­å®šï¼ˆGPT-5, GPT-4, GPT-3.5, o1ãªã©å…¨ã¦ã®OpenAIãƒ¢ãƒ‡ãƒ«ï¼‰
            sys.stdout.write(f'[NLG] OpenAI {self.model_name}ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...\n')
            sys.stdout.flush()

            # OpenAI APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®š
            openai.api_key = os.environ.get("OPENAI_API_KEY")
            if not openai.api_key:
                sys.stdout.write('[NLG ERROR] OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“\n')
                sys.stdout.flush()
                raise ValueError("OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            else:
                sys.stdout.write(f'[NLG] âœ… {self.model_name}ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†\n')
                sys.stdout.flush()

        else:
            raise ValueError(f"æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«: {self.model_name}")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèªï¼ˆè¤‡æ•°ãƒ‘ã‚¹ã®è©¦è¡Œï¼‰
        self.prompt_file_name = PROMPT_FILE_NAME

        # è¤‡æ•°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‘ã‚¹ã‚’è©¦è¡Œï¼ˆå„ªå…ˆé †ï¼‰
        possible_paths = [
            # 1. é–‹ç™ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨ï¼‰
            os.path.join(os.path.dirname(__file__), 'prompts', self.prompt_file_name),
            # 2. site-packagesã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ï¼ˆpip installå®Ÿè¡Œå¾Œï¼‰
            os.path.join(os.path.dirname(__file__), 'prompts', self.prompt_file_name),
            # 3. ç’°å¢ƒå¤‰æ•°ã§æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            os.path.join(os.environ.get('DIAROS_PROMPTS_DIR', ''), self.prompt_file_name) if os.environ.get('DIAROS_PROMPTS_DIR') else None,
        ]

        self.prompt_file_path = None
        for path in possible_paths:
            if path and os.path.exists(path):
                self.prompt_file_path = path
                sys.stdout.write(f'[NLG] âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª: {self.prompt_file_name} ({path})\n')
                sys.stdout.flush()
                break

        if not self.prompt_file_path:
            sys.stdout.write(f'[NLG WARNING] âš ï¸  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.prompt_file_name}\n')
            sys.stdout.write(f'[NLG WARNING]    è©¦è¡Œãƒ‘ã‚¹: {possible_paths}\n')
            sys.stdout.flush()
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ã‚’è¨­å®šï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãªãã¦ã‚‚ç¶šè¡Œï¼‰
            self.prompt_file_path = os.path.join(os.path.dirname(__file__), 'prompts', self.prompt_file_name)

        sys.stdout.write('NaturalLanguageGeneration (å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹) start up.\n')
        sys.stdout.write(f'ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.model_name}\n')
        sys.stdout.write('=====================================================\n')

    def update(self, words, stage='first', turn_taking_decision_timestamp_ns=0, first_stage_backchannel_at_tt=None, asr_history_2_5s=None):
        """
        ãƒ¡ã‚¤ãƒ³PCã‹ã‚‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†
        words: éŸ³å£°èªè­˜çµæœã®ãƒªã‚¹ãƒˆ
        stage: 'first' ã¾ãŸã¯ 'second'
        turn_taking_decision_timestamp_ns: TurnTakingåˆ¤å®šæ™‚åˆ»ï¼ˆãƒŠãƒç§’ï¼‰
        first_stage_backchannel_at_tt: TurnTakingåˆ¤å®šæ™‚ã«å†ç”Ÿäºˆå®šã®ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰å†…å®¹ï¼ˆSecond stageç”¨ï¼‰
        asr_history_2_5s: 2.5ç§’é–“éš”ã®ASRçµæœãƒªã‚¹ãƒˆï¼ˆSecond stageç”Ÿæˆç”¨ï¼‰
        """
        now = datetime.now()

        # æ¥ç¶šã‚¨ãƒ©ãƒ¼æŠ‘åˆ¶ä¸­ã¯æ–°ã—ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ã‘ä»˜ã‘ãªã„
        if self.connection_error_suppress_until and now < self.connection_error_suppress_until:
            return

        # â˜…ã€é‡è¦ã€‘Second stage ç”Ÿæˆä¸­ã« first_stage ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒæ¥ãŸå ´åˆã¯ä¿ç•™
        if self.is_generating_second_stage and stage == 'first':
            # Second stage ç”Ÿæˆä¸­ã® first_stage ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä¿ç•™
            timestamp = now.strftime('%H:%M:%S.%f')[:-3]
            sys.stdout.write(f"[{timestamp}] â¸ï¸  Second stage ç”Ÿæˆä¸­ã®ãŸã‚ã€first_stage ã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚­ãƒ¥ãƒ¼ã«ä¿å­˜\n")
            sys.stdout.flush()

            # æœ€æ–°ã® first_stage ãƒªã‚¯ã‚¨ã‚¹ãƒˆã ã‘ã‚’ä¿æŒï¼ˆä¸Šæ›¸ãï¼‰
            self.pending_first_stage_request = {
                'words': words,
                'turn_taking_decision_timestamp_ns': turn_taking_decision_timestamp_ns,
                'first_stage_backchannel_at_tt': first_stage_backchannel_at_tt,
                'asr_history_2_5s': asr_history_2_5s
            }
            return

        # â˜…stageæƒ…å ±ã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä¿å­˜
        self.current_stage = stage
        self.turn_taking_decision_timestamp_ns = turn_taking_decision_timestamp_ns
        # â˜…TTåˆ¤å®šæ™‚ã®ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ã‚’ä¿å­˜ï¼ˆSecond stageç”¨ï¼‰
        # â˜…ä¿®æ­£ï¼šç©ºæ–‡å­—åˆ—ã‚‚å«ã‚ã¦å¸¸ã«æ›´æ–°ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ¸¡ã•ã‚ŒãŸå ´åˆï¼‰
        if first_stage_backchannel_at_tt is not None:
            self.first_stage_response = first_stage_backchannel_at_tt
        # â˜…2.5ç§’é–“éš”ASRçµæœã‚’ä¿å­˜ï¼ˆSecond stageç”¨ï¼‰
        # â˜…ä¿®æ­£ï¼šç©ºãƒªã‚¹ãƒˆã‚‚å«ã‚ã¦å¸¸ã«æ›´æ–°ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ¸¡ã•ã‚ŒãŸå ´åˆï¼‰
        if asr_history_2_5s is not None:
            self.asr_history_2_5s = asr_history_2_5s

        # â˜…æ€§èƒ½ç›£è¦–: å¤§é‡å±¥æ­´ã®å—ä¿¡ã‚’è¨˜éŒ²
        word_count = len(words) if isinstance(words, list) else 1
        timestamp = now.strftime('%H:%M:%S.%f')[:-3]

        # â˜…ãƒ­ã‚°å‡ºåŠ›ã‚’ç°¡ç•¥åŒ–ï¼š[HH:MM:SS.mmm] å½¢å¼ã«çµ±ä¸€
        sys.stdout.write(f"[{timestamp}] stage='{stage}' ã§æ›´æ–°\n")
        sys.stdout.flush()

        if word_count > 20:
            sys.stdout.write(f"[{timestamp}] å¤§å®¹é‡å±¥æ­´å—ä¿¡: {word_count}å€‹\n")
            sys.stdout.flush()

        # æœ€åˆã®3å€‹ã¨æœ€å¾Œã®3å€‹ã®ã¿ã‚’è¡¨ç¤ºï¼ˆä¸­é–“ã¯çœç•¥ï¼‰
        if isinstance(words, list):
            if word_count > 6:
                preview_words = words[:3] + ["..."] + words[-3:]
                sys.stdout.write(f"[{timestamp}] å±¥æ­´å—ä¿¡ï¼ˆ{word_count}å€‹ï¼‰\n")
            elif word_count > 0:
                sys.stdout.write(f"[{timestamp}] å±¥æ­´å—ä¿¡ï¼ˆ{word_count}å€‹ï¼‰\n")
            sys.stdout.flush()

        query = words

        # éŸ³å£°èªè­˜çµæœãŒãƒªã‚¹ãƒˆã®å ´åˆã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸ‹ã‚è¾¼ã‚€
        self.asr_results = None
        # â˜…Second stageã®å ´åˆã¯ç©ºã®queryã‚’è¨±å®¹ï¼ˆfirst_stage_responseã‹ã‚‰ç¶šãã‚’ç”Ÿæˆï¼‰
        # First stageã®å ´åˆã¯ç©ºãƒªã‚¹ãƒˆã¾ãŸã¯å…¨ã¦ç©ºæ–‡å­—åˆ—ãªã‚‰ä½•ã‚‚ã—ãªã„
        if isinstance(query, list):
            if not query or all((not x or x.strip() == "") for x in query):
                # â˜…Second stageã®å ´åˆã¯å‡¦ç†ã‚’ç¶šã‘ã‚‹ï¼ˆfirst_stage_responseã‚’ä½¿ç”¨ï¼‰
                if self.current_stage != 'second':
                    self.update_flag = False
                    return
                # Second stageã®å ´åˆã¯ç©ºã®queryã§ã‚‚å‡¦ç†ã‚’ç¶šã‘ã‚‹
            self.asr_results = query if query else []
            self.query = query
        else:
            if not query or (isinstance(query, str) and query.strip() == ""):
                # â˜…Second stageã®å ´åˆã¯å‡¦ç†ã‚’ç¶šã‘ã‚‹
                if self.current_stage != 'second':
                    self.update_flag = False
                    return
            self.query = query
            self.asr_results = None

        # 2.5ç§’é–“éš”åˆ¶å¾¡ã‚’å‰Šé™¤: éŸ³å£°èªè­˜çµæœãŒæ¥ã‚‹ãŸã³ã«ã™ãå¿œç­”ç”Ÿæˆ
        # (ä»¥å‰ã®é–“éš”åˆ¶å¾¡ã‚³ãƒ¼ãƒ‰ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ)

        # å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹æ¨è«–ã«å¤‰æ›´ï¼ˆä¸¦åˆ—å‡¦ç†ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
        # self.request_counter += 1
        # request_id = self.request_counter
        request_id = 1  # å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹ã§ã¯å›ºå®šID

        # â˜…ãƒ­ã‚°å½¢å¼ã‚’çµ±ä¸€ï¼š[HH:MM:SS.mmm] ã®ã¿è¡¨ç¤º
        # sys.stdout.write(f"[{now.strftime('%H:%M:%S.%f')[:-3]}] ğŸš€ æ¨è«–é–‹å§‹\n")
        # sys.stdout.flush()

        # â˜…ã‚¹ãƒ†ãƒ¼ã‚¸ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé¸æŠã¨æ¨è«–å®Ÿè¡Œ
        # Stage ã”ã¨ã«ç•°ãªã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ã„åˆ†ã‘ã¦å®Ÿè¡Œï¼ˆåŒæœŸå‡¦ç†ï¼‰
        if self.current_stage == 'first':
            # First stage: dialog_first_stage.txt ã§ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ
            # â˜…ãƒ­ã‚°å‡ºåŠ›ã‚’å‰Šé™¤ï¼ˆç°¡ç•¥åŒ–ï¼‰
            self.generate_first_stage(query)
        elif self.current_stage == 'second':
            # Second stage: dialog_second_stage.txt ã§æœ¬å¿œç­”ç”Ÿæˆ
            # â˜…ãƒ­ã‚°å‡ºåŠ›ã‚’å‰Šé™¤ï¼ˆç°¡ç•¥åŒ–ï¼‰
            self.generate_second_stage(query)
        else:
            # ãã®ä»–: å¾“æ¥ã® _perform_simple_inference()
            self._perform_simple_inference(query)

        # æœ€å¾Œã®æ¨è«–æ™‚åˆ»ã‚’æ›´æ–°
        self.last_inference_time = now
        self.last_request_time = now
        self.update_flag = True
        
    def set_session_id(self, session_id: str):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’è¨­å®š"""
        self.current_session_id = session_id

    def generate_first_stage(self, query):
        """First stage: ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆï¼ˆdialog_first_stage.txt + humanã‚¿ã‚°ã§ASRçµæœã‚’åˆ¥å£å…¥åŠ›ï¼‰"""
        start_time = datetime.now()

        try:
            asr_results = query if isinstance(query, list) else [str(query)]

            # â˜…ä¿®æ­£ï¼šéŸ³å£°èªè­˜çµæœãŒç©ºã®å ´åˆã¯ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆã‚’è¡Œã‚ãªã„
            if not asr_results or all((not x or x.strip() == "") for x in asr_results):
                self.first_stage_response = ""
                timestamp = start_time.strftime('%H:%M:%S.%f')[:-3]
                sys.stdout.write(f"[{timestamp}] First stage: ASRçµæœãŒç©ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—\n")
                sys.stdout.flush()
                return

            # â˜…ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            prompt_build_start = datetime.now()
            try:
                prompt_text = self._load_first_stage_prompt()
            except FileNotFoundError as e:
                sys.stdout.write(f"[NLG ERROR] first_stageãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}\n")
                sys.stdout.flush()
                self.first_stage_response = "ã†ã‚“"
                return

            prompt_build_end = datetime.now()
            # â˜…ãƒ­ã‚°å‡ºåŠ›ã‚’ç°¡ç•¥åŒ–ï¼šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿ãƒ­ã‚°ã‚’å‰Šé™¤

            # LLMå‘¼ã³å‡ºã—
            llm_start_time = datetime.now()

            try:
                if self.model_name.startswith("gemma3:") or self.model_name.startswith("gpt-oss:"):
                    # â˜…Ollama API /api/chat ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆhumanã‚¿ã‚°å½¢å¼ï¼‰
                    import requests

                    api_start = datetime.now()

                    # humanã‚¿ã‚°ã§åˆ¥å£å…¥åŠ›: system (ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ) + user (ASRçµæœ)
                    asr_text = ', '.join(asr_results)
                    messages = [
                        {"role": "system", "content": prompt_text},
                        {"role": "user", "content": f"ã¶ã¤åˆ‡ã‚Šã®éŸ³å£°èªè­˜çµæœ: {asr_text}"}
                    ]

                    # â˜…ãƒ­ã‚°å‡ºåŠ›ã‚’ç°¡ç•¥åŒ–ï¼šmessagesé€ä¿¡ãƒ­ã‚°ã‚’å‰Šé™¤

                    response = requests.post(
                        'http://localhost:11434/api/chat',
                        json={
                            'model': self.model_name,
                            'messages': messages,
                            'stream': True,
                            'options': {
                                'temperature': 0.3,
                                'num_predict': 10,
                                'num_ctx': 512,
                                'num_batch': 256
                            }
                        },
                        stream=True,
                        timeout=30
                    )

                    res = ""
                    first_token_time = None
                    token_count = 0

                    for line in response.iter_lines():
                        if line:
                            try:
                                chunk_data = json.loads(line)
                                message_data = chunk_data.get('message', {})
                                token_fragment = message_data.get('content', '')

                                if token_fragment:
                                    token_count += 1

                                    # Time to First Token (TTFT) è¨ˆæ¸¬
                                    if first_token_time is None:
                                        first_token_time = datetime.now()
                                        ttft_ms = (first_token_time - api_start).total_seconds() * 1000
                                        # â˜…ãƒ­ã‚°å‡ºåŠ›ã‚’ç°¡ç•¥åŒ–ï¼šTTFTè¨ˆæ¸¬ãƒ­ã‚°ã‚’å‰Šé™¤

                                    res += token_fragment

                                # å®Œäº†ãƒã‚§ãƒƒã‚¯
                                if chunk_data.get('done', False):
                                    api_end = datetime.now()
                                    total_time = (api_end - api_start).total_seconds() * 1000
                                    # â˜…ãƒ­ã‚°å‡ºåŠ›ã‚’ç°¡ç•¥åŒ–ï¼šä¸­é–“ã®æ¨è«–æ™‚é–“ãƒ­ã‚°ã‚’å‰Šé™¤
                                    break

                            except json.JSONDecodeError:
                                continue

                elif self.model_name.startswith("gpt-") or self.model_name.startswith("o1"):
                    # OpenAI API: systemãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + humanã‚¿ã‚°ï¼ˆuserï¼‰ã§ASRçµæœ
                    asr_text = ', '.join(asr_results)
                    messages = [
                        {"role": "system", "content": prompt_text},
                        {"role": "user", "content": f"ã¶ã¤åˆ‡ã‚Šã®éŸ³å£°èªè­˜çµæœ: {asr_text}"}
                    ]

                    response = openai.chat.completions.create(
                        model=self.model_name,
                        messages=messages,
                        max_completion_tokens=20,
                        temperature=0.3
                    )
                    res = response.choices[0].message.content.strip() if response.choices[0].message.content else ""

                # ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ã®å¾Œå‡¦ç†: æ”¹è¡Œãƒ»å¥èª­ç‚¹é™¤å»
                res = res.replace('\n', '').replace('\r', '').replace('ã€‚', '').replace('ã€', '').strip()

                llm_end_time = datetime.now()
                llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000

                self.first_stage_response = res
                # â˜…ROS ãƒˆãƒ”ãƒƒã‚¯ç™ºè¡Œç”¨ã« last_reply ã«ã‚‚æ ¼ç´ï¼ˆROS2ãƒ©ãƒƒãƒ‘ãƒ¼ãŒç›£è¦–ã—ã¦ã„ã‚‹ï¼‰
                self.last_reply = res
                # â˜…ç°¡ç•¥åŒ–ï¼š[HH:MM:SS.mmm] å½¢å¼ã®ã¿è¡¨ç¤º
                sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}]\n")
                sys.stdout.flush()

            except Exception as api_error:
                sys.stdout.write(f"[NLG ERROR] first_stageç”Ÿæˆã‚¨ãƒ©ãƒ¼: {api_error}\n")
                sys.stdout.flush()
                self.first_stage_response = "ã†ã‚“"  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

        except Exception as e:
            sys.stdout.write(f"[NLG ERROR] first_stageå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}\n")
            sys.stdout.flush()
            self.first_stage_response = "ã†ã‚“"  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

    def _load_first_stage_prompt(self):
        """dialog_first_stage.txt ã‚’èª­ã¿è¾¼ã¿ã¾ã™"""
        # è¤‡æ•°ã®ãƒ‘ã‚¹ã‚’è©¦è¡Œ
        possible_paths = [
            # é–‹ç™ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            os.path.join(os.path.dirname(__file__), 'prompts', 'dialog_first_stage.txt'),
            # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
            os.path.join(os.path.dirname(__file__), '..', 'diaros', 'prompts', 'dialog_first_stage.txt'),
        ]

        # ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’å„ªå…ˆ
        if 'DIAROS_PROMPTS_DIR' in os.environ:
            possible_paths.insert(0, os.path.join(os.environ['DIAROS_PROMPTS_DIR'], 'dialog_first_stage.txt'))

        for path in possible_paths:
            if os.path.isfile(path):
                with open(path, 'r', encoding='utf-8') as f:
                    return f.read()

        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
        raise FileNotFoundError(f"dialog_first_stage.txt ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è©¦è¡Œãƒ‘ã‚¹: {possible_paths}")

    def generate_second_stage(self, query):
        """Second stage: turnTakingãŒå¿œç­”åˆ¤å®šã‚’å‡ºã—ãŸã‚‰å®Ÿè¡Œ"""
        start_time = datetime.now()

        # â˜…ã€é‡è¦ã€‘Second stage å‡¦ç†é–‹å§‹æ™‚ã«ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
        self.is_generating_second_stage = True
        timestamp = start_time.strftime('%H:%M:%S.%f')[:-3]
        sys.stdout.write(f"[{timestamp}] ğŸ”„ Second stage å‡¦ç†é–‹å§‹\n")
        sys.stdout.flush()

        try:
            # â˜…ä¿®æ­£ï¼šqueryãŒç©ºã®å ´åˆã¯ã€2.5ç§’é–“éš”ASRçµæœã¾ãŸã¯first_stageã®ASRçµæœã‚’ä½¿ç”¨
            if isinstance(query, list) and (not query or all((not x or x.strip() == "") for x in query)):
                # query ãŒç©º â†’ 2.5ç§’é–“éš”ASRçµæœã‚’å„ªå…ˆä½¿ç”¨ï¼ˆSecond stageç”¨ï¼‰
                if self.asr_history_2_5s:
                    asr_results = self.asr_history_2_5s
                    sys.stdout.write(f"[{start_time.strftime('%H:%M:%S.%f')[:-3]}][NLG SECOND_STAGE] ğŸ’¾ 2.5ç§’é–“éš”ASRçµæœã‚’ä½¿ç”¨\n")
                    sys.stdout.flush()
                else:
                    # asr_history_2_5s ãŒãªã„å ´åˆã¯å‰å›ã®ASRçµæœã‚’å†åˆ©ç”¨
                    asr_results = self.asr_results if self.asr_results else []
                    sys.stdout.write(f"[{start_time.strftime('%H:%M:%S.%f')[:-3]}][NLG SECOND_STAGE] ğŸ’¾ å‰å›ã® ASR çµæœã‚’å†åˆ©ç”¨\n")
                    sys.stdout.flush()
            else:
                # query ãŒæœ‰åŠ¹ â†’ ãã‚Œã‚’ä½¿ç”¨
                asr_results = query if isinstance(query, list) else [str(query)]

            # â˜…ä¿®æ­£ï¼šSecond stageã§ã¯ç©ºã®ASRçµæœã§ã‚‚å‡¦ç†ã‚’ç¶šã‘ã‚‹ï¼ˆfirst_stage_responseã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ï¼‰
            # ãŸã ã—first_stage_responseã‚‚ç©ºã®å ´åˆã¯è¿”ã™
            # â˜…ãƒ­ã‚°å‡ºåŠ›ã‚’ç°¡ç•¥åŒ–ï¼ˆãƒ‡ãƒãƒƒã‚°æƒ…å ±ã¯å‰Šé™¤ï¼‰

            if (not asr_results or all((not x or x.strip() == "") for x in asr_results)) and not self.first_stage_response:
                # â˜…ãƒ­ã‚°å‡ºåŠ›ã‚’ç°¡ç•¥åŒ–
                self.last_reply = ""
                self.last_source_words = []
                return

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆè¤‡æ•°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ–¹å¼ï¼‰
            prompt_load_start = datetime.now()
            prompt_dir = os.path.join(os.path.dirname(__file__), 'prompts')

            # â˜…ä¿®æ­£ï¼šdialog_second_stage_triple_input.txt ã‚’ä½¿ç”¨ï¼ˆplaceholder ãªã—ï¼‰
            second_stage_prompt_path = os.path.join(prompt_dir, 'dialog_second_stage_triple_input_example_role.txt')

            try:
                with open(second_stage_prompt_path, 'r', encoding='utf-8') as f:
                    system_prompt = f.read()

                # â˜…ãƒ­ã‚°å‡ºåŠ›ã‚’ç°¡ç•¥åŒ–ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿ãƒ­ã‚°ã‚’å‰Šé™¤ï¼‰

                # â˜…ä¿®æ­£ï¼šè¤‡æ•°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ–¹å¼ - æ­£ã—ã„ãƒ­ãƒ¼ãƒ«æ§‹é€ ã§å…¥åŠ›
                # 1. system: ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¿ã‚¹ã‚¯èª¬æ˜
                # 2. user: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éŸ³å£°èªè­˜çµæœï¼ˆç™ºè©±ï¼‰
                # 3. assistant: ã‚·ã‚¹ãƒ†ãƒ ãŒæ—¢ã«å‡ºåŠ›ã—ãŸãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ï¼ˆç¬¬1æ®µéšã®å¿œç­”ï¼‰
                # ã“ã®æµã‚Œã«ã‚ˆã‚Šã€LLMãŒå¯¾è©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£ã—ãèªè­˜ã§ãã‚‹

                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰
                asr_text = ', '.join(asr_results) if asr_results else "[éŸ³å£°èªè­˜çµæœãªã—]"
                backchannel_text = self.first_stage_response if self.first_stage_response else "[ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ãªã—]"

                # â˜…ä¿®æ­£ï¼šfirst_stageï¼ˆãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ï¼‰ã®æœ«å°¾ã«ã€Œã€ã€ãŒãªã‘ã‚Œã°è¿½åŠ 
                if backchannel_text and backchannel_text != "[ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ãªã—]":
                    if not backchannel_text.endswith("ã€"):
                        backchannel_text = backchannel_text + "ã€"

                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"è¤‡æ•°ã®ã¶ã¤åˆ‡ã‚Šã®éŸ³å£°èªè­˜çµæœï¼šé€±æœ«ã«æ™‚é–“ãŒã§ãã‚‹ã¨ã¤ã„ã¤ã„ã‚¹ãƒãƒ›ã‚’è¦‹ã¦, é€±æœ«ã«æ™‚é–“ãŒã§ãã‚‹ã¨ã¤ã„ã¤ã„ã‚¹ãƒãƒ›ã‚’è¦‹ã¦[ç„¡éŸ³], ã‚¹ãƒãƒ›ã‚’è¦‹ã¦ä¸€æ—¥ãŒçµ‚ã‚ã£ã¡ã‚ƒã†ã®ãŒå«Œã§, ä¸€æ—¥ãŒçµ‚ã‚ã£ã¡ã‚ƒã†ã®ãŒå«Œã§ä½•ã‹æ–°ã—ã„ã“ã¨ã‚’ã¯ã˜ã‚ãŸã„, ä½•ã‹æ–°ã—ã„ã“ã¨å§‹ã‚ãŸã„ã‚“ã ã‘ã©å®¶ã®ä¸­ã§ä¸€äººäºº, å®¶ã®ä¸­ã§ä¸€äººäººã§ã‚‚æ²¡é—˜ã§ãã‚‹ã‚ˆã†ãªè¶£å‘³ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã£ãŸ, æ²¡é—˜ã§ãã‚‹ã‚ˆã†ãªè¶£å‘³ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã£ã¦ãªã„ã‹ãª[ç„¡éŸ³][ç„¡éŸ³]"},
                    {"role": "assistant", "content": f"ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ï¼šãªã‚‹ã»ã©"},
                    {"role": "assistant", "content": f"ã‚¿ãƒ¡å£ã®å¿œç­”ï¼šãã‚Œãªã‚‰èª­æ›¸ã¨ã‹ãƒ—ãƒ©ãƒ¢ãƒ‡ãƒ«ä½œã‚Šã¨ã‹ã¯ã©ã†ï¼Ÿ"},
                    {"role": "user", "content": f"è¤‡æ•°ã®ã¶ã¤åˆ‡ã‚Šã®éŸ³å£°èªè­˜çµæœï¼š{asr_text}"},           # â˜…ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè©±ï¼ˆãƒ©ãƒ™ãƒ«ä»˜ãï¼‰
                    {"role": "assistant", "content": f"ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ï¼š{backchannel_text}"},  # â˜…ã‚·ã‚¹ãƒ†ãƒ ãŒæ—¢ã«å‡ºåŠ›ã—ãŸãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ï¼ˆãƒ©ãƒ™ãƒ«ä»˜ãã€æœ«å°¾ã«ã€Œã€ã€è¿½åŠ ï¼‰
                    {"role": "assistant", "content": f"ã‚¿ãƒ¡å£ã®å¿œç­”ï¼š"}
                ]

            except FileNotFoundError:
                sys.stdout.write(f"[NLG ERROR] second_stageãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {second_stage_prompt_path}\n")
                sys.stdout.flush()
                return

            # â˜…ç¢ºèªç”¨å‡ºåŠ›ï¼šä½¿ç”¨ã™ã‚‹ASRçµæœã¨first_stageçµæœã‚’è¡¨ç¤º
            timestamp = datetime.now().strftime('%H:%M:%S.%f')[:-3]

            # â˜…2.5ç§’é–“éš”ASRå±¥æ­´ã‚’å–å¾—ï¼ˆè¤‡æ•°ã®å„ªå…ˆåº¦ã§å–å¾—ï¼‰
            asr_2_5s_list = []
            # å„ªå…ˆåº¦1: ROS2NLGå‚ç…§ã‹ã‚‰å–å¾—ï¼ˆROS2ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰å—ã‘å–ã£ãŸå€¤ï¼‰
            if self.rnlg_ref and hasattr(self.rnlg_ref, 'asr_history_2_5s'):
                asr_2_5s_list = self.rnlg_ref.asr_history_2_5s
            # å„ªå…ˆåº¦2: DMå‚ç…§ã‹ã‚‰å–å¾—ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œæ™‚ï¼‰
            elif self.dm_ref and hasattr(self.dm_ref, 'asr_history_at_tt_decision_2_5s'):
                asr_2_5s_list = self.dm_ref.asr_history_at_tt_decision_2_5s
            # å„ªå…ˆåº¦3: ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã‹ã‚‰å–å¾—
            elif self.asr_history_2_5s:
                asr_2_5s_list = self.asr_history_2_5s

            sys.stdout.write(f"[{timestamp}] [Second Stage] 2.5ç§’é–“éš”ASRçµæœ: {asr_2_5s_list}\n")
            sys.stdout.write(f"[{timestamp}] [Second Stage] First Stageçµæœ: '{self.first_stage_response}'\n")

            # â˜…ä¿®æ­£ï¼šè¤‡æ•°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ–¹å¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’è¡¨ç¤º
            sys.stdout.write(f"[{timestamp}] [Second Stage] LLMã¸é€ä¿¡ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:\n")
            for i, msg in enumerate(messages, 1):
                sys.stdout.write(f"  {i} (role={msg['role']}): {msg['content']}\n")
            sys.stdout.flush()

            # LLMå‘¼ã³å‡ºã—
            llm_start_time = datetime.now()
            # â˜…ãƒ­ã‚°å‡ºåŠ›ã‚’ç°¡ç•¥åŒ–ï¼ˆLLMé–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤ï¼‰

            try:
                if self.model_name.startswith("gemma3:") or self.model_name.startswith("gpt-oss:"):
                    # â˜…Second stage ã§ã‚‚ requests API ã‚’ç›´æ¥å‘¼ã³å‡ºã—ï¼ˆfirst stage ã¨åŒã˜æ–¹å¼ï¼‰
                    # LangChain ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’æ’é™¤
                    import requests

                    api_start = datetime.now()

                    # â˜…ä¿®æ­£ï¼šè¤‡æ•°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ–¹å¼ã§é€ä¿¡ï¼ˆsystem + user1 + user2ï¼‰
                    response = requests.post(
                        'http://localhost:11434/api/chat',
                        json={
                            'model': self.model_name,
                            'messages': messages,  # â˜…è¤‡æ•°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’ç›´æ¥ä½¿ç”¨
                            'stream': True,
                            'options': {
                                'temperature': 0.3,
                                'num_predict': 50,  # second stage ã¯æœ€å¤§50ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆ20æ–‡å­—ç¨‹åº¦ã®ä¸€è¨€ç”¨ï¼‰
                                'num_ctx': 512,
                                'num_batch': 256
                            }
                        },
                        stream=True,
                        timeout=30
                    )

                    res = ""
                    first_token_time = None
                    token_count = 0

                    for line in response.iter_lines():
                        if line:
                            try:
                                chunk_data = json.loads(line)
                                message_data = chunk_data.get('message', {})
                                token_fragment = message_data.get('content', '')

                                if token_fragment:
                                    token_count += 1

                                    # Time to First Token (TTFT) è¨ˆæ¸¬
                                    if first_token_time is None:
                                        first_token_time = datetime.now()
                                        ttft_ms = (first_token_time - api_start).total_seconds() * 1000
                                        # â˜…ãƒ­ã‚°å‡ºåŠ›ã‚’ç°¡ç•¥åŒ–ï¼ˆTTFT ãƒ­ã‚°å‰Šé™¤ï¼‰

                                    res += token_fragment

                                # å®Œäº†ãƒã‚§ãƒƒã‚¯
                                if chunk_data.get('done', False):
                                    api_end = datetime.now()
                                    total_time = (api_end - api_start).total_seconds() * 1000
                                    # â˜…ãƒ­ã‚°å‡ºåŠ›ã‚’ç°¡ç•¥åŒ–ï¼ˆæ¨è«–æ™‚é–“ãƒ­ã‚°å‰Šé™¤ï¼‰
                                    break

                            except json.JSONDecodeError:
                                continue

                elif self.model_name.startswith("gpt-") or self.model_name.startswith("o1"):
                    # â˜…ä¿®æ­£ï¼šæ—¢ã«æ§‹ç¯‰ã•ã‚ŒãŸmessagesãƒªã‚¹ãƒˆï¼ˆè¤‡æ•°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ–¹å¼ï¼‰ã‚’ä½¿ç”¨
                    response = openai.chat.completions.create(
                        model=self.model_name,
                        messages=messages,  # â˜…è¤‡æ•°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’ç›´æ¥ä½¿ç”¨
                        max_completion_tokens=50,
                        temperature=0.3
                    )
                    res = response.choices[0].message.content.strip() if response.choices[0].message.content else ""

                # æ”¹è¡Œé™¤å»
                res = res.replace('\n', '').replace('\r', '')

                llm_end_time = datetime.now()
                llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                total_duration = (llm_end_time - start_time).total_seconds() * 1000

                # æœ€çµ‚çš„ãªå¿œç­”ã¯æœ¬å¿œç­”ã®ã¿ï¼ˆãƒ¡ã‚¤ãƒ³PCå´ã§first_stageã¯æ—¢ã«å†ç”Ÿã•ã‚Œã¦ã„ã‚‹ï¼‰
                final_response = res

                self.last_reply = final_response
                self.last_source_words = asr_results

                # ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±ã‚’è¨­å®š
                self.request_id = 1
                self.worker_name = "nlg-two-stage"
                self.start_timestamp_ns = int(start_time.timestamp() * 1_000_000_000)
                self.completion_timestamp_ns = int(llm_end_time.timestamp() * 1_000_000_000)
                self.inference_duration_ms = total_duration

                # â˜…ç°¡ç•¥åŒ–ï¼š[HH:MM:SS.mmm] å½¢å¼ã®ã¿è¡¨ç¤º
                sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}]\n")
                sys.stdout.flush()

                # â˜…ã€é‡è¦ã€‘Second stage å‡¦ç†å®Œäº†æ™‚ã«ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
                self.is_generating_second_stage = False
                timestamp = datetime.now().strftime('%H:%M:%S.%f')[:-3]
                sys.stdout.write(f"[{timestamp}] âœ… Second stage å‡¦ç†å®Œäº†\n")
                sys.stdout.flush()

                # â˜…ä¿ç•™ä¸­ã® first_stage ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã‚ã‚Œã°å‡¦ç†
                if self.pending_first_stage_request:
                    sys.stdout.write(f"[{timestamp}] â–¶ï¸  ä¿ç•™ä¸­ã® first_stage ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œ\n")
                    sys.stdout.flush()

                    pending_req = self.pending_first_stage_request
                    self.pending_first_stage_request = None  # ä¿ç•™ã‚­ãƒ¥ãƒ¼ã‚’ã‚¯ãƒªã‚¢

                    # ä¿ç•™ã•ã‚Œã¦ã„ãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œ
                    self.update(
                        words=pending_req['words'],
                        stage='first',
                        turn_taking_decision_timestamp_ns=pending_req['turn_taking_decision_timestamp_ns'],
                        first_stage_backchannel_at_tt=pending_req['first_stage_backchannel_at_tt'],
                        asr_history_2_5s=pending_req['asr_history_2_5s']
                    )

            except Exception as api_error:
                sys.stdout.write(f"[NLG ERROR] second_stageç”Ÿæˆã‚¨ãƒ©ãƒ¼: {api_error}\n")
                sys.stdout.flush()
                self.last_reply = self.first_stage_response  # ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ã®ã¿ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                self.last_source_words = asr_results

                # â˜…ã€é‡è¦ã€‘ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
                self.is_generating_second_stage = False
                timestamp = datetime.now().strftime('%H:%M:%S.%f')[:-3]
                sys.stdout.write(f"[{timestamp}] âœ… Second stage å‡¦ç†å®Œäº†ï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰\n")
                sys.stdout.flush()

                # â˜…ä¿ç•™ä¸­ã® first_stage ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã‚ã‚Œã°å‡¦ç†
                if self.pending_first_stage_request:
                    sys.stdout.write(f"[{timestamp}] â–¶ï¸  ä¿ç•™ä¸­ã® first_stage ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼å¾Œï¼‰\n")
                    sys.stdout.flush()

                    pending_req = self.pending_first_stage_request
                    self.pending_first_stage_request = None  # ä¿ç•™ã‚­ãƒ¥ãƒ¼ã‚’ã‚¯ãƒªã‚¢

                    # ä¿ç•™ã•ã‚Œã¦ã„ãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œ
                    self.update(
                        words=pending_req['words'],
                        stage='first',
                        turn_taking_decision_timestamp_ns=pending_req['turn_taking_decision_timestamp_ns'],
                        first_stage_backchannel_at_tt=pending_req['first_stage_backchannel_at_tt'],
                        asr_history_2_5s=pending_req['asr_history_2_5s']
                    )

        except Exception as e:
            sys.stdout.write(f"[NLG ERROR] second_stageå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}\n")
            sys.stdout.flush()
            self.last_reply = self.first_stage_response  # ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ã®ã¿ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            self.last_source_words = asr_results

            # â˜…ã€é‡è¦ã€‘å¤–å´ã®ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã§ã‚‚ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
            self.is_generating_second_stage = False
            timestamp = datetime.now().strftime('%H:%M:%S.%f')[:-3]
            sys.stdout.write(f"[{timestamp}] âœ… Second stage å‡¦ç†å®Œäº†ï¼ˆå¤–éƒ¨ã‚¨ãƒ©ãƒ¼ï¼‰\n")
            sys.stdout.flush()

            # â˜…ä¿ç•™ä¸­ã® first_stage ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã‚ã‚Œã°å‡¦ç†
            if self.pending_first_stage_request:
                sys.stdout.write(f"[{timestamp}] â–¶ï¸  ä¿ç•™ä¸­ã® first_stage ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œï¼ˆå¤–éƒ¨ã‚¨ãƒ©ãƒ¼å¾Œï¼‰\n")
                sys.stdout.flush()

                pending_req = self.pending_first_stage_request
                self.pending_first_stage_request = None  # ä¿ç•™ã‚­ãƒ¥ãƒ¼ã‚’ã‚¯ãƒªã‚¢

                # ä¿ç•™ã•ã‚Œã¦ã„ãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œ
                self.update(
                    words=pending_req['words'],
                    stage='first',
                    turn_taking_decision_timestamp_ns=pending_req['turn_taking_decision_timestamp_ns'],
                    first_stage_backchannel_at_tt=pending_req['first_stage_backchannel_at_tt'],
                    asr_history_2_5s=pending_req['asr_history_2_5s']
                )

    def _perform_simple_inference(self, query):
        """ã‚·ãƒ³ãƒ—ãƒ«ãªå˜ä¸€ã‚¹ãƒ¬ãƒƒãƒ‰æ¨è«– (gemma3:12bä½¿ç”¨)"""
        start_time = datetime.now()

        # æ¨è«–é–‹å§‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        if self.current_session_id:
            self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "inference_start", {
                "query_type": "list" if isinstance(query, list) else "string",
                "query_length": len(query) if isinstance(query, list) else len(str(query))
            })

        try:
            res = ""  # resã‚’å¿…ãšåˆæœŸåŒ–
            asr_results = self.asr_results

            if asr_results and isinstance(asr_results, list) and len(asr_results) >= 1:
                if all((not x or x.strip() == "") for x in asr_results):
                    self.last_reply = ""
                    self.last_source_words = []
                    return

                # ç‰¹å®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨æ™‚ã¯ [é›‘éŸ³][ç„¡éŸ³]<unk> ã‚’é™¤å»
                noise_tag_removal_prompts = [
                    "dialog_simple.txt",
                    "fix_asr_simple.txt",
                    "dialog_predict.txt",
                    "dialog_explain.txt",
                    "dialog_example.txt"
                ]

                if self.prompt_file_name in noise_tag_removal_prompts:
                    # ä¸è¦ãªã‚¿ã‚°ã‚’é™¤å»
                    cleaned_asr_results = []
                    for asr in asr_results:
                        # [é›‘éŸ³]ã€[ç„¡éŸ³]ã€<unk>ã‚’é™¤å»
                        cleaned = asr.replace("[é›‘éŸ³]", "").replace("[ç„¡éŸ³]", "").replace("<unk>", "")
                        cleaned = cleaned.strip()
                        if cleaned:  # ç©ºæ–‡å­—åˆ—ã§ãªã„å ´åˆã®ã¿è¿½åŠ 
                            cleaned_asr_results.append(cleaned)

                    # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨
                    asr_results_for_prompt = cleaned_asr_results if cleaned_asr_results else asr_results
                else:
                    # ãã®ä»–ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã¯ãã®ã¾ã¾ä½¿ç”¨
                    asr_results_for_prompt = asr_results

                # éŸ³å£°èªè­˜çµæœã‚’ã™ã¹ã¦åˆ—æŒ™
                asr_lines = []
                for idx, asr in enumerate(asr_results_for_prompt):
                    asr_lines.append(f"èªè­˜çµæœ{idx+1}: {asr}")

                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆè¨­å®šã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½¿ç”¨ï¼‰
                try:
                    with open(self.prompt_file_path, 'r', encoding='utf-8') as f:
                        prompt = f.read()
                    if not prompt.strip():
                        sys.stdout.write(f"[NLG ERROR] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™: {self.prompt_file_path}\n")
                        sys.stdout.flush()
                        return
                except FileNotFoundError:
                    sys.stdout.write(f"[NLG ERROR] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.prompt_file_path}\n")
                    sys.stdout.flush()
                    return
                except Exception as e:
                    sys.stdout.write(f"[NLG ERROR] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {self.prompt_file_path} - {e}\n")
                    sys.stdout.flush()
                    return
                
                # LLMå‘¼ã³å‡ºã—
                llm_start_time = datetime.now()
                sys.stdout.write(f"[{llm_start_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸ¤– {self.model_name}æ¨è«–é–‹å§‹\n")
                # LLMæ¨è«–é–‹å§‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                if self.current_session_id:
                    self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "llm_start", {
                        "model": self.model_name,
                        "prompt_type": "asr_dialogue",
                        "prompt_length": len(prompt),
                        "asr_count": len(asr_results)
                    })

                # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæ¨è«–å‡¦ç†
                try:
                    if self.model_name.startswith("gemma3:") or self.model_name.startswith("gpt-oss:"):
                        # Ollama ãƒ¢ãƒ‡ãƒ«ï¼ˆAPIç›´æ¥å‘¼ã³å‡ºã—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹ï¼‰
                        full_prompt = f"{prompt}\n\nã¶ã¤åˆ‡ã‚Šã®éŸ³å£°èªè­˜çµæœ: {', '.join(asr_results_for_prompt)}"

                        # Ollama APIç›´æ¥å‘¼ã³å‡ºã—ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
                        try:
                            api_start_time = datetime.now()
                            api_response = requests.post(
                                'http://localhost:11434/api/generate',
                                json={
                                    'model': self.model_name,
                                    'prompt': full_prompt,
                                    'stream': True,  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹åŒ–ï¼ˆTTFTæœ€å°åŒ–ï¼‰
                                    'options': {
                                        'temperature': 0.7,
                                        'top_p': 0.9,
                                        'num_predict': 10,  # çŸ­ã„ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ãƒ‰ã§é«˜é€ŸåŒ–
                                        'num_ctx': 4096,
                                        'num_batch': 3072
                                    }
                                },
                                timeout=30,
                                stream=True
                            )

                            res = ""
                            first_token_time = None

                            if api_response.status_code == 200:
                                for line in api_response.iter_lines():
                                    if line:
                                        try:
                                            chunk_data = json.loads(line)
                                            token_fragment = chunk_data.get('response', '')

                                            if token_fragment:
                                                # TTFTè¨ˆæ¸¬
                                                if first_token_time is None:
                                                    first_token_time = datetime.now()
                                                    ttft_ms = (first_token_time - api_start_time).total_seconds() * 1000
                                                    sys.stdout.write(f"[{first_token_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸ¯ TTFT: {ttft_ms:.1f}ms\n")
                                                    sys.stdout.flush()

                                                res += token_fragment

                                            # å®Œäº†ãƒã‚§ãƒƒã‚¯
                                            if chunk_data.get('done', False):
                                                llm_end_time = datetime.now()
                                                llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000

                                                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ï¼ˆè©³ç´°ï¼‰
                                                load_duration = chunk_data.get('load_duration', 0) / 1e6
                                                prompt_eval_duration = chunk_data.get('prompt_eval_duration', 0) / 1e6
                                                eval_duration = chunk_data.get('eval_duration', 0) / 1e6
                                                sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] â±ï¸ load: {load_duration:.1f}ms, prompt: {prompt_eval_duration:.1f}ms, eval: {eval_duration:.1f}ms â†’ total: {llm_duration:.1f}ms\n")
                                                break
                                        except json.JSONDecodeError:
                                            continue
                            else:
                                res = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
                        except Exception as api_error:
                            sys.stdout.write(f"[NLG ERROR] Ollama APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {api_error}\n")
                            sys.stdout.flush()
                            res = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"

                    elif self.model_name.startswith("gpt-") or self.model_name.startswith("o1"):
                        # OpenAI APIï¼ˆGPT-5, GPT-4, o1ãªã©ï¼‰
                        messages = [
                            {"role": "system", "content": prompt}
                        ]

                        # â˜…dialog_example_role.txtä½¿ç”¨æ™‚ã¯1-shotä¾‹ç¤ºãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
                        if self.prompt_file_name == "dialog_example_role.txt":
                            # 1-shotä¾‹ç¤ºï¼šä¾‹ç¤ºãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè©±
                            messages.append({
                                "role": "user",
                                "content": "è¤‡æ•°ã®ã¶ã¤åˆ‡ã‚Šã®éŸ³å£°èªè­˜çµæœ: ä»Šæ—¥ä¼šç¤¾ã§æ–°ã—ã„, ä»Šæ—¥ä¼šç¤¾ã§æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è©±ãŒã‚ã£ã¦, ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è©±ãŒã‚ã£ã¦æœ€åˆã¯ã™ã”ãé¢ç™½ãã†ã§ã‚„ã£ã¦ã¿, ã™ã”ãé¢ç™½ãã†ã§ã‚„ã£ã¦ã¿ãŸã„ã£ã¦æ€ã‚“ã ã‘ã©ã‚·ãƒ¡, æ€ã‚“ã ã‘ã©ç· ã‚åˆ‡ã‚ŒãŒã‹ãªã‚Šã‚¿ã‚¤ãƒˆã ã‹ã‚‰é ‘å¼µã‚‰"
                            })
                            # 1-shotä¾‹ç¤ºï¼šä¾‹ç¤ºå¿œç­”
                            messages.append({
                                "role": "assistant",
                                "content": "ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰ã®å¿œç­”: ãã†ãªã‚“ã ã€ç„¡ç†ã—ãªã„ã§é ‘å¼µã£ã¦ã­ï¼"
                            })

                        # å¯¾è©±å±¥æ­´/ä¾‹ç¤ºãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å«ã‚ã‚‹å ´åˆ
                        if hasattr(self, 'example_messages') and self.example_messages:
                            messages.extend(self.example_messages)

                        # ç¾åœ¨ã®ASRçµæœï¼ˆuserï¼‰
                        messages.append({
                            "role": "user",
                            "content": f"è¤‡æ•°ã®ã¶ã¤åˆ‡ã‚Šã®éŸ³å£°èªè­˜çµæœï¼š{', '.join(asr_results_for_prompt)}"
                        })

                        # å¿œç­”ç”ŸæˆæŒ‡ç¤ºï¼ˆassistantï¼‰
                        messages.append({
                            "role": "assistant",
                            "content": "ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰ã®å¿œç­”: "
                        })

                        # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°
                        sys.stdout.write(f"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}][NLG DEBUG] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt)}æ–‡å­—, ASRçµæœæ•°: {len(asr_results_for_prompt)}, messageså½¢å¼: {len(messages)}ã‚¿ãƒ¼ãƒ³\n")
                        sys.stdout.flush()

                        # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ã®æœ€é©åŒ–è¨­å®š
                        if self.model_name.startswith("gpt-4.1"):
                            # GPT-4.1ç³»: æœ€é€Ÿ540ms
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_completion_tokens=50,
                                temperature=0.3
                            )
                        elif "chat-latest" in self.model_name:
                            # GPT-5-chat-latest: æ¨è«–æœ€å°åŒ–ç‰ˆ
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_completion_tokens=50
                            )
                        elif self.model_name.startswith("gpt-5") or self.model_name.startswith("o1"):
                            # GPT-5/o1: æ¨è«–ãƒ¢ãƒ‡ãƒ«
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_completion_tokens=500,
                                reasoning_effort="low"
                            )
                        else:
                            # GPT-4oç³»: æ¨™æº–
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_tokens=50,
                                temperature=0.3
                            )

                        res = response.choices[0].message.content.strip() if response.choices[0].message.content else ""

                        # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°
                        sys.stdout.write(f"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}][NLG DEBUG] APIå¿œç­”é•·: {len(res)}æ–‡å­—\n")
                        sys.stdout.flush()

                    llm_end_time = datetime.now()
                    llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âœ… {self.model_name}æ¨è«–å®Œäº† (LLMæ™‚é–“: {llm_duration:.1f}ms)\n")
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] ç”Ÿæˆå¿œç­”: '{res}'\n")
                    sys.stdout.flush()

                except Exception as api_error:
                    # APIå‘¼ã³å‡ºã—å¤±æ•—æ™‚ã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®š
                    res = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
                    llm_end_time = datetime.now()
                    llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âŒ {self.model_name} APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {api_error}\n")
                    sys.stdout.flush()

                # LLMæ¨è«–å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                if self.current_session_id:
                    self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "llm_complete", {
                        "model": self.model_name,
                        "llm_duration_ms": llm_duration,
                        "response_length": len(res)
                    })
                
                source_words = asr_results
                
            else:
                if not query or (isinstance(query, list) and all((not x or x.strip() == "") for x in query)):
                    self.last_reply = ""
                    self.last_source_words = []
                    return
                elif query == "dummy":
                    res = "ã¯ã„"
                    source_words = [str(query)]
                else:
                    text_input = query
                    role = "å„ªã—ã„æ€§æ ¼ã®ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰ã¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè©±ã«å¯¾ã—ã¦ç›¸æ‰‹ã‚’åŠ´ã‚‹ã‚ˆã†ãªè¿”ç­”ã®ã¿ã‚’ï¼’ï¼æ–‡å­—ä»¥å†…ã§ã—ã¦ãã ã•ã„ã€‚"

                    llm_start_time = datetime.now()

                    if self.model_name.startswith("gemma3:") or self.model_name.startswith("gpt-oss:"):
                        # Ollama gemma3ç³»ãƒ¢ãƒ‡ãƒ«ï¼ˆLangChainçµŒç”±ï¼‰
                        messages = [
                            ("system", role),
                            ("human", text_input)
                        ]
                        query_prompt = ChatPromptTemplate.from_messages(messages)
                        chain = query_prompt | self.ollama_model | StrOutputParser()
                        res = chain.invoke({})

                    elif self.model_name.startswith("gpt-") or self.model_name.startswith("o1"):
                        # OpenAI APIï¼ˆGPT-5, GPT-4, o1ãªã©ï¼‰
                        messages = [
                            {"role": "system", "content": role},
                            {"role": "user", "content": text_input}
                        ]

                        # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ã®æœ€é©åŒ–è¨­å®š
                        if self.model_name.startswith("gpt-4.1"):
                            # GPT-4.1ç³»: æœ€é€Ÿ540ms
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_completion_tokens=50,
                                temperature=0.3
                            )
                        elif "chat-latest" in self.model_name:
                            # GPT-5-chat-latest: æ¨è«–æœ€å°åŒ–ç‰ˆ
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_completion_tokens=50
                            )
                        elif self.model_name.startswith("gpt-5") or self.model_name.startswith("o1"):
                            # GPT-5/o1: æ¨è«–ãƒ¢ãƒ‡ãƒ«
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_completion_tokens=500,
                                reasoning_effort="low"
                            )
                        else:
                            # GPT-4oç³»: æ¨™æº–
                            response = openai.chat.completions.create(
                                model=self.model_name,
                                messages=messages,
                                max_tokens=50,
                                temperature=0.3
                            )

                        res = response.choices[0].message.content.strip()

                    llm_end_time = datetime.now()
                    llm_duration = (llm_end_time - llm_start_time).total_seconds() * 1000
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] âœ… {self.model_name}æ¨è«–å®Œäº† (LLMæ™‚é–“: {llm_duration:.1f}ms)\n")
                    sys.stdout.write(f"[{llm_end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG VERBOSE] ç”Ÿæˆå¿œç­”: '{res}'\n")
                    sys.stdout.flush()

                    if ":" in res:
                        res = res.split(":", 1)[1]

                    source_words = [str(query)]
            
            # æ”¹è¡Œã‚’é™¤å»ã—ã¦1è¡Œã«ã™ã‚‹
            res = res.replace('\n', '').replace('\r', '')
            
            end_time = datetime.now()
            total_duration = (end_time - start_time).total_seconds() * 1000
            
            # çµæœã‚’è¨­å®š
            self.last_reply = res
            self.last_source_words = source_words
            
            # ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±ã‚’è¨­å®š
            self.request_id = 1
            self.worker_name = "nlg-single"
            self.start_timestamp_ns = int(start_time.timestamp() * 1_000_000_000)
            self.completion_timestamp_ns = int(end_time.timestamp() * 1_000_000_000)
            self.inference_duration_ms = total_duration
            
            # æˆåŠŸæ™‚ã¯æ¥ç¶šã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
            self.connection_error_count = 0
            self.connection_error_suppress_until = None
            
            # æ¨è«–å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
            if self.current_session_id:
                self.time_tracker.add_checkpoint(self.current_session_id, "nlg", "inference_complete", {
                    "total_duration_ms": total_duration,
                    "response": res,
                    "source_words": source_words
                })
            
            sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG] ğŸ å‡¦ç†å®Œäº† (ç·æ™‚é–“: {total_duration:.1f}ms): {res}\n")
            sys.stdout.flush()
            
        except Exception as e:
            end_time = datetime.now()
            
            # æ¥ç¶šã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†
            error_str = str(e)
            is_connection_error = (
                "[Errno 111] Connection refused" in error_str or
                "llama runner process has terminated" in error_str or
                "broken pipe" in error_str or
                "status code: 500" in error_str
            )
            
            if is_connection_error:
                self.connection_error_count += 1
                # é€£ç¶šæ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒ5å›ä»¥ä¸Šãªã‚‰30ç§’é–“ãƒªã‚¯ã‚¨ã‚¹ãƒˆæŠ‘åˆ¶
                if self.connection_error_count >= 5:
                    self.connection_error_suppress_until = end_time + timedelta(seconds=30)
                    if self.connection_error_count == 5:  # åˆå›æŠ‘åˆ¶æ™‚ã®ã¿ãƒ­ã‚°å‡ºåŠ›
                        sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG WARNING] ğŸš« é€£ç¶šæ¥ç¶šã‚¨ãƒ©ãƒ¼æ¤œå‡ºã€‚30ç§’é–“ãƒªã‚¯ã‚¨ã‚¹ãƒˆæŠ‘åˆ¶ã—ã¾ã™\n")
                        sys.stdout.flush()
                # æ¥ç¶šã‚¨ãƒ©ãƒ¼ã¯è©³ç´°ãƒ­ã‚°ã‚’æŠ‘åˆ¶
                if self.connection_error_count <= 3:  # æœ€åˆã®3å›ã®ã¿ãƒ­ã‚°å‡ºåŠ›
                    sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG ERROR] âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: Ollamaæ¥ç¶šå¤±æ•—\n")
                    sys.stdout.flush()
            else:
                # æ¥ç¶šã‚¨ãƒ©ãƒ¼ä»¥å¤–ã®å ´åˆã¯ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆ
                self.connection_error_count = 0
                self.connection_error_suppress_until = None
                sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG ERROR] âŒ æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}\n")
                sys.stdout.flush()
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ï¼ˆå›ºå®šå¿œç­”ã®ã¿ï¼‰
            if is_connection_error:
                # Ollamaã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç°¡å˜ãªå›ºå®šå¿œç­”ã®ã¿
                fallback_responses = [
                    "ãã†ã§ã™ã­ã€‚",
                    "ãªã‚‹ã»ã©ã€‚", 
                    "ã‚ã‹ã‚Šã¾ã—ãŸã€‚",
                    "ã¯ã„ã€‚",
                    "ãã†ãªã‚“ã§ã™ã­ã€‚"
                ]
                import random
                fallback_response = random.choice(fallback_responses)
                
                self.last_reply = fallback_response
                self.last_source_words = query if isinstance(query, list) else [str(query)]
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±è¨­å®š
                self.request_id = 999  # å›ºå®šå¿œç­”ID
                self.worker_name = "static-fallback"
                self.start_timestamp_ns = int(start_time.timestamp() * 1_000_000_000)
                self.completion_timestamp_ns = int(end_time.timestamp() * 1_000_000_000)
                self.inference_duration_ms = (end_time - start_time).total_seconds() * 1000
                
                if self.connection_error_count <= 3:
                    sys.stdout.write(f"[{end_time.strftime('%H:%M:%S.%f')[:-3]}][NLG FALLBACK] ğŸ”„ å›ºå®šå¿œç­”ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {fallback_response}\n")
                    sys.stdout.flush()
            else:
                # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®çµæœã‚’è¨­å®š
                self.last_reply = ""
                self.last_source_words = []


    def run(self):
        sys.stdout.write("[NLG] å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹æ¨è«–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹ (2.5ç§’é–“éš”åˆ¶å¾¡)\n")
        sys.stdout.write(f"[NLG] ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.model_name}\n")
        sys.stdout.flush()
        
        # ä¸¦åˆ—å‡¦ç†ç‰ˆã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
        # while True:
        #     # ä¸¦åˆ—æ¨è«–ã‚·ã‚¹ãƒ†ãƒ ã§ã¯çµæœç›£è¦–ã®ã¿
        #     try:
        #         # å®Œäº†ã—ãŸæ¨è«–çµæœãŒã‚ã‚Œã°å‡¦ç†
        #         if not self.result_queue.empty():
        #             result = self.result_queue.get_nowait()
        #             # çµæœå‡¦ç†ã¯ãƒ¯ãƒ¼ã‚«ãƒ¼å†…ã§å®Œçµã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ç‰¹ã«å‡¦ç†ãªã—
        #     except:
        #         pass
        #     
        #     time.sleep(0.01)  # 10mså¾…æ©Ÿ
        
        # å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹ç‰ˆã§ã¯ç‰¹ã«ç„¡é™ãƒ«ãƒ¼ãƒ—ã¯ä¸è¦
        pass

if __name__ == "__main__":
    gen = NaturalLanguageGeneration()
    gen.run()