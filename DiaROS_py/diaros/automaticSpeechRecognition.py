# import time
# import sys
# import threading
# import queue
# import numpy as np

# import torch
# from transformers import AutoModelForCTC, Wav2Vec2Processor, Wav2Vec2CTCTokenizer
# from transformers.utils import logging
# import difflib
# import warnings

# # Audio recording parameters
# SAMPLE_RATE = 16000
# CHUNK_SIZE = 240
# MODEL_ID = 'SiRoZaRuPa/wav2vec2-base-kanji-unigram-RS-s-1120'
# AUDIO_DURATION = 5  # seconds
# INPUT_LEN = int(SAMPLE_RATE * AUDIO_DURATION)
# USE_GPU = True

# logging.set_verbosity_error()
# warnings.filterwarnings('ignore')

# def create_diff_list(old, new):
#     diff = list(difflib.ndiff(old, new))
#     lines = []
#     current_text = ""
#     is_change = False
#     for i in diff:
#         if i[0] == ' ':
#             if is_change:
#                 lines.append((1, current_text))
#                 current_text = ""
#             is_change = False
#             current_text += i[2:]
#         elif i[0] == '-':
#             continue
#         elif i[0] == '+':
#             if not is_change:
#                 if current_text:
#                     lines.append((0, current_text))
#                     current_text = ""
#                 is_change = True
#             current_text += i[2:]
#     if current_text:
#         lines.append((is_change, current_text))
#     return lines

# def apply_color_to_diff(lines, end_string=']'):
#     result = ""
#     for is_change, text in lines:
#         if is_change:
#             if lines[-1] == (1, text):
#                 if end_string in text and text.endswith(end_string):
#                     if text[-3] == 'é›‘':
#                         result += f'\033[91m{text[:-4]}\033[0m' + f'\033[42m{text[-4:]}\033[0m'
#                     elif text[-3] == 'ç„¡':
#                         result += f'\033[91m{text[:-4]}\033[0m' + f'\033[44m{text[-4:]}\033[0m'
#                 else:
#                     result += f'\033[91m{text}\033[0m'
#             else:
#                 result += f'\033[93m{text}\033[0m'
#         else:
#             result += text
#     return result

# class AutomaticSpeechRecognition:
#     def __init__(self):
#         self.last_audio = None
#         self.word = ""
#         self.is_final = False
#         self.recv_count = 0
#         self.audio_buffer = np.array([], dtype=np.float32)
#         self.audio_queue = queue.Queue()
#         self.running = True
#         self.last_sent = ""
#         self.model = None
#         self.processor = None
#         self.tokenizer = None
#         self.model_thread = threading.Thread(target=self.recognition_thread)
#         self.model_thread.daemon = True
#         self.model_thread.start()
#         sys.stdout.write('ASR node start up.\n')
#         sys.stdout.write('=====================================================\n')

#     def update_audio(self, audio_np):
#         self.audio_queue.put(audio_np)
#         self.recv_count += 1

#     def pubASR(self):
#         return {"you": self.word, "is_final": self.is_final}

#     def run(self):
#         while self.running:
#             time.sleep(0.1)

#     def recognition_thread(self):
#         sys.stdout.write('Loading ASR model...\n')
#         self.tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(MODEL_ID)
#         self.processor = Wav2Vec2Processor.from_pretrained(MODEL_ID, tokenizer=self.tokenizer)
#         self.model = AutoModelForCTC.from_pretrained(MODEL_ID)
#         self.model.eval()
#         if USE_GPU and torch.cuda.is_available():
#             device = torch.device("cuda")
#             self.model.to(device)
#         else:
#             device = torch.device("cpu")
#         sys.stdout.write('ASR model loaded.\n')
#         sys.stdout.flush()

#         mic_input = np.array([], dtype=np.float32)
#         last_sent = ""
#         start_time = time.time()
#         last_time = time.time()
#         last_infer_len = 0  # å‰å›æ¨è«–æ™‚ã®mic_inputã®é•·ã•
#         try:
#             while self.running:
#                 new_data_added = False
#                 while not self.audio_queue.empty():
#                     data = self.audio_queue.get()
#                     mic_input = np.append(mic_input, data)
#                     new_data_added = True
#                 # 5ç§’ã‚’è¶…ãˆãŸã‚‰å¤ã„ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¨ã¦ã‚‹
#                 if len(mic_input) > INPUT_LEN:
#                     mic_input = mic_input[-INPUT_LEN:]
#                 # æ–°ãŸãªéŸ³å£°ãŒ100msåˆ†æºœã¾ã£ã¦ã„ãŸã‚‰æ¨è«–
#                 # ä¿®æ­£: last_infer_lenã®æ›´æ–°ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’æ¨è«–å¾Œã«ã—ã€æ¨è«–æ¡ä»¶ã‚’ã€Œæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒ100msåˆ†ä»¥ä¸Šæºœã¾ã£ã¦ã„ã‚‹å ´åˆã€ã«é™å®š
#                 if len(mic_input) >= int(SAMPLE_RATE * 0.1) and (len(mic_input) - last_infer_len >= int(SAMPLE_RATE * 0.1)):
#                     print(f"Received audio data length: {len(mic_input)}")
#                     sys.stdout.flush()
#                     array = mic_input.astype(np.float32)
#                     inputs = self.processor(array, sampling_rate=SAMPLE_RATE, return_tensors="pt", padding=True)
#                     if USE_GPU and torch.cuda.is_available():
#                         inputs = {k: v.to(device) for k, v in inputs.items()}
#                         self.model = self.model.to(device)
#                     with torch.no_grad():
#                         logits = self.model(**inputs).logits
#                     predicted_ids = torch.argmax(logits, dim=-1)
#                     sentence = self.processor.batch_decode(predicted_ids)[0]
#                     now = time.time()
#                     elapsed_time = now - start_time
#                     process_time = int(1000 * (now - last_time))
#                     last_time = now
#                     diff = create_diff_list(last_sent, sentence)
#                     colored = apply_color_to_diff(diff)
#                     output = f'{elapsed_time:7.3f} ({process_time:5d} ms): {colored}'
#                     if last_sent != sentence:
#                         print(output)
#                     else:
#                         sys.stdout.write("\r" + output + " " * 20 + "\r")
#                         sys.stdout.flush()
#                     self.word = sentence
#                     self.is_final = True if sentence.strip() != "" else False
#                     last_sent = sentence
#                     last_infer_len = len(mic_input)  # æ¨è«–å¾Œã«æ›´æ–°
#                 time.sleep(0.01)  # ãƒ«ãƒ¼ãƒ—ãŒé«˜é€Ÿã™ãã‚‹å ´åˆã®CPUè² è·è»½æ¸›
#         except Exception as e:
#             print(f"Error in recognition_thread: {e}")

# NOTE éŸ³å£°å…¥åŠ›é•·å›ºå®š
import time
import sys
import threading
import queue
import numpy as np

import torch
from transformers import AutoModelForCTC, Wav2Vec2Processor, Wav2Vec2CTCTokenizer
from transformers.utils import logging
import difflib
import warnings

# Audio recording parameters
SAMPLE_RATE = 16000
CHUNK_SIZE = 240
# MODEL_ID = 'SiRoZaRuPa/wav2vec2-base-kanji-unigram-RS-s-1120'
MODEL_ID = 'SiRoZaRuPa/japanese-HuBERT-base-VADLess-ASR-RSm'
AUDIO_DURATION = 5  # seconds
INPUT_LEN = int(SAMPLE_RATE * AUDIO_DURATION)
USE_GPU = True
SHOW_BASIC_LOGS = False
SHOW_DEBUG_LOGS = False

logging.set_verbosity_error()
warnings.filterwarnings('ignore')

def create_diff_list(old, new):
    diff = list(difflib.ndiff(old, new))
    lines = []
    current_text = ""
    is_change = False
    for i in diff:
        if i[0] == ' ':
            if is_change:
                lines.append((1, current_text))
                current_text = ""
            is_change = False
            current_text += i[2:]
        elif i[0] == '-':
            continue
        elif i[0] == '+':
            if not is_change:
                if current_text:
                    lines.append((0, current_text))
                    current_text = ""
                is_change = True
            current_text += i[2:]
    if current_text:
        lines.append((is_change, current_text))
    return lines

def apply_color_to_diff(lines, end_string=']'):
    result = ""
    for is_change, text in lines:
        if is_change:
            if lines[-1] == (1, text):
                if end_string in text and text.endswith(end_string):
                    if text[-3] == 'é›‘':
                        result += f'\033[91m{text[:-4]}\033[0m' + f'\033[42m{text[-4:]}\033[0m'
                    elif text[-3] == 'ç„¡':
                        result += f'\033[91m{text[:-4]}\033[0m' + f'\033[44m{text[-4:]}\033[0m'
                else:
                    result += f'\033[91m{text}\033[0m'
            else:
                result += f'\033[93m{text}\033[0m'
        else:
            result += text
    return result

class AutomaticSpeechRecognition:
    def __init__(self):
        self.last_audio = None
        self.word = ""
        self.is_final = False
        self.recv_count = 0
        self.audio_buffer = np.array([], dtype=np.float32)
        self.audio_queue = queue.Queue()
        self.running = True
        self.last_sent = ""
        self.model = None
        self.processor = None
        self.tokenizer = None
        self.new_result = False  # è¿½åŠ : æ–°ã—ã„èªè­˜çµæœãƒ•ãƒ©ã‚°
        self.model_thread = threading.Thread(target=self.recognition_thread)
        self.model_thread.daemon = True
        self.model_thread.start()
        if SHOW_BASIC_LOGS:
            sys.stdout.write('ASR node start up.\n')
            sys.stdout.write('=====================================================\n')

    def update_audio(self, audio_np):
        import time
        import hashlib
        from datetime import datetime
        
        # ASRã§ã®å—ä¿¡æ™‚åˆ»ã‚’è¨˜éŒ²
        asr_receive_timestamp = time.time()
        
        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¸€æ„IDã‚’ç”Ÿæˆï¼ˆspeechInput.pyã¨åŒã˜ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰
        if len(audio_np) >= 10:
            sample_data = audio_np[:10].astype(np.float32).tobytes()
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¯ä½¿ã‚ãšã€éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§IDã‚’ç”Ÿæˆï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿ã¨ã®ä¸€è‡´ã‚’ä¿è¨¼ï¼‰
            audio_id = hashlib.md5(sample_data).hexdigest()[:8]
        else:
            audio_id = hashlib.md5(audio_np.astype(np.float32).tobytes()).hexdigest()[:8]
        
        # ASRéŸ³å£°å—ä¿¡ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ï¼‰
        timestamp_str = datetime.now().strftime("%H:%M:%S.%f")[:-3]

        # ãƒ‡ãƒ¼ã‚¿å¯¾å¿œç¢ºèªç”¨: å…ˆé ­5ã‚µãƒ³ãƒ—ãƒ«ã®å…·ä½“çš„ãªå€¤ã‚’è¡¨ç¤º
        sample_values = audio_np[:5].tolist() if len(audio_np) >= 5 else audio_np.tolist()
        sample_str = '[' + ','.join([f'{v:.6f}' for v in sample_values]) + ']'

        # sys.stdout.write(f"[ğŸ§  ASR_RECEIVE] {timestamp_str} | AudioID:{audio_id} | å—ä¿¡æ•°:{self.recv_count} | ã‚µãƒ³ãƒ—ãƒ«:{len(audio_np)}\n")
        # sys.stdout.flush()
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãã§ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
        audio_metadata = {
            'audio_data': audio_np,
            'audio_id': audio_id,
            'asr_receive_timestamp': asr_receive_timestamp,
            'sample_count': len(audio_np)
        }
        
        self.audio_queue.put(audio_metadata)
        self.recv_count += 1

    def pubASR(self):
        if self.new_result:
            self.new_result = False
            # â˜…ãƒ‡ãƒãƒƒã‚°: pubASR() ã§è¿”ã•ã‚Œã‚‹çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
            if SHOW_DEBUG_LOGS:
                if not hasattr(self, 'pubASR_call_count'):
                    self.pubASR_call_count = 0
                self.pubASR_call_count += 1
                if self.pubASR_call_count % 10 == 0 or len(self.word) > 0:  # èªè­˜çµæœãŒã‚ã‚‹å ´åˆã¯æ¯å›è¡¨ç¤º
                    from datetime import datetime
                    timestamp_str = datetime.now().strftime("%H:%M:%S.%f")[:-3]
                    sys.stdout.write(f"[{timestamp_str}][DEBUG-pubASR] Returning result: '{self.word}' (len={len(self.word)}), is_final={self.is_final}\n")
                    sys.stdout.flush()
            return {"you": self.word, "is_final": self.is_final}
        else:
            return None

    def run(self):
        while self.running:
            time.sleep(0.1)

    def recognition_thread(self):
        if SHOW_BASIC_LOGS:
            sys.stdout.write('Loading ASR model...\n')
        self.tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(MODEL_ID)
        self.processor = Wav2Vec2Processor.from_pretrained(MODEL_ID, tokenizer=self.tokenizer)
        self.model = AutoModelForCTC.from_pretrained(MODEL_ID)
        self.model.eval()
        if USE_GPU and torch.cuda.is_available():
            device = torch.device("cuda")
            self.model.to(device)
        else:
            device = torch.device("cpu")
        if SHOW_BASIC_LOGS:
            sys.stdout.write('ASR model loaded.\n')
            sys.stdout.flush()

        mic_input = np.array([], dtype=np.float32)  # ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ç”¨ãƒãƒƒãƒ•ã‚¡ï¼ˆ5ç§’åˆ†ã®éŸ³å£°ã‚’ä¿æŒï¼‰
        last_sent = ""
        start_time = time.time()
        last_inference_time = time.time()  # å‰å›æ¨è«–å®Ÿè¡Œæ™‚åˆ»
        new_audio_samples = 0  # å‰å›æ¨è«–ä»¥é™ã®æ–°ã—ã„éŸ³å£°ã‚µãƒ³ãƒ—ãƒ«æ•°
        
        try:
            # sys.stdout.write('[DEBUG] ASR recognition_thread started\n')
            # sys.stdout.flush()
            while self.running:
                # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ°—ã«å…¨ã¦èª­ã¿è¾¼ã¿
                audio_metadata_list = []  # éŸ³å£°ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¨˜éŒ²
                new_data_found = False
                
                # ã‚­ãƒ¥ãƒ¼ãŒç©ºã«ãªã‚‹ã¾ã§å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                batch_count = 0
                while not self.audio_queue.empty():
                    try:
                        queue_item = self.audio_queue.get_nowait()
                        process_timestamp = time.time()
                        
                        if isinstance(queue_item, dict) and 'audio_data' in queue_item:
                            # æ–°ã—ã„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ããƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                            data = queue_item['audio_data']
                            audio_id = queue_item['audio_id']
                            asr_receive_timestamp = queue_item['asr_receive_timestamp']
                            sample_count = queue_item['sample_count']
                            
                            audio_metadata_list.append({
                                'audio_id': audio_id,
                                'asr_receive_timestamp': asr_receive_timestamp,
                                'process_timestamp': process_timestamp,
                                'sample_count': sample_count
                            })
                            
                        elif isinstance(queue_item, tuple):
                            # æ—§å½¢å¼ï¼ˆã‚¿ãƒ—ãƒ«ï¼‰
                            data, timestamp = queue_item
                            data_id = f"{data[0]:.6f},{data[1]:.6f},{data[2]:.6f}" if len(data) >= 3 else "short_data"
                            audio_metadata_list.append({
                                'audio_id': data_id,
                                'asr_receive_timestamp': timestamp,
                                'process_timestamp': process_timestamp,
                                'sample_count': len(data)
                            })
                        else:
                            # å¤ã„å½¢å¼ã¸ã®å¯¾å¿œ
                            data = queue_item
                            data_id = f"{data[0]:.6f},{data[1]:.6f},{data[2]:.6f}" if len(data) >= 3 else "short_data"
                            audio_metadata_list.append({
                                'audio_id': data_id,
                                'asr_receive_timestamp': process_timestamp,
                                'process_timestamp': process_timestamp,
                                'sample_count': len(data)
                            })
                        
                        # æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ãƒ­ã‚°å‡ºåŠ›ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
                        # if batch_count == 0:
                        #     from datetime import datetime
                        #     timestamp_str = datetime.now().strftime("%H:%M:%S.%f")[:-3]
                        #     if audio_metadata_list:
                        #         metadata = audio_metadata_list[-1]
                        #         latency_ms = (process_timestamp - metadata['asr_receive_timestamp']) * 1000
                        #         
                        #         # ãƒ‡ãƒ¼ã‚¿å¯¾å¿œç¢ºèªç”¨: å…ˆé ­5ã‚µãƒ³ãƒ—ãƒ«ã®å…·ä½“çš„ãªå€¤ã‚’è¡¨ç¤º
                        #         sample_values = data[:5].tolist() if len(data) >= 5 else data.tolist()
                        #         sample_str = '[' + ','.join([f'{v:.6f}' for v in sample_values]) + ']'
                        #         
                        #         # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆå€¤ã‚‚è¿½åŠ 
                        #         rms = np.sqrt(np.mean(data**2))
                        #         max_val = np.max(np.abs(data))
                        #         
                        #         sys.stdout.write(f"[âš™ï¸ ASR_PROCESS] {timestamp_str} | AudioID:{metadata['audio_id']} | ProcessLatency:{latency_ms:.2f}ms | Batch:{batch_count+1} | Data:{sample_str} | RMS:{rms:.6f} | Max:{max_val:.6f}\n")
                        #         sys.stdout.flush()
                        
                        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¿ãƒƒã‚¯ã«è¿½åŠ 
                        mic_input = np.append(mic_input, data)
                        new_audio_samples += len(data)
                        new_data_found = True
                        batch_count += 1
                        
                    except queue.Empty:
                        break
                
                # ãƒ‡ãƒãƒƒã‚°ç”¨: ãƒãƒƒãƒå‡¦ç†æƒ…å ±ã‚’è¡¨ç¤ºï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
                # if batch_count > 0:
                #     sys.stdout.write(f"[ğŸ”„ SDS_BATCH] Processed {batch_count} chunks, new_audio: {new_audio_samples}samples\n")
                #     sys.stdout.flush()
                
                # 5ç§’ã‚’è¶…ãˆãŸåˆ†ã¯å¤ã„ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‰Šé™¤ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼‰
                if len(mic_input) > INPUT_LEN:
                    removed_samples = len(mic_input) - INPUT_LEN
                    mic_input = mic_input[-INPUT_LEN:]
                    # ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ã¯æ–°ã—ã„éŸ³å£°ã‚«ã‚¦ãƒ³ãƒˆã¯ãƒªã‚»ãƒƒãƒˆã—ãªã„
                    # new_audio_samplesã¯æ¨è«–å®Ÿè¡Œæ™‚ã®ã¿ãƒªã‚»ãƒƒãƒˆã™ã‚‹
                
                # æ¨è«–å®Ÿè¡Œæ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
                current_time = time.time()
                should_run_inference = (
                    len(mic_input) >= int(SAMPLE_RATE * 0.1) and  # æœ€ä½100msåˆ†ã®ãƒ‡ãƒ¼ã‚¿
                    new_audio_samples >= int(SAMPLE_RATE * 0.1)   # å‰å›æ¨è«–ã‹ã‚‰100msä»¥ä¸Šã®æ–°ã—ã„éŸ³å£°
                )
                
                # ãƒ‡ãƒãƒƒã‚°ç”¨: æ¨è«–æ¡ä»¶ã‚’å®šæœŸçš„ã«è¡¨ç¤º
                if hasattr(self, 'debug_count'):
                    self.debug_count += 1
                else:
                    self.debug_count = 0

                if self.debug_count % 50 == 0:  # 50å›ã«1å›è¡¨ç¤º
                    if SHOW_DEBUG_LOGS:
                        from datetime import datetime
                        timestamp_str = datetime.now().strftime("%H:%M:%S.%f")[:-3]
                        min_data = int(SAMPLE_RATE * 0.1)
                        min_new = int(SAMPLE_RATE * 0.1)
                        sys.stdout.write(f"[{timestamp_str}][ASR_INFER_CONDITION] mic:{len(mic_input)}samples (min:{min_data}) | new:{new_audio_samples}samples (min:{min_new}) | should_infer:{should_run_inference} | queue:{self.audio_queue.qsize()}\n")
                        sys.stdout.flush()
                
                # æ¨è«–æ¡ä»¶ã«è¿‘ã¥ã„ãŸæ™‚ã‚‚è¡¨ç¤º
                # if new_audio_samples >= 1400:  # 1600ã«è¿‘ã¥ã„ãŸæ™‚
                #     sys.stdout.write(f"[DEBUG] Almost ready for inference: new_audio: {new_audio_samples}/1600 samples\n")
                #     sys.stdout.flush()
                
                if should_run_inference:
                    try:
                        # ASRæ¨è«–é–‹å§‹æ™‚åˆ»
                        inference_start_time = time.time()
                        from datetime import datetime
                        timestamp_str = datetime.now().strftime("%H:%M:%S.%f")[:-3]
                        if SHOW_DEBUG_LOGS:
                            sys.stdout.write(f"[{timestamp_str}][ASR_INFERENCE_START] mic:{len(mic_input)}samples | new_audio:{new_audio_samples}samples\n")
                            sys.stdout.flush()

                        # æœ€æ–°5ç§’åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã§æ¨è«–ï¼ˆã™ã§ã«mic_inputã«è“„ç©æ¸ˆã¿ï¼‰
                        inference_data = mic_input[-int(5 * SAMPLE_RATE):] if len(mic_input) >= int(5 * SAMPLE_RATE) else mic_input

                        array = inference_data.astype(np.float32)
                        inputs = self.processor(array, sampling_rate=SAMPLE_RATE, return_tensors="pt", padding=True)
                        if USE_GPU and torch.cuda.is_available():
                            inputs = {k: v.to(device) for k, v in inputs.items()}
                            self.model = self.model.to(device)
                        with torch.no_grad():
                            logits = self.model(**inputs).logits
                        predicted_ids = torch.argmax(logits, dim=-1)
                        sentence = self.processor.batch_decode(predicted_ids)[0]

                        # ASRæ¨è«–å®Œäº†æ™‚åˆ»ã¨å‡¦ç†æ™‚é–“è¨ˆç®—
                        inference_end_time = time.time()
                        inference_duration_ms = (inference_end_time - inference_start_time) * 1000

                        # éŸ³å£°èªè­˜çµæœã‚’æ¨™æº–å‡ºåŠ›ï¼ˆæ¯å›å‡ºåŠ›ï¼‰
                        from datetime import datetime
                        timestamp_str = datetime.now().strftime("%H:%M:%S.%f")[:-3]

                        if audio_metadata_list:
                            # æœ€ã‚‚å¤ã„éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®é…å»¶è¨ˆç®—
                            oldest_metadata = min(audio_metadata_list, key=lambda x: x['asr_receive_timestamp'])
                            total_latency_ms = (inference_end_time - oldest_metadata['asr_receive_timestamp']) * 1000

                            # é–¢é€£ã™ã‚‹éŸ³å£°IDã‚’åé›†
                            audio_ids = [metadata['audio_id'] for metadata in audio_metadata_list]
                            audio_ids_str = ','.join(audio_ids[:3]) if len(audio_ids) <= 3 else f"{','.join(audio_ids[:2])}...(+{len(audio_ids)-2})"

                            # sys.stdout.write(f"[ğŸ’¬ ASR_INFERENCE] {timestamp_str} | æ¨è«–æ™‚é–“:{inference_duration_ms:.1f}ms | ç·é…å»¶:{total_latency_ms:.1f}ms | IDs:[{audio_ids_str}] | æ–°éŸ³å£°:{new_audio_samples}samples | èªè­˜:'{sentence}'\n")
                            # sys.stdout.flush()
                        else:
                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                            # sys.stdout.write(f"[ğŸ’¬ ASR_INFERENCE] {timestamp_str} | æ¨è«–æ™‚é–“:{inference_duration_ms:.1f}ms | æ–°éŸ³å£°:{new_audio_samples}samples | èªè­˜:'{sentence}'\n")
                            # sys.stdout.flush()
                            pass

                        # æ¨è«–å®Ÿè¡Œå¾Œã®çŠ¶æ…‹æ›´æ–°
                        last_inference_time = inference_end_time
                        new_audio_samples = 0  # æ–°ã—ã„éŸ³å£°ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ

                        now = time.time()
                        elapsed_time = now - start_time
                        diff = create_diff_list(last_sent, sentence)
                        colored = apply_color_to_diff(diff)

                        self.word = sentence
                        self.is_final = True
                        self.new_result = True  # è¿½åŠ : æ–°ã—ã„èªè­˜çµæœãŒå¾—ã‚‰ã‚ŒãŸ
                        last_sent = sentence

                        if SHOW_BASIC_LOGS:
                            sys.stdout.write(f"[{timestamp_str}][ASR_INFERENCE_COMPLETE] èªè­˜çµæœ: '{sentence}' (é…å»¶:{inference_duration_ms:.1f}ms)\n")
                            sys.stdout.flush()

                    except Exception as e:
                        if SHOW_BASIC_LOGS:
                            from datetime import datetime
                            timestamp_str = datetime.now().strftime("%H:%M:%S.%f")[:-3]
                            sys.stdout.write(f"[{timestamp_str}][ERROR-ASR_INFERENCE] {type(e).__name__}: {str(e)}\n")
                            import traceback
                            sys.stdout.write(f"[{timestamp_str}][ERROR-TRACEBACK]\n{traceback.format_exc()}\n")
                            sys.stdout.flush()
                
                # ã‚­ãƒ¥ãƒ¼ãŒç©ºã®å ´åˆã¯å°‘ã—å¾…æ©Ÿ
                if not new_data_found:
                    time.sleep(0.01)  # 10mså¾…æ©Ÿã§CPUè² è·è»½æ¸›
        except Exception as e:
            if SHOW_BASIC_LOGS:
                sys.stdout.write(f"[ERROR] ASR recognition_thread crashed: {e}\n")
                sys.stdout.write(f"[ERROR] Queue size at crash: {self.audio_queue.qsize()}\n")
                import traceback
                sys.stdout.write(f"[ERROR] Traceback: {traceback.format_exc()}\n")
                sys.stdout.flush()
            raise


