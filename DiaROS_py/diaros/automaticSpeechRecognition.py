# import time
# import sys
# import threading
# import queue
# import numpy as np

# import torch
# from transformers import AutoModelForCTC, Wav2Vec2Processor, Wav2Vec2CTCTokenizer
# from transformers.utils import logging
# import difflib
# import warnings

# # Audio recording parameters
# SAMPLE_RATE = 16000
# CHUNK_SIZE = 240
# MODEL_ID = 'SiRoZaRuPa/wav2vec2-base-kanji-unigram-RS-s-1120'
# AUDIO_DURATION = 5  # seconds
# INPUT_LEN = int(SAMPLE_RATE * AUDIO_DURATION)
# USE_GPU = True

# logging.set_verbosity_error()
# warnings.filterwarnings('ignore')

# def create_diff_list(old, new):
#     diff = list(difflib.ndiff(old, new))
#     lines = []
#     current_text = ""
#     is_change = False
#     for i in diff:
#         if i[0] == ' ':
#             if is_change:
#                 lines.append((1, current_text))
#                 current_text = ""
#             is_change = False
#             current_text += i[2:]
#         elif i[0] == '-':
#             continue
#         elif i[0] == '+':
#             if not is_change:
#                 if current_text:
#                     lines.append((0, current_text))
#                     current_text = ""
#                 is_change = True
#             current_text += i[2:]
#     if current_text:
#         lines.append((is_change, current_text))
#     return lines

# def apply_color_to_diff(lines, end_string=']'):
#     result = ""
#     for is_change, text in lines:
#         if is_change:
#             if lines[-1] == (1, text):
#                 if end_string in text and text.endswith(end_string):
#                     if text[-3] == 'é›‘':
#                         result += f'\033[91m{text[:-4]}\033[0m' + f'\033[42m{text[-4:]}\033[0m'
#                     elif text[-3] == 'ç„¡':
#                         result += f'\033[91m{text[:-4]}\033[0m' + f'\033[44m{text[-4:]}\033[0m'
#                 else:
#                     result += f'\033[91m{text}\033[0m'
#             else:
#                 result += f'\033[93m{text}\033[0m'
#         else:
#             result += text
#     return result

# class AutomaticSpeechRecognition:
#     def __init__(self):
#         self.last_audio = None
#         self.word = ""
#         self.is_final = False
#         self.recv_count = 0
#         self.audio_buffer = np.array([], dtype=np.float32)
#         self.audio_queue = queue.Queue()
#         self.running = True
#         self.last_sent = ""
#         self.model = None
#         self.processor = None
#         self.tokenizer = None
#         self.model_thread = threading.Thread(target=self.recognition_thread)
#         self.model_thread.daemon = True
#         self.model_thread.start()
#         sys.stdout.write('ASR node start up.\n')
#         sys.stdout.write('=====================================================\n')

#     def update_audio(self, audio_np):
#         self.audio_queue.put(audio_np)
#         self.recv_count += 1

#     def pubASR(self):
#         return {"you": self.word, "is_final": self.is_final}

#     def run(self):
#         while self.running:
#             time.sleep(0.1)

#     def recognition_thread(self):
#         sys.stdout.write('Loading ASR model...\n')
#         self.tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(MODEL_ID)
#         self.processor = Wav2Vec2Processor.from_pretrained(MODEL_ID, tokenizer=self.tokenizer)
#         self.model = AutoModelForCTC.from_pretrained(MODEL_ID)
#         self.model.eval()
#         if USE_GPU and torch.cuda.is_available():
#             device = torch.device("cuda")
#             self.model.to(device)
#         else:
#             device = torch.device("cpu")
#         sys.stdout.write('ASR model loaded.\n')
#         sys.stdout.flush()

#         mic_input = np.array([], dtype=np.float32)
#         last_sent = ""
#         start_time = time.time()
#         last_time = time.time()
#         last_infer_len = 0  # å‰å›æ¨è«–æ™‚ã®mic_inputã®é•·ã•
#         try:
#             while self.running:
#                 new_data_added = False
#                 while not self.audio_queue.empty():
#                     data = self.audio_queue.get()
#                     mic_input = np.append(mic_input, data)
#                     new_data_added = True
#                 # 5ç§’ã‚’è¶…ãˆãŸã‚‰å¤ã„ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¨ã¦ã‚‹
#                 if len(mic_input) > INPUT_LEN:
#                     mic_input = mic_input[-INPUT_LEN:]
#                 # æ–°ãŸãªéŸ³å£°ãŒ100msåˆ†æºœã¾ã£ã¦ã„ãŸã‚‰æ¨è«–
#                 # ä¿®æ­£: last_infer_lenã®æ›´æ–°ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’æ¨è«–å¾Œã«ã—ã€æ¨è«–æ¡ä»¶ã‚’ã€Œæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒ100msåˆ†ä»¥ä¸Šæºœã¾ã£ã¦ã„ã‚‹å ´åˆã€ã«é™å®š
#                 if len(mic_input) >= int(SAMPLE_RATE * 0.1) and (len(mic_input) - last_infer_len >= int(SAMPLE_RATE * 0.1)):
#                     print(f"Received audio data length: {len(mic_input)}")
#                     sys.stdout.flush()
#                     array = mic_input.astype(np.float32)
#                     inputs = self.processor(array, sampling_rate=SAMPLE_RATE, return_tensors="pt", padding=True)
#                     if USE_GPU and torch.cuda.is_available():
#                         inputs = {k: v.to(device) for k, v in inputs.items()}
#                         self.model = self.model.to(device)
#                     with torch.no_grad():
#                         logits = self.model(**inputs).logits
#                     predicted_ids = torch.argmax(logits, dim=-1)
#                     sentence = self.processor.batch_decode(predicted_ids)[0]
#                     now = time.time()
#                     elapsed_time = now - start_time
#                     process_time = int(1000 * (now - last_time))
#                     last_time = now
#                     diff = create_diff_list(last_sent, sentence)
#                     colored = apply_color_to_diff(diff)
#                     output = f'{elapsed_time:7.3f} ({process_time:5d} ms): {colored}'
#                     if last_sent != sentence:
#                         print(output)
#                     else:
#                         sys.stdout.write("\r" + output + " " * 20 + "\r")
#                         sys.stdout.flush()
#                     self.word = sentence
#                     self.is_final = True if sentence.strip() != "" else False
#                     last_sent = sentence
#                     last_infer_len = len(mic_input)  # æ¨è«–å¾Œã«æ›´æ–°
#                 time.sleep(0.01)  # ãƒ«ãƒ¼ãƒ—ãŒé«˜é€Ÿã™ãã‚‹å ´åˆã®CPUè² è·è»½æ¸›
#         except Exception as e:
#             print(f"Error in recognition_thread: {e}")

# NOTE éŸ³å£°å…¥åŠ›é•·å›ºå®š
import time
import sys
import threading
import queue
import numpy as np

import torch
from transformers import AutoModelForCTC, Wav2Vec2Processor, Wav2Vec2CTCTokenizer
from transformers.utils import logging
import difflib
import warnings

# Audio recording parameters
SAMPLE_RATE = 16000
CHUNK_SIZE = 240
# MODEL_ID = 'SiRoZaRuPa/wav2vec2-base-kanji-unigram-RS-s-1120'
MODEL_ID = 'SiRoZaRuPa/japanese-HuBERT-base-VADLess-ASR-RSm'
AUDIO_DURATION = 5  # seconds
INPUT_LEN = int(SAMPLE_RATE * AUDIO_DURATION)
USE_GPU = True

logging.set_verbosity_error()
warnings.filterwarnings('ignore')

def create_diff_list(old, new):
    diff = list(difflib.ndiff(old, new))
    lines = []
    current_text = ""
    is_change = False
    for i in diff:
        if i[0] == ' ':
            if is_change:
                lines.append((1, current_text))
                current_text = ""
            is_change = False
            current_text += i[2:]
        elif i[0] == '-':
            continue
        elif i[0] == '+':
            if not is_change:
                if current_text:
                    lines.append((0, current_text))
                    current_text = ""
                is_change = True
            current_text += i[2:]
    if current_text:
        lines.append((is_change, current_text))
    return lines

def apply_color_to_diff(lines, end_string=']'):
    result = ""
    for is_change, text in lines:
        if is_change:
            if lines[-1] == (1, text):
                if end_string in text and text.endswith(end_string):
                    if text[-3] == 'é›‘':
                        result += f'\033[91m{text[:-4]}\033[0m' + f'\033[42m{text[-4:]}\033[0m'
                    elif text[-3] == 'ç„¡':
                        result += f'\033[91m{text[:-4]}\033[0m' + f'\033[44m{text[-4:]}\033[0m'
                else:
                    result += f'\033[91m{text}\033[0m'
            else:
                result += f'\033[93m{text}\033[0m'
        else:
            result += text
    return result

class AutomaticSpeechRecognition:
    def __init__(self):
        self.last_audio = None
        self.word = ""
        self.is_final = False
        self.recv_count = 0
        self.audio_buffer = np.array([], dtype=np.float32)
        self.audio_queue = queue.Queue()
        self.running = True
        self.last_sent = ""
        self.model = None
        self.processor = None
        self.tokenizer = None
        self.new_result = False  # è¿½åŠ : æ–°ã—ã„èªè­˜çµæœãƒ•ãƒ©ã‚°
        self.model_thread = threading.Thread(target=self.recognition_thread)
        self.model_thread.daemon = True
        self.model_thread.start()
        sys.stdout.write('ASR node start up.\n')
        sys.stdout.write('=====================================================\n')

    def update_audio(self, audio_np):
        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿å—ä¿¡æ™‚åˆ»ã‚’è¨˜éŒ²
        import time
        from datetime import datetime
        receive_timestamp = time.time()
        
        # ãƒ‡ãƒ¼ã‚¿è¿½è·¡ç”¨: å…ˆé ­3ã‚µãƒ³ãƒ—ãƒ«ã®å€¤ã§ãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å®š
        data_id = f"{audio_np[0]:.6f},{audio_np[1]:.6f},{audio_np[2]:.6f}" if len(audio_np) >= 3 else "short_data"
        # 10å›ã«1å›ã®ã¿ãƒ­ã‚°å‡ºåŠ›ï¼ˆç°¡ç•¥åŒ–ï¼‰
        if not hasattr(self, 'log_counter'):
            self.log_counter = 0
        self.log_counter += 1
        if self.log_counter % 10 == 0:
            timestamp_str = datetime.now().strftime("%H:%M:%S.%f")[:-3]
            sys.stdout.write(f"[ğŸ“¥ SDS_ASR_QUEUE] {timestamp_str} | ID:{data_id} | T:{receive_timestamp:.6f}\n")
            sys.stdout.flush()
        
        self.audio_queue.put((audio_np, receive_timestamp))
        self.recv_count += 1

    def pubASR(self):
        if self.new_result:
            self.new_result = False
            return {"you": self.word, "is_final": self.is_final}
        else:
            return None

    def run(self):
        while self.running:
            time.sleep(0.1)

    def recognition_thread(self):
        sys.stdout.write('Loading ASR model...\n')
        self.tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(MODEL_ID)
        self.processor = Wav2Vec2Processor.from_pretrained(MODEL_ID, tokenizer=self.tokenizer)
        self.model = AutoModelForCTC.from_pretrained(MODEL_ID)
        self.model.eval()
        if USE_GPU and torch.cuda.is_available():
            device = torch.device("cuda")
            self.model.to(device)
        else:
            device = torch.device("cpu")
        sys.stdout.write('ASR model loaded.\n')
        sys.stdout.flush()

        mic_input = np.array([], dtype=np.float32)  # ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ç”¨ãƒãƒƒãƒ•ã‚¡ï¼ˆ5ç§’åˆ†ã®éŸ³å£°ã‚’ä¿æŒï¼‰
        last_sent = ""
        start_time = time.time()
        last_inference_time = time.time()  # å‰å›æ¨è«–å®Ÿè¡Œæ™‚åˆ»
        new_audio_samples = 0  # å‰å›æ¨è«–ä»¥é™ã®æ–°ã—ã„éŸ³å£°ã‚µãƒ³ãƒ—ãƒ«æ•°
        
        try:
            # sys.stdout.write('[DEBUG] ASR recognition_thread started\n')
            # sys.stdout.flush()
            while self.running:
                # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ°—ã«å…¨ã¦èª­ã¿è¾¼ã¿
                audio_timestamps = []  # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¨˜éŒ²
                new_data_found = False
                
                # ã‚­ãƒ¥ãƒ¼ãŒç©ºã«ãªã‚‹ã¾ã§å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                batch_count = 0
                while not self.audio_queue.empty():
                    try:
                        data_with_timestamp = self.audio_queue.get_nowait()
                        process_timestamp = time.time()
                        
                        if isinstance(data_with_timestamp, tuple):
                            data, timestamp = data_with_timestamp
                            audio_timestamps.append(timestamp)
                        else:
                            # å¤ã„å½¢å¼ã¸ã®å¯¾å¿œ
                            data = data_with_timestamp
                            audio_timestamps.append(time.time())
                        
                        # ãƒ‡ãƒ¼ã‚¿è¿½è·¡ç”¨: å…ˆé ­3ã‚µãƒ³ãƒ—ãƒ«ã®å€¤ã§ãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å®šã¨ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å–å¾—ã®ãƒ­ã‚°ï¼ˆæœ€åˆã®1å€‹ã®ã¿ï¼‰
                        if batch_count == 0:  # æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ãƒ­ã‚°å‡ºåŠ›
                            data_id = f"{data[0]:.6f},{data[1]:.6f},{data[2]:.6f}" if len(data) >= 3 else "short_data"
                            from datetime import datetime
                            timestamp_str = datetime.now().strftime("%H:%M:%S.%f")[:-3]
                            sys.stdout.write(f"[ğŸ”„ SDS_ASR_PROCESS] {timestamp_str} | ID:{data_id} | T:{process_timestamp:.6f} | batch:{batch_count+1}\n")
                            sys.stdout.flush()
                        
                        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¿ãƒƒã‚¯ã«è¿½åŠ 
                        mic_input = np.append(mic_input, data)
                        new_audio_samples += len(data)
                        new_data_found = True
                        batch_count += 1
                        
                    except queue.Empty:
                        break
                
                # ãƒ‡ãƒãƒƒã‚°ç”¨: ãƒãƒƒãƒå‡¦ç†æƒ…å ±ã‚’è¡¨ç¤º
                if batch_count > 0:
                    sys.stdout.write(f"[ğŸ”„ SDS_BATCH] Processed {batch_count} chunks, new_audio: {new_audio_samples}samples\n")
                    sys.stdout.flush()
                
                # 5ç§’ã‚’è¶…ãˆãŸåˆ†ã¯å¤ã„ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‰Šé™¤ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼‰
                if len(mic_input) > INPUT_LEN:
                    removed_samples = len(mic_input) - INPUT_LEN
                    mic_input = mic_input[-INPUT_LEN:]
                    # ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ã¯æ–°ã—ã„éŸ³å£°ã‚«ã‚¦ãƒ³ãƒˆã¯ãƒªã‚»ãƒƒãƒˆã—ãªã„
                    # new_audio_samplesã¯æ¨è«–å®Ÿè¡Œæ™‚ã®ã¿ãƒªã‚»ãƒƒãƒˆã™ã‚‹
                
                # æ¨è«–å®Ÿè¡Œæ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
                current_time = time.time()
                should_run_inference = (
                    len(mic_input) >= int(SAMPLE_RATE * 0.1) and  # æœ€ä½100msåˆ†ã®ãƒ‡ãƒ¼ã‚¿
                    new_audio_samples >= int(SAMPLE_RATE * 0.1)   # å‰å›æ¨è«–ã‹ã‚‰100msä»¥ä¸Šã®æ–°ã—ã„éŸ³å£°
                )
                
                # ãƒ‡ãƒãƒƒã‚°ç”¨: æ¨è«–æ¡ä»¶ã‚’å®šæœŸçš„ã«è¡¨ç¤º
                if hasattr(self, 'debug_count'):
                    self.debug_count += 1
                else:
                    self.debug_count = 0
                
                # if self.debug_count % 1000 == 0:  # 1000å›ã«1å›è¡¨ç¤º
                #     sys.stdout.write(f"[DEBUG] mic_input: {len(mic_input)}samples, new_audio: {new_audio_samples}samples, should_run: {should_run_inference}\n")
                #     sys.stdout.flush()
                
                # æ¨è«–æ¡ä»¶ã«è¿‘ã¥ã„ãŸæ™‚ã‚‚è¡¨ç¤º
                # if new_audio_samples >= 1400:  # 1600ã«è¿‘ã¥ã„ãŸæ™‚
                #     sys.stdout.write(f"[DEBUG] Almost ready for inference: new_audio: {new_audio_samples}/1600 samples\n")
                #     sys.stdout.flush()
                
                if should_run_inference:
                    # ASRæ¨è«–é–‹å§‹æ™‚åˆ»
                    inference_start_time = time.time()
                    
                    # æœ€æ–°5ç§’åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã§æ¨è«–ï¼ˆã™ã§ã«mic_inputã«è“„ç©æ¸ˆã¿ï¼‰
                    inference_data = mic_input[-int(5 * SAMPLE_RATE):] if len(mic_input) >= int(5 * SAMPLE_RATE) else mic_input
                    
                    array = inference_data.astype(np.float32)
                    inputs = self.processor(array, sampling_rate=SAMPLE_RATE, return_tensors="pt", padding=True)
                    if USE_GPU and torch.cuda.is_available():
                        inputs = {k: v.to(device) for k, v in inputs.items()}
                        self.model = self.model.to(device)
                    with torch.no_grad():
                        logits = self.model(**inputs).logits
                    predicted_ids = torch.argmax(logits, dim=-1)
                    sentence = self.processor.batch_decode(predicted_ids)[0]
                    
                    # ASRæ¨è«–å®Œäº†æ™‚åˆ»ã¨å‡¦ç†æ™‚é–“è¨ˆç®—
                    inference_end_time = time.time()
                    inference_duration_ms = (inference_end_time - inference_start_time) * 1000
                    
                    # æœ€ã‚‚å¤ã„éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®é…å»¶è¨ˆç®—
                    if audio_timestamps:
                        oldest_audio_timestamp = min(audio_timestamps)
                        total_latency_ms = (inference_end_time - oldest_audio_timestamp) * 1000
                        from datetime import datetime
                        timestamp_str = datetime.now().strftime("%H:%M:%S.%f")[:-3]
                        sys.stdout.write(f"[âš¡ ASR_INFERENCE] {timestamp_str} | æ¨è«–æ™‚é–“: {inference_duration_ms:.1f}ms | ç·é…å»¶: {total_latency_ms:.1f}ms | æ–°éŸ³å£°: {new_audio_samples}samples | èªè­˜: '{sentence}'\n")
                        sys.stdout.flush()
                    
                    # æ¨è«–å®Ÿè¡Œå¾Œã®çŠ¶æ…‹æ›´æ–°
                    last_inference_time = inference_end_time
                    new_audio_samples = 0  # æ–°ã—ã„éŸ³å£°ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
                    
                    now = time.time()
                    elapsed_time = now - start_time
                    diff = create_diff_list(last_sent, sentence)
                    colored = apply_color_to_diff(diff)
                    
                    self.word = sentence
                    self.is_final = True
                    self.new_result = True  # è¿½åŠ : æ–°ã—ã„èªè­˜çµæœãŒå¾—ã‚‰ã‚ŒãŸ
                    last_sent = sentence
                
                # ã‚­ãƒ¥ãƒ¼ãŒç©ºã®å ´åˆã¯å°‘ã—å¾…æ©Ÿ
                if not new_data_found:
                    time.sleep(0.01)  # 10mså¾…æ©Ÿã§CPUè² è·è»½æ¸›
        except Exception as e:
            sys.stdout.write(f"[ERROR] ASR recognition_thread crashed: {e}\n")
            sys.stdout.write(f"[ERROR] Queue size at crash: {self.audio_queue.qsize()}\n")
            import traceback
            sys.stdout.write(f"[ERROR] Traceback: {traceback.format_exc()}\n")
            sys.stdout.flush()
            raise


