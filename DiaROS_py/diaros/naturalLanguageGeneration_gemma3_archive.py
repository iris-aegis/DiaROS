# Gemma 3ãƒ¢ãƒ‡ãƒ«å°‚ç”¨ã®è‡ªç„¶è¨€èªç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 

import requests
import json
import sys
import os
import time
import threading
from datetime import datetime
from queue import Queue, Empty
import openai
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

class NaturalLanguageGeneration:
    def __init__(self):
        self.rc = { "word": "" }
        
        self.query = ""
        self.update_flag = False
        self.dialogue_history = []
        self.user_speak_is_final = False
        self.last_reply = ""  # ç”Ÿæˆã—ãŸå¯¾è©±æ–‡ã‚’ã“ã“ã«æ ¼ç´
        self.last_source_words = []  # å¯¾è©±ç”Ÿæˆã®å…ƒã«ã—ãŸéŸ³å£°èªè­˜çµæœã‚’æ ¼ç´
        
        # ä¸¦åˆ—æ¨è«–ç”¨ã®è¨­å®š
        self.inference_queue = Queue()  # æ¨è«–ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚­ãƒ¥ãƒ¼
        self.current_inference_start_time = None  # ç¾åœ¨ã®æ¨è«–é–‹å§‹æ™‚åˆ»
        self.inference_lock = threading.Lock()  # æ¨è«–çŠ¶æ…‹ç®¡ç†ç”¨ãƒ­ãƒƒã‚¯
        self.is_inferencing = False  # æ¨è«–ä¸­ãƒ•ãƒ©ã‚°
        self.latest_result = None  # æœ€æ–°ã®æ¨è«–çµæœ
        self.result_lock = threading.Lock()  # çµæœã‚¢ã‚¯ã‚»ã‚¹ç”¨ãƒ­ãƒƒã‚¯
        
        # Gemma 3ãƒ¢ãƒ‡ãƒ«å°‚ç”¨åˆæœŸåŒ–
        sys.stdout.write("[NLG] ğŸš€ Gemma 3ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...\n")
        sys.stdout.flush()
        
        # Gemma 3ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        if not self._init_gemma3_models():
            sys.stdout.write("[NLG] âŒ Gemma 3ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ\n")
            sys.exit(1)
        
        # GPUä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèª
        self._check_gpu_status()
        
        # æ¨è«–ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
        sys.stdout.write("[NLG] æ¨è«–ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹ä¸­...\n")
        self.worker_thread_1 = threading.Thread(target=self._inference_worker, args=(1,), daemon=True)
        self.worker_thread_2 = threading.Thread(target=self._inference_worker, args=(2,), daemon=True)
        self.worker_thread_1.start()
        self.worker_thread_2.start()
        sys.stdout.write("[NLG] âœ… æ¨è«–ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹å®Œäº†\n")

        sys.stdout.write('NaturalLanguageGeneration start up.\n')
        sys.stdout.write('=====================================================\n')
        # OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        self.openai_client = openai.OpenAI(
            api_key=os.environ.get("OPENAI_API_KEY")
        )

    def _check_gpu_status(self):
        """GPUä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèª"""
        try:
            import subprocess
            # nvidia-smiã§GPUä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèª
            result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.used,memory.free', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                gpu_info = result.stdout.strip()
                sys.stdout.write(f"[NLG] ğŸ–¥ï¸  GPUçŠ¶æ³: {gpu_info}\n")
        except Exception as e:
            sys.stdout.write(f"[NLG] âš ï¸  GPUçŠ¶æ³ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}\n")
        sys.stdout.flush()

    def _log_gpu_memory_usage(self, context):
        """GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        try:
            import subprocess
            result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True, timeout=2)
            if result.returncode == 0:
                memory_info = result.stdout.strip()
                if memory_info:
                    used, total = memory_info.split(', ')
                    usage_percent = (int(used) / int(total)) * 100
                    timestamp = datetime.now().strftime('%H:%M:%S.%f')[:-3]
                    sys.stdout.write(f"[{timestamp}][NLG] ğŸ–¥ï¸  {context}: GPUä½¿ç”¨ç‡ {usage_percent:.1f}% ({used}/{total}MB)\n")
                    sys.stdout.flush()
        except Exception:
            # GPUç›£è¦–ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆã‚µã‚¤ãƒ¬ãƒ³ãƒˆï¼‰
            pass

    def _check_model_completeness(self, model_path, model_size, expected_files):
        """ãƒ¢ãƒ‡ãƒ«ãŒå®Œå…¨ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            if not os.path.exists(model_path):
                return False
            
            # snapshotsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®safetensorsãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
            snapshots_dir = os.path.join(model_path, "snapshots")
            if not os.path.exists(snapshots_dir):
                return False
            
            # æœ€æ–°ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
            snapshot_dirs = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
            if not snapshot_dirs:
                return False
            
            latest_snapshot = os.path.join(snapshots_dir, snapshot_dirs[0])
            
            # safetensorsãƒ•ã‚¡ã‚¤ãƒ«ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            safetensors_files = [f for f in os.listdir(latest_snapshot) if f.endswith('.safetensors')]
            actual_files = len(safetensors_files)
            
            sys.stdout.write(f"[NLG] ğŸ“Š {model_size}ãƒ¢ãƒ‡ãƒ«: {actual_files}/{expected_files} ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨\n")
            
            if actual_files >= expected_files:
                return True
            else:
                sys.stdout.write(f"[NLG] âš ï¸  {model_size}ãƒ¢ãƒ‡ãƒ«ãŒä¸å®Œå…¨ã§ã™ ({actual_files}/{expected_files})\n")
                return False
                
        except Exception as e:
            sys.stdout.write(f"[NLG] âŒ {model_size}ãƒ¢ãƒ‡ãƒ«ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}\n")
            return False

    def _init_gemma3_models(self):
        """äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿Gemma 3ãƒ¢ãƒ‡ãƒ«ã‚’4ãƒ“ãƒƒãƒˆé‡å­åŒ–ã§èª­ã¿è¾¼ã¿"""
        try:
            import torch
            from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
            import os
            
            # GPUä½¿ç”¨é‡ç¢ºèª
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                sys.stdout.write(f"[NLG] ğŸ–¥ï¸  GPU: {gpu_count}å°, GPU0ãƒ¡ãƒ¢ãƒª: {total_memory:.1f}GB\n")
            else:
                sys.stdout.write("[NLG] âŒ CUDAå¯¾å¿œGPUãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\n")
                return False
            
            # äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª
            cache_dir = "/workspace/models"
            model_name_27b = "google/gemma-3-27b-it"
            model_name_12b = "google/gemma-3-12b-it"
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèª
            model_27b_path = os.path.join(cache_dir, "models--google--gemma-3-27b-it")
            model_12b_path = os.path.join(cache_dir, "models--google--gemma-3-12b-it")
            
            # ãƒ¢ãƒ‡ãƒ«å­˜åœ¨ç¢ºèªï¼ˆç°¡å˜ãªãƒ‘ã‚¹ãƒã‚§ãƒƒã‚¯ã®ã¿ï¼‰
            model_27b_exists = os.path.exists(model_27b_path) and os.path.exists(os.path.join(model_27b_path, "snapshots"))
            model_12b_exists = os.path.exists(model_12b_path) and os.path.exists(os.path.join(model_12b_path, "snapshots"))
            
            sys.stdout.write(f"[NLG] ğŸ“ äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ç¢ºèª:\n")
            sys.stdout.write(f"[NLG]   - Gemma 3 27B: {'âœ… å­˜åœ¨' if model_27b_exists else 'âŒ æœªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰'}\n")
            sys.stdout.write(f"[NLG]   - Gemma 3 12B: {'âœ… å­˜åœ¨' if model_12b_exists else 'âŒ æœªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰'}\n")
            
            # ä½¿ç”¨å¯èƒ½ãªGPUãƒ¡ãƒ¢ãƒªã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
            available_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            # ãƒ¢ãƒ‡ãƒ«é¸æŠãƒ­ã‚¸ãƒƒã‚¯ï¼ˆGPUãƒ¡ãƒ¢ãƒªã«åŸºã¥ãè‡ªå‹•é¸æŠï¼‰
            if available_memory >= 24 and model_27b_exists:
                model_name = model_name_27b
                sys.stdout.write(f"[NLG] ğŸ¯ Gemma 3 27Bãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨: {model_name} (GPU: {available_memory:.1f}GB)\n")
            elif model_12b_exists:
                model_name = model_name_12b
                sys.stdout.write(f"[NLG] ğŸ¯ Gemma 3 12Bãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨: {model_name} (GPU: {available_memory:.1f}GB)\n")
            else:
                sys.stdout.write("[NLG] âŒ åˆ©ç”¨å¯èƒ½ãªGemma 3ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\n")
                return False
            
            # GPUèƒ½åŠ›ã«å¿œã˜ãŸãƒ‡ãƒ¼ã‚¿å‹é¸æŠ
            if torch.cuda.is_available() and torch.cuda.is_bf16_supported():
                compute_dtype = torch.bfloat16
                sys.stdout.write("[NLG] ğŸ”§ bfloat16ã‚’ä½¿ç”¨ï¼ˆæ•°å€¤å®‰å®šæ€§æœ€é©åŒ–ï¼‰\n")
            else:
                compute_dtype = torch.float16
                sys.stdout.write("[NLG] ğŸ”§ float16ã‚’ä½¿ç”¨\n")
            
            # 4bité‡å­åŒ–è¨­å®šï¼ˆé€Ÿåº¦æœ€é©åŒ–ï¼‰
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=False,  # é€Ÿåº¦é‡è¦–ã®ãŸã‚ç„¡åŠ¹åŒ–
                bnb_4bit_compute_dtype=compute_dtype,
                llm_int8_threshold=6.0,
                llm_int8_has_fp16_weight=False,
                bnb_4bit_quant_storage=compute_dtype  # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æœ€é©åŒ–
            )
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã¿ä½¿ç”¨ï¼‰
            sys.stdout.write("[NLG] ğŸ“ äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...\n")
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    cache_dir=cache_dir,
                    local_files_only=True  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ä½¿ç”¨
                )
            except Exception as e:
                error_str = str(e)
                if "argument of type 'NoneType' is not iterable" in error_str:
                    sys.stdout.write("[NLG] âš ï¸  ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§NoneTypeã‚¨ãƒ©ãƒ¼ã€‚local_files_onlyã‚’ç„¡åŠ¹ã«ã—ã¦å†è©¦è¡Œä¸­...\n")
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        model_name,
                        cache_dir=cache_dir,
                        local_files_only=False  # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¥ç¶šã‚’è¨±å¯
                    )
                    sys.stdout.write("[NLG] âœ… å†è©¦è¡Œã§ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿æˆåŠŸ\n")
                else:
                    raise e
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã¿ä½¿ç”¨ï¼‰
            sys.stdout.write("[NLG] ğŸ¤– äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿Gemma 3 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...\n")
            try:
                # Flash Attention 2ã§ã®èª­ã¿è¾¼ã¿ã‚’è©¦è¡Œ
                try:
                    sys.stdout.write("[NLG] ğŸš€ Flash Attention 2ã§é«˜é€ŸåŒ–ãƒ¢ãƒ¼ãƒ‰ã‚’è©¦è¡Œä¸­...\n")
                    self.local_model = AutoModelForCausalLM.from_pretrained(
                        model_name,
                        quantization_config=quantization_config,
                        torch_dtype=compute_dtype,
                        device_map="auto",
                        cache_dir=cache_dir,
                        local_files_only=True,
                        trust_remote_code=True,
                        low_cpu_mem_usage=True,
                        use_flash_attention_2=True,  # Flash Attention 2ã§é«˜é€ŸåŒ–
                        attn_implementation="flash_attention_2"
                    )
                    sys.stdout.write("[NLG] âœ… Flash Attention 2ã«ã‚ˆã‚‹é«˜é€ŸåŒ–ã‚’é©ç”¨\n")
                except Exception as flash_error:
                    sys.stdout.write(f"[NLG] âš ï¸  Flash Attention 2ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {str(flash_error)}\n")
                    sys.stdout.write("[NLG] ğŸ”„ æ¨™æº–ãƒ¢ãƒ¼ãƒ‰ã§èª­ã¿è¾¼ã¿ä¸­...\n")
                    self.local_model = AutoModelForCausalLM.from_pretrained(
                        model_name,
                        quantization_config=quantization_config,
                        torch_dtype=compute_dtype,
                        device_map="auto",
                        cache_dir=cache_dir,
                        local_files_only=True,
                        trust_remote_code=True,
                        low_cpu_mem_usage=True
                    )
            except Exception as e:
                error_str = str(e)
                sys.stdout.write(f"[NLG] âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼è©³ç´°: {error_str}\n")
                
                if "argument of type 'NoneType' is not iterable" in error_str:
                    sys.stdout.write("[NLG] âš ï¸  NoneTypeã‚¨ãƒ©ãƒ¼ã‚’æ¤œå‡ºã€‚è¤‡æ•°ã®å›é¿ç­–ã‚’è©¦è¡Œä¸­...\n")
                    
                    # å›é¿ç­–1: local_files_onlyã‚’ç„¡åŠ¹ã«ã—ã¦å†è©¦è¡Œ
                    try:
                        sys.stdout.write("[NLG] å›é¿ç­–1: ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¥ç¶šã‚’è¨±å¯ã—ã¦å†è©¦è¡Œ...\n")
                        self.local_model = AutoModelForCausalLM.from_pretrained(
                            model_name,
                            quantization_config=quantization_config,
                            torch_dtype=compute_dtype,
                            device_map="auto",
                            cache_dir=cache_dir,
                            local_files_only=False,
                            trust_remote_code=True,
                            low_cpu_mem_usage=True
                        )
                        sys.stdout.write("[NLG] âœ… å›é¿ç­–1æˆåŠŸ: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†\n")
                    except Exception as e2:
                        sys.stdout.write(f"[NLG] âŒ å›é¿ç­–1å¤±æ•—: {str(e2)}\n")
                        
                        # å›é¿ç­–2: quantization_configã‚’ç„¡åŠ¹ã«ã—ã¦å†è©¦è¡Œ
                        try:
                            sys.stdout.write("[NLG] å›é¿ç­–2: é‡å­åŒ–ãªã—ã§å†è©¦è¡Œ...\n")
                            self.local_model = AutoModelForCausalLM.from_pretrained(
                                model_name,
                                torch_dtype=compute_dtype,
                                device_map="auto",
                                cache_dir=cache_dir,
                                local_files_only=False,
                                trust_remote_code=True,
                                low_cpu_mem_usage=True
                            )
                            sys.stdout.write("[NLG] âœ… å›é¿ç­–2æˆåŠŸ: é‡å­åŒ–ãªã—ã§ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†\n")
                        except Exception as e3:
                            sys.stdout.write(f"[NLG] âŒ å›é¿ç­–2å¤±æ•—: {str(e3)}\n")
                            
                            # å›é¿ç­–3: device_mapã‚’ç„¡åŠ¹ã«ã—ã¦å†è©¦è¡Œ
                            try:
                                sys.stdout.write("[NLG] å›é¿ç­–3: CPUèª­ã¿è¾¼ã¿ã§å†è©¦è¡Œ...\n")
                                self.local_model = AutoModelForCausalLM.from_pretrained(
                                    model_name,
                                    torch_dtype=compute_dtype,
                                    cache_dir=cache_dir,
                                    local_files_only=False,
                                    trust_remote_code=True,
                                    low_cpu_mem_usage=True
                                )
                                sys.stdout.write("[NLG] âœ… å›é¿ç­–3æˆåŠŸ: CPUã§ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†\n")
                                # CPUã‹ã‚‰GPUã«ç§»å‹•
                                if torch.cuda.is_available():
                                    sys.stdout.write("[NLG] GPUã«ç§»å‹•ä¸­...\n")
                                    self.local_model = self.local_model.to("cuda:0")
                                    sys.stdout.write("[NLG] âœ… GPUã«ç§»å‹•å®Œäº†\n")
                            except Exception as e4:
                                sys.stdout.write(f"[NLG] âŒ å…¨ã¦ã®å›é¿ç­–ãŒå¤±æ•—: {str(e4)}\n")
                                raise e4
                else:
                    raise e
            
            # ä¸¦åˆ—å®Ÿè¡Œç”¨ã®è¨­å®š
            self.model_lock_1 = threading.Lock()
            self.model_lock_2 = threading.Lock()
            
            # GPUä½¿ç”¨é‡ã‚’ç¢ºèª
            if torch.cuda.is_available():
                torch.cuda.empty_cache()  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
                allocated = torch.cuda.memory_allocated(0) / 1024**3
                cached = torch.cuda.memory_reserved(0) / 1024**3
                sys.stdout.write(f"[NLG] ğŸ–¥ï¸  Gemma 3 GPUä½¿ç”¨é‡: {allocated:.1f}GB (ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {cached:.1f}GB)\n")
            
            sys.stdout.write("[NLG] âœ… äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿Gemma 3ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†\n")
            return True
            
        except Exception as e:
            import traceback
            error_str = str(e)
            sys.stdout.write(f"[NLG] âŒ Gemma 3ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {error_str}\n")
            sys.stdout.write(f"[NLG] ğŸ” è©³ç´°ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:\n")
            traceback_str = traceback.format_exc()
            for line in traceback_str.split('\n'):
                if line.strip():
                    sys.stdout.write(f"[NLG] {line}\n")
            
            if "argument of type 'NoneType' is not iterable" in error_str:
                sys.stdout.write("[NLG] ğŸ’¡ NoneTypeã‚¨ãƒ©ãƒ¼ã¯transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æ—¢çŸ¥ã®å•é¡Œã§ã™\n")
                sys.stdout.write("[NLG] ğŸ”§ transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æ›´æ–°ã‚’æ¨å¥¨ã—ã¾ã™: pip install --upgrade transformers\n")
            else:
                sys.stdout.write("[NLG] ğŸ’¡ äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:\n")
                sys.stdout.write("[NLG]    ./scripts/setup/predownload_all_models.sh\n")
            return False

    def update(self, query):
        # éŸ³å£°èªè­˜çµæœãŒãƒªã‚¹ãƒˆã®å ´åˆã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸ‹ã‚è¾¼ã‚€
        self.asr_results = None
        # ç©ºãƒªã‚¹ãƒˆã¾ãŸã¯å…¨ã¦ç©ºæ–‡å­—åˆ—ãªã‚‰ä½•ã‚‚ã—ãªã„
        if isinstance(query, list):
            if not query or all((not x or x.strip() == "") for x in query):
                self.update_flag = False
                return
            self.asr_results = query
            self.query = query
        else:
            if not query or (isinstance(query, str) and query.strip() == ""):
                self.update_flag = False
                return
            self.query = query
            self.asr_results = None
        
        # ä¸¦åˆ—æ¨è«–ã‚·ã‚¹ãƒ†ãƒ ã«è¿½åŠ 
        now = datetime.now()
        inference_request = {
            'query': query,
            'asr_results': self.asr_results,
            'timestamp': now,
            'request_id': f"{now.strftime('%H%M%S%f')}"
        }
        
        with self.inference_lock:
            # æ¨è«–ä¸­ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
            if self.is_inferencing and self.current_inference_start_time:
                time_diff = (now - self.current_inference_start_time).total_seconds() * 1000
                if time_diff >= 200:  # 200msä»¥ä¸ŠçµŒéã—ã¦ã„ã‚Œã°æ–°ã—ã„æ¨è«–ã‚’é–‹å§‹
                    sys.stdout.write(f"[{now.strftime('%H:%M:%S.%f')[:-3]}][NLG] æ¨è«–ä¸­ã«æ–°è¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä¿¡ (çµŒéæ™‚é–“: {time_diff:.1f}ms) - ä¸¦åˆ—æ¨è«–é–‹å§‹\n")
                    self.inference_queue.put(inference_request)
                else:
                    sys.stdout.write(f"[{now.strftime('%H:%M:%S.%f')[:-3]}][NLG] æ¨è«–ä¸­ã«æ–°è¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä¿¡ (çµŒéæ™‚é–“: {time_diff:.1f}ms) - 200msæœªæº€ã®ãŸã‚å¾…æ©Ÿ\n")
            else:
                # æ¨è«–ä¸­ã§ãªã‘ã‚Œã°å³åº§ã«è¿½åŠ 
                self.inference_queue.put(inference_request)
        
        self.update_flag = True

    def _inference_worker(self, worker_id):
        """æ¨è«–ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•° - 2ã¤ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒä¸¦åˆ—ã§å‹•ä½œ"""
        while True:
            try:
                # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰æ¨è«–ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å–å¾—ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
                request = self.inference_queue.get(timeout=1.0)
                
                with self.inference_lock:
                    if not self.is_inferencing:
                        # æ¨è«–é–‹å§‹
                        self.is_inferencing = True
                        self.current_inference_start_time = datetime.now()
                        start_timestamp = self.current_inference_start_time.strftime('%H:%M:%S.%f')[:-3]
                        sys.stdout.write(f"[{start_timestamp}][NLG] Worker{worker_id} æ¨è«–é–‹å§‹ (ID: {request['request_id']})\n")
                        # GPUä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèª
                        self._log_gpu_memory_usage(f"Worker{worker_id} æ¨è«–é–‹å§‹æ™‚")
                    else:
                        # æ—¢ã«ä»–ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒæ¨è«–ä¸­ã®å ´åˆ
                        time_diff = (datetime.now() - self.current_inference_start_time).total_seconds() * 1000
                        if time_diff < 200:
                            # 200msæœªæº€ãªã‚‰å¾…æ©Ÿ
                            sys.stdout.write(f"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}][NLG] Worker{worker_id} 200msæœªæº€ã®ãŸã‚å¾…æ©Ÿ\n")
                            self.inference_queue.put(request)  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æˆ»ã™
                            continue
                        else:
                            # 200msä»¥ä¸ŠçµŒéã—ã¦ã„ã‚Œã°ä¸¦åˆ—æ¨è«–é–‹å§‹
                            sys.stdout.write(f"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}][NLG] Worker{worker_id} ä¸¦åˆ—æ¨è«–é–‹å§‹ (ID: {request['request_id']})\n")
                            # GPUä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèª
                            self._log_gpu_memory_usage(f"Worker{worker_id} ä¸¦åˆ—æ¨è«–é–‹å§‹æ™‚")
                
                # Gemma 3ã§æ¨è«–å®Ÿè¡Œ
                model_lock = self.model_lock_1 if worker_id == 1 else self.model_lock_2
                result = self._perform_gemma3_inference(request, model_lock, worker_id)
                sys.stdout.write(f"[NLG] Worker{worker_id} Gemma 3æ¨è«–çµæœ: '{result}'\n")
                
                if result:
                    with self.result_lock:
                        # ã‚ˆã‚Šæ–°ã—ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®çµæœãªã‚‰æ›´æ–°
                        if (self.latest_result is None or 
                            request['timestamp'] >= self.latest_result['timestamp']):
                            self.latest_result = {
                                'reply': result,
                                'source_words': request['asr_results'] if request['asr_results'] else [str(request['query'])],
                                'timestamp': request['timestamp'],
                                'worker_id': worker_id,
                                'request_id': request['request_id']
                            }
                            end_time = datetime.now()
                            end_timestamp = end_time.strftime('%H:%M:%S.%f')[:-3]
                            duration = (end_time - self.current_inference_start_time).total_seconds() * 1000
                            sys.stdout.write(f"[{end_timestamp}][NLG] Worker{worker_id} æ¨è«–å®Œäº† (ID: {request['request_id']}, {duration:.1f}ms)\n")
                            # æ¨è«–å®Œäº†æ™‚ã®GPUçŠ¶æ³ç¢ºèª
                            self._log_gpu_memory_usage(f"Worker{worker_id} æ¨è«–å®Œäº†æ™‚")
                
                with self.inference_lock:
                    self.is_inferencing = False
                    self.current_inference_start_time = None
                    
            except Empty:
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - æ­£å¸¸ãªå‹•ä½œ
                continue
            except Exception as e:
                sys.stdout.write(f"[NLG ERROR] Worker{worker_id}: {e}\n")
                with self.inference_lock:
                    self.is_inferencing = False
                    self.current_inference_start_time = None

    def _perform_gemma3_inference(self, request, model_lock, worker_id):
        """Gemma 3ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ã‚’å®Ÿè¡Œ"""
        try:
            query = request['query']
            asr_results = request['asr_results']
            
            import torch
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            if asr_results and isinstance(asr_results, list) and len(asr_results) >= 1:
                if all((not x or x.strip() == "") for x in asr_results):
                    return ""
                
                # éŸ³å£°èªè­˜çµæœã‚’ã™ã¹ã¦åˆ—æŒ™
                asr_lines = []
                for idx, asr in enumerate(asr_results):
                    asr_lines.append(f"èªè­˜çµæœ{idx+1}: {asr}")
                asr_block = "\n".join(asr_lines)
                
                prompt = f"""ä»¥ä¸‹ã®éŸ³å£°èªè­˜çµæœã‹ã‚‰ã€è‡ªç„¶ã§è¦ªã—ã¿ã‚„ã™ã„å¿œç­”ã‚’40æ–‡å­—ä»¥å†…ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

{asr_block}

å¿œç­”:"""
            else:
                if not query or (isinstance(query, list) and all((not x or x.strip() == "") for x in query)):
                    return ""
                prompt = f"è¦ªã—ã¿ã‚„ã™ã15æ–‡å­—ä»¥å†…ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚\n\nãƒ¦ãƒ¼ã‚¶: {query}\nå¿œç­”:"
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒƒã‚¯ã‚’å–å¾—ã—ã¦æ¨è«–å®Ÿè¡Œï¼ˆè¤‡æ•°å›è©¦è¡Œï¼‰
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    with model_lock:
                        inputs = self.tokenizer(
                            prompt, 
                            return_tensors="pt", 
                            truncation=True, 
                            max_length=512,  # çŸ­ç¸®ã—ã¦é«˜é€ŸåŒ–
                            padding=False    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–
                        )
                        inputs = {k: v.to(self.local_model.device) for k, v in inputs.items()}
                        
                        # NaN/Infãƒã‚§ãƒƒã‚¯
                        if torch.isnan(inputs['input_ids']).any() or torch.isinf(inputs['input_ids']).any():
                            sys.stdout.write(f"[NLG] âš ï¸  å…¥åŠ›ã«NaN/Infã‚’æ¤œå‡º (è©¦è¡Œ {attempt+1}/{max_retries})\n")
                            continue
                        
                        with torch.no_grad():
                            # è©¦è¡Œå›æ•°ã«å¿œã˜ã¦è¨­å®šã‚’èª¿æ•´
                            if attempt == 0:
                                # æœ€åˆã¯é«˜é€Ÿè¨­å®š
                                generation_config = {
                                    "max_new_tokens": 50,  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¸›ã‚‰ã—ã¦é«˜é€ŸåŒ–
                                    "temperature": 0.8,
                                    "top_p": 0.9,
                                    "top_k": 20,  # å€™è£œã‚’çµã£ã¦é«˜é€ŸåŒ–
                                    "do_sample": True,
                                    "pad_token_id": self.tokenizer.eos_token_id,
                                    "repetition_penalty": 1.05,  # è»½é‡åŒ–
                                    "use_cache": True,
                                    "num_beams": 1,  # ãƒ“ãƒ¼ãƒ æ¢ç´¢ã‚’ç„¡åŠ¹åŒ–
                                }
                            elif attempt == 1:
                                # 2å›ç›®ã¯ã‚ˆã‚Šä¿å®ˆçš„è¨­å®š
                                generation_config = {
                                    "max_new_tokens": 50,
                                    "temperature": 0.5,
                                    "top_p": 0.9,
                                    "top_k": 20,
                                    "do_sample": True,
                                    "pad_token_id": self.tokenizer.eos_token_id,
                                    "repetition_penalty": 1.2,
                                    "use_cache": False
                                }
                            else:
                                # æœ€å¾Œã¯greedy decoding
                                generation_config = {
                                    "max_new_tokens": 30,
                                    "do_sample": False,
                                    "pad_token_id": self.tokenizer.eos_token_id,
                                    "use_cache": False
                                }
                            
                            outputs = self.local_model.generate(**inputs, **generation_config)
                        
                        # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
                        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                        
                        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»ã—ã¦å¿œç­”ã®ã¿æŠ½å‡º
                        if "å¿œç­”:" in generated_text:
                            response = generated_text.split("å¿œç­”:")[-1].strip()
                        elif "ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰:" in generated_text:
                            response = generated_text.split("ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰:")[-1].strip()
                        else:
                            response = generated_text[len(prompt):].strip()
                        
                        result = response.replace('\n', '').replace('\r', '')
                        
                        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                        del outputs, generated_text
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                        
                        # çµæœã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                        if result and len(result.strip()) > 0:
                            if attempt > 0:
                                sys.stdout.write(f"[NLG] âœ… æ¨è«–æˆåŠŸ (è©¦è¡Œ {attempt+1}/{max_retries})\n")
                            return result
                        else:
                            sys.stdout.write(f"[NLG] âš ï¸  ç©ºã®çµæœ (è©¦è¡Œ {attempt+1}/{max_retries})\n")
                            
                except RuntimeError as e:
                    error_str = str(e)
                    if "probability tensor contains either `inf`, `nan` or element < 0" in error_str:
                        sys.stdout.write(f"[NLG] âš ï¸  ç¢ºç‡ãƒ†ãƒ³ã‚½ãƒ«ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt+1}/{max_retries}): {error_str}\n")
                        if attempt < max_retries - 1:
                            # GPUçŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
                            if torch.cuda.is_available():
                                torch.cuda.empty_cache()
                            continue
                        else:
                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡å˜ãªå¿œç­”ã‚’è¿”ã™
                            return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
                    else:
                        raise e
                except Exception as e:
                    sys.stdout.write(f"[NLG] âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt+1}/{max_retries}): {str(e)}\n")
                    if attempt == max_retries - 1:
                        raise e
                    continue
            
            return "å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
            
        except Exception as e:
            sys.stdout.write(f"[NLG ERROR] Worker{worker_id} Gemma 3 inference failed: {e}\n")
            return ""
    
    def run(self):
        """ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ— - ä¸¦åˆ—æ¨è«–ã‚·ã‚¹ãƒ†ãƒ ã®çµæœã‚’ç›£è¦–"""
        sys.stdout.write("[NLG] Gemma 3ä¸¦åˆ—æ¨è«–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹\n")
        sys.stdout.flush()
        
        while True:
            # æœ€æ–°ã®æ¨è«–çµæœã‚’ãƒã‚§ãƒƒã‚¯
            with self.result_lock:
                if self.latest_result and (
                    self.latest_result['reply'] != self.last_reply or
                    self.latest_result['source_words'] != self.last_source_words
                ):
                    # æ–°ã—ã„çµæœãŒåˆ©ç”¨å¯èƒ½
                    self.last_reply = self.latest_result['reply']
                    self.last_source_words = self.latest_result['source_words']
                    
                    now = datetime.now()
                    timestamp = now.strftime('%H:%M:%S.%f')[:-3]
                    sys.stdout.write(f"[{timestamp}][NLG] æ–°ã—ã„å¯¾è©±çµæœã‚’é©ç”¨ (Worker{self.latest_result['worker_id']}, ID: {self.latest_result['request_id']})\n")
                    sys.stdout.write(f"[{timestamp}][NLG] æœ€çµ‚ç”Ÿæˆçµæœ: {self.last_reply}\n")
                    sys.stdout.write(f"[{timestamp}][NLG] ä½¿ç”¨ã—ãŸéŸ³å£°èªè­˜çµæœ: {self.last_source_words}\n")
                    sys.stdout.flush()
                    
                    # çµæœã‚’ã‚¯ãƒªã‚¢
                    self.latest_result = None
            
            time.sleep(0.01)