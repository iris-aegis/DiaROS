#!/usr/bin/env python3
"""
Gemma3-12B GPUä½¿ç”¨ãƒ†ã‚¹ãƒˆ
"""

import time
from datetime import datetime
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

def test_gemma12b_gpu():
    print("ğŸ”§ Gemma3-12B GPUä½¿ç”¨ãƒ†ã‚¹ãƒˆ")
    print("="*50)
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«è¨­å®š
    ollama_model = ChatOllama(
        model="gemma3:12b",
        max_tokens=100,
        temperature=0.3,
        top_p=0.9,
        num_predict=80
    )
    
    # è¤‡é›‘ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå®Ÿéš›ã®ASRç”¨ï¼‰
    prompt_file_path = "/workspace/DiaROS_py/diaros/prompts/asr_dialogue_prompt.txt"
    try:
        with open(prompt_file_path, 'r', encoding='utf-8') as f:
            complex_prompt_text = f.read()
    except:
        complex_prompt_text = "è¦ªã—ã¿ã‚„ã™ã60æ–‡å­—ç¨‹åº¦ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚"
    
    # ãƒ†ã‚¹ãƒˆç”¨ASRçµæœ
    asr_lines = [
        "èªè­˜çµæœ1: å¤§éƒ½ã ã‹ã‚‰é ‘å¼µãªã„ã¨ã„ã‘ãªã„ã‚“ã ã‚ˆã­[ç„¡éŸ³][ç„¡éŸ³]",
        "èªè­˜çµæœ2: ã‚ˆã­[ç„¡éŸ³]",
        "èªè­˜çµæœ3: ä»Šæ—¥",
        "èªè­˜çµæœ4: ä»Šæ—¥ä¼šç¤¾ã§æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è©±",
        "èªè­˜çµæœ5: ã§æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è©±ãŒã‚ã£ã¦æœ€åˆã¯",
        "èªè­˜çµæœ6: æœ€åˆã¯ã™ã”ãé¢ç™½ãã†ã§ã‚„ã£ã¦ã¿ãŸã„",
        "èªè­˜çµæœ7: é¢ç™½ãã†ã§ã‚„ã£ã¦ã¿ãŸã„ã£ã¦æ€ã‚“ã ã‘ã©ç· ã‚åˆ‡ã‚ŠãŒã‹ãªã‚Š",
        "èªè­˜çµæœ8: ã‚“ã ã‘ã©ç· ã‚åˆ‡ã‚ŠãŒã‹ãªã‚Šã‚¿ã‚¤é€”ã ã‹ã‚‰é ‘å¼µã‚‰ãªã„ã¨ã„ã‘ãªã„ã‚“ã ã‚ˆ"
    ]
    
    print("ğŸ“Š è¤‡é›‘ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + å®ŸASRãƒ‡ãƒ¼ã‚¿ãƒ†ã‚¹ãƒˆ:")
    
    durations = []
    for i in range(3):
        messages = [("system", complex_prompt_text)]
        for line in asr_lines:
            messages.append(("human", line))
        
        complex_prompt = ChatPromptTemplate.from_messages(messages)
        chain = complex_prompt | ollama_model | StrOutputParser()
        
        start_time = datetime.now()
        response = chain.invoke({})
        end_time = datetime.now()
        
        duration = (end_time - start_time).total_seconds() * 1000
        durations.append(duration)
        
        print(f"  ãƒ†ã‚¹ãƒˆ{i+1}: {duration:.1f}ms")
        print(f"    å¿œç­”: {response[:60]}...")
        print()
        
        time.sleep(1)  # 1ç§’é–“éš”
    
    avg_duration = sum(durations) / len(durations)
    print(f"ğŸ“ˆ å¹³å‡æ¨è«–æ™‚é–“: {avg_duration:.1f}ms")
    
    if avg_duration < 500:
        print("âœ… é«˜é€Ÿ: æœŸå¾…é€šã‚Šã®æ€§èƒ½ã§ã™")
    elif avg_duration < 1000:
        print("ğŸŸ¡ æ™®é€š: è¨±å®¹ç¯„å›²å†…ã§ã™")
    elif avg_duration < 3000:
        print("âš ï¸  é…ã„: æœ€é©åŒ–ãŒå¿…è¦ã§ã™")
    else:
        print("âŒ éå¸¸ã«é…ã„: å•é¡ŒãŒã‚ã‚Šã¾ã™")
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãƒ†ã‚¹ãƒˆã¨ã®æ¯”è¼ƒ
    print("\nğŸ”„ ã‚·ãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚¹ãƒˆ:")
    simple_prompt = ChatPromptTemplate.from_messages([
        ("system", "20æ–‡å­—ä»¥å†…ã§è¦ªã—ã¿ã‚„ã™ãå¿œç­”ã—ã¦ãã ã•ã„ã€‚"),
        ("human", "ã“ã‚“ã«ã¡ã¯")
    ])
    simple_chain = simple_prompt | ollama_model | StrOutputParser()
    
    simple_durations = []
    for i in range(3):
        start_time = datetime.now()
        response = simple_chain.invoke({})
        end_time = datetime.now()
        
        duration = (end_time - start_time).total_seconds() * 1000
        simple_durations.append(duration)
        
        print(f"  ã‚·ãƒ³ãƒ—ãƒ«{i+1}: {duration:.1f}ms - '{response}'")
        time.sleep(0.5)
    
    simple_avg = sum(simple_durations) / len(simple_durations)
    print(f"\nğŸ“ˆ ã‚·ãƒ³ãƒ—ãƒ«å¹³å‡: {simple_avg:.1f}ms")
    print(f"ğŸ” ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½±éŸ¿: {avg_duration / simple_avg:.1f}å€é…ã„")

if __name__ == "__main__":
    test_gemma12b_gpu()