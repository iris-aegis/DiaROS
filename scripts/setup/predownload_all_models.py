#!/usr/bin/env python3
"""
éŸ³å£°å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ç”¨ã®ãƒ¢ãƒ‡ãƒ«äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•å‰ã«ã™ã¹ã¦ã®å¿…è¦ãªãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãŠããŸã‚ã®ãƒ„ãƒ¼ãƒ«
"""

import os
import sys
import time
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from huggingface_hub import HfFolder
import torch

def get_hf_token():
    """HuggingFaceãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ï¼ˆç’°å¢ƒå¤‰æ•°ã¾ãŸã¯CLIãƒ­ã‚°ã‚¤ãƒ³ã‹ã‚‰ï¼‰"""
    # 1. ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
    token = os.environ.get("HF_TOKEN") or os.environ.get("HUGGINGFACE_TOKEN")
    
    # 2. huggingface-cli loginã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
    if not token:
        try:
            token = HfFolder.get_token()
        except:
            pass
    
    return token

def check_disk_space(required_gb=20):
    """å¿…è¦ãªãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ4ãƒ“ãƒƒãƒˆé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰"""
    import shutil
    
    cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
    os.makedirs(cache_dir, exist_ok=True)
    
    free_space = shutil.disk_usage(cache_dir).free / (1024**3)  # GB
    
    if free_space < required_gb:
        print(f"âš ï¸  è­¦å‘Š: ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        print(f"å¿…è¦å®¹é‡: {required_gb}GBï¼ˆ4ãƒ“ãƒƒãƒˆé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰, åˆ©ç”¨å¯èƒ½: {free_space:.1f}GB")
        return False
    
    print(f"âœ… ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãƒã‚§ãƒƒã‚¯ OK ({free_space:.1f}GBåˆ©ç”¨å¯èƒ½)")
    return True

def download_model_safely(model_name, model_type="auto", custom_cache_dir=None, use_4bit=False):
    """å®‰å…¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ4ãƒ“ãƒƒãƒˆé‡å­åŒ–å¯¾å¿œï¼‰"""
    hf_token = get_hf_token()
    
    try:
        print(f"\nğŸ“¥ {model_name} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        if use_4bit:
            print(f"  ğŸ”§ 4ãƒ“ãƒƒãƒˆé‡å­åŒ–ã‚’ä½¿ç”¨ã—ã¾ã™")
        
        kwargs = {
            "token": hf_token,
            "trust_remote_code": True,
            "low_cpu_mem_usage": True
        }
        
        # 4ãƒ“ãƒƒãƒˆé‡å­åŒ–è¨­å®š
        if use_4bit:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            kwargs["quantization_config"] = quantization_config
            kwargs["torch_dtype"] = torch.float16
        else:
            kwargs["torch_dtype"] = "auto"
        
        if custom_cache_dir:
            kwargs["cache_dir"] = custom_cache_dir
        
        start_time = time.time()
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        tokenizer = AutoTokenizer.from_pretrained(model_name, **{k: v for k, v in kwargs.items() if k not in ["quantization_config", "torch_dtype"]})
        print(f"  âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs)
        
        download_time = time.time() - start_time
        print(f"  âœ… ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº† ({download_time:.1f}ç§’)")
        
        # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        del model
        del tokenizer
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        error_str = str(e)
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {error_str}")
        
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰åŸå› ã‚’åˆ¤å®š
        if "403" in error_str or "restricted" in error_str:
            print(f"  åŸå› : {model_name}ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ãŒã‚ã‚Šã¾ã›ã‚“")
            print(f"  è§£æ±ºæ–¹æ³•: https://huggingface.co/{model_name} ã§ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ã‚’å–å¾—")
        elif "connection" in error_str.lower() or "timeout" in error_str.lower():
            print(f"  åŸå› : ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã®å•é¡Œ")
        elif "disk" in error_str.lower() or "space" in error_str.lower():
            print(f"  åŸå› : ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³")
        elif "bitsandbytes" in error_str.lower():
            print(f"  åŸå› : bitsandbytesãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³")
            print(f"  è§£æ±ºæ–¹æ³•: pip install bitsandbytes")
        elif "argument of type 'NoneType' is not iterable" in error_str:
            print(f"  åŸå› : å†…éƒ¨ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ¢ãƒ‡ãƒ«ã¯æ­£å¸¸ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰")
            print(f"  ç¢ºèª: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        
        return False

def download_nlg_models():
    """è‡ªç„¶è¨€èªç”Ÿæˆç”¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆGemma 3 4ãƒ“ãƒƒãƒˆé‡å­åŒ–ï¼‰"""
    print("\nğŸ¤– è‡ªç„¶è¨€èªç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆGemma 3 - 4ãƒ“ãƒƒãƒˆé‡å­åŒ–ï¼‰")
    print("=" * 60)
    
    # ã‚«ã‚¹ã‚¿ãƒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    custom_cache_dir = "/workspace/models"
    os.makedirs(custom_cache_dir, exist_ok=True)
    
    models = [
        {
            "name": "google/gemma-3-27b-it",
            "description": "Gemma 3 27Bãƒ¢ãƒ‡ãƒ«ï¼ˆ4ãƒ“ãƒƒãƒˆé‡å­åŒ–ï¼‰",
            "size": "ç´„14GBï¼ˆé‡å­åŒ–å¾Œï¼‰"
        },
        {
            "name": "google/gemma-3-12b-it", 
            "description": "Gemma 3 12Bãƒ¢ãƒ‡ãƒ«ï¼ˆ4ãƒ“ãƒƒãƒˆé‡å­åŒ–ï¼‰",
            "size": "ç´„6GBï¼ˆé‡å­åŒ–å¾Œï¼‰"
        }
    ]
    
    success_count = 0
    
    for model in models:
        print(f"\nğŸ“‹ {model['description']}: {model['name']} ({model['size']})")
        
        if download_model_safely(model["name"], "auto", custom_cache_dir, use_4bit=True):
            success_count += 1
        else:
            print(f"  âš ï¸  {model['name']} ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—")
    
    return success_count

def download_audio_models():
    """éŸ³å£°å‡¦ç†ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆä¸è¦ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰"""
    print("\nğŸ¤ éŸ³å£°å‡¦ç†ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    print("=" * 50)
    print("â„¹ï¸  éŸ³å£°å‡¦ç†ãƒ¢ãƒ‡ãƒ«ã¯ä¸è¦ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
    
    return 0  # éŸ³å£°ãƒ¢ãƒ‡ãƒ«ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãªã„

def show_cache_info():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±ã‚’è¡¨ç¤º"""
    print("\nğŸ“ ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±")
    print("=" * 30)
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥
    default_cache = os.path.expanduser("~/.cache/huggingface/hub")
    if os.path.exists(default_cache):
        print(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥: {default_cache}")
    
    # ã‚«ã‚¹ã‚¿ãƒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    custom_cache = "/workspace/models"
    if os.path.exists(custom_cache):
        print(f"ã‚«ã‚¹ã‚¿ãƒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {custom_cache}")
        
        # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
        models = [d for d in os.listdir(custom_cache) if d.startswith("models--")]
        if models:
            print("\nãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«:")
            for model in sorted(models):
                model_name = model.replace("models--", "").replace("--", "/")
                print(f"  âœ… {model_name}")

def main():
    print("ğŸš€ DiaROSéŸ³å£°å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ  - ãƒ¢ãƒ‡ãƒ«äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ„ãƒ¼ãƒ«")
    print("=" * 60)
    print("Gemma 3 ãƒ¢ãƒ‡ãƒ«ï¼ˆ27B/12Bï¼‰ã‚’4ãƒ“ãƒƒãƒˆé‡å­åŒ–ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€")
    print("ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•æ™‚é–“ã‚’çŸ­ç¸®ã—ã¾ã™ã€‚")
    print("")
    
    # HuggingFaceãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºèª
    hf_token = get_hf_token()
    if not hf_token:
        print("âš ï¸  è­¦å‘Š: HuggingFaceãƒˆãƒ¼ã‚¯ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("")
        print("ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®æ–¹æ³•ã§ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ï¼š")
        print("")
        print("æ–¹æ³•1: HuggingFace CLIã§ãƒ­ã‚°ã‚¤ãƒ³ï¼ˆæ¨å¥¨ï¼‰")
        print("  huggingface-cli login")
        print("")
        print("æ–¹æ³•2: ç’°å¢ƒå¤‰æ•°ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š")
        print("  export HF_TOKEN=your_huggingface_token")
        print("")
        print("è¨­å®šå¾Œã€ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return False
    
    print("âœ… HuggingFaceãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
    
    # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãƒã‚§ãƒƒã‚¯ï¼ˆ4ãƒ“ãƒƒãƒˆé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰
    if not check_disk_space(20):
        print("\nãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã‚’ç¢ºä¿ã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return False
    
    # GPUæƒ…å ±è¡¨ç¤º
    if torch.cuda.is_available():
        print(f"âœ… CUDAå¯¾å¿œGPUæ¤œå‡º: {torch.cuda.get_device_name()}")
    else:
        print("â„¹ï¸  CPUç’°å¢ƒã§å®Ÿè¡Œã—ã¾ã™")
    
    print("\nâ° ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
    start_time = time.time()
    
    # NLGãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    nlg_success = download_nlg_models()
    
    # éŸ³å£°å‡¦ç†ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    audio_success = download_audio_models()
    
    total_time = time.time() - start_time
    
    # çµæœè¡¨ç¤º
    print("\n" + "=" * 60)
    print("ğŸ“Š ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ")
    print("=" * 60)
    print(f"Gemma 3 ãƒ¢ãƒ‡ãƒ«ï¼ˆ4ãƒ“ãƒƒãƒˆé‡å­åŒ–ï¼‰: {nlg_success}/2 æˆåŠŸ")
    print(f"ç·æ‰€è¦æ™‚é–“: {total_time:.1f}ç§’")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±è¡¨ç¤º
    show_cache_info()
    
    if nlg_success > 0:
        print("\nâœ… ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼")
        print("\nã“ã‚Œã§éŸ³å£°å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã‚’é«˜é€Ÿèµ·å‹•ã§ãã¾ã™ï¼š")
        print("  ./scripts/launch/launch_diaros_local.sh")
        print("  ./scripts/launch/launch_diaros_chatgpt.sh")
        return True
    else:
        print("\nâŒ å¿…è¦ãªãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
        print("ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)